<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>A Proofs | Statistical Tools for Causal Inference</title>
  <meta name="description" content="This is an open source collaborative book." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="A Proofs | Statistical Tools for Causal Inference" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is an open source collaborative book." />
  <meta name="github-repo" content="chabefer/STCI" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Proofs | Statistical Tools for Causal Inference" />
  
  <meta name="twitter:description" content="This is an open source collaborative book." />
  

<meta name="author" content="Sylvain ChabÃ©-Ferret" />


<meta name="date" content="2022-07-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mediation-analysis.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
$$
\newcommand{\uns}[1]{\mathbf{1}[#1]}
\newcommand{\esp}[1]{\mathbf{E}[#1]}
\newcommand{\hatesp}[1]{\hat{\mathbf{E}}[ #1 ]}
\newcommand{\Ind}{\perp\kern-5pt\perp}
\newcommand{\var}[1]{\mathbf{V}[ #1 ]}
\newcommand{\cov}[1]{\mathbf{C}[ #1 ]}
\newcommand{\plim}[1]{\text{plim}_{ #1 \rightarrow \infty}}
\newcommand{\plims}{\text{plim}}
\newcommand{\partder}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\partdersq}[2]{\frac{\partial^2 #1}{\partial #2^2}}
\DeclareMathOperator{\diag}{diag}
$$


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Tools for Causal Inference</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>I Fundamental Problems of Inference</b></span></li>
<li class="chapter" data-level="" data-path="introduction-the-two-fundamental-problems-of-inference.html"><a href="introduction-the-two-fundamental-problems-of-inference.html"><i class="fa fa-check"></i>Introduction: the Two Fundamental Problems of Inference</a></li>
<li class="chapter" data-level="1" data-path="FPCI.html"><a href="FPCI.html"><i class="fa fa-check"></i><b>1</b> Fundamental Problem of Causal Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="FPCI.html"><a href="FPCI.html#rubin-causal-model"><i class="fa fa-check"></i><b>1.1</b> Rubin Causal Model</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="FPCI.html"><a href="FPCI.html#treatment-allocation-rule"><i class="fa fa-check"></i><b>1.1.1</b> Treatment allocation rule</a></li>
<li class="chapter" data-level="1.1.2" data-path="FPCI.html"><a href="FPCI.html#potential-outcomes"><i class="fa fa-check"></i><b>1.1.2</b> Potential outcomes</a></li>
<li class="chapter" data-level="1.1.3" data-path="FPCI.html"><a href="FPCI.html#switching-equation"><i class="fa fa-check"></i><b>1.1.3</b> Switching equation</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="FPCI.html"><a href="FPCI.html#treatment-effects"><i class="fa fa-check"></i><b>1.2</b> Treatment effects</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="FPCI.html"><a href="FPCI.html#individual-level-treatment-effects"><i class="fa fa-check"></i><b>1.2.1</b> Individual level treatment effects</a></li>
<li class="chapter" data-level="1.2.2" data-path="FPCI.html"><a href="FPCI.html#average-treatment-effect-on-the-treated"><i class="fa fa-check"></i><b>1.2.2</b> Average treatment effect on the treated</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="FPCI.html"><a href="FPCI.html#fundamental-problem-of-causal-inference"><i class="fa fa-check"></i><b>1.3</b> Fundamental problem of causal inference</a></li>
<li class="chapter" data-level="1.4" data-path="FPCI.html"><a href="FPCI.html#intuitive-estimators-confounding-factors-and-selection-bias"><i class="fa fa-check"></i><b>1.4</b> Intuitive estimators, confounding factors and selection bias</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="FPCI.html"><a href="FPCI.html#withwithout-comparison-selection-bias-and-cross-sectional-confounders"><i class="fa fa-check"></i><b>1.4.1</b> With/Without comparison, selection bias and cross-sectional confounders</a></li>
<li class="chapter" data-level="1.4.2" data-path="FPCI.html"><a href="FPCI.html#the-beforeafter-comparison-temporal-confounders-and-time-trend-bias"><i class="fa fa-check"></i><b>1.4.2</b> The before/after comparison, temporal confounders and time trend bias</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="FPSI.html"><a href="FPSI.html"><i class="fa fa-check"></i><b>2</b> Fundamental Problem of Statistical Inference</a>
<ul>
<li class="chapter" data-level="2.1" data-path="FPSI.html"><a href="FPSI.html#sec:sampnoise"><i class="fa fa-check"></i><b>2.1</b> What is sampling noise? Definition and illustration</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="FPSI.html"><a href="FPSI.html#sec:definitionnoise"><i class="fa fa-check"></i><b>2.1.1</b> Sampling noise, a definition</a></li>
<li class="chapter" data-level="2.1.2" data-path="FPSI.html"><a href="FPSI.html#sec:illusnoisepop"><i class="fa fa-check"></i><b>2.1.2</b> Sampling noise for the population treatment effect</a></li>
<li class="chapter" data-level="2.1.3" data-path="FPSI.html"><a href="FPSI.html#sec:illusnoisesamp"><i class="fa fa-check"></i><b>2.1.3</b> Sampling noise for the sample treatment effect</a></li>
<li class="chapter" data-level="2.1.4" data-path="FPSI.html"><a href="FPSI.html#sec:confinterv"><i class="fa fa-check"></i><b>2.1.4</b> Building confidence intervals from estimates of sampling noise</a></li>
<li class="chapter" data-level="2.1.5" data-path="FPSI.html"><a href="FPSI.html#reporting-sampling-noise-a-proposal"><i class="fa fa-check"></i><b>2.1.5</b> Reporting sampling noise: a proposal</a></li>
<li class="chapter" data-level="2.1.6" data-path="FPSI.html"><a href="FPSI.html#sec:effectsize"><i class="fa fa-check"></i><b>2.1.6</b> Using effect sizes to normalize the reporting of treatment effects and their precision</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="FPSI.html"><a href="FPSI.html#sec:estimsampnoise"><i class="fa fa-check"></i><b>2.2</b> Estimating sampling noise</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="FPSI.html"><a href="FPSI.html#sec:assumptions"><i class="fa fa-check"></i><b>2.2.1</b> Assumptions</a></li>
<li class="chapter" data-level="2.2.2" data-path="FPSI.html"><a href="FPSI.html#sec:cheb"><i class="fa fa-check"></i><b>2.2.2</b> Using Chebyshevâs inequality</a></li>
<li class="chapter" data-level="2.2.3" data-path="FPSI.html"><a href="FPSI.html#sec:CLT"><i class="fa fa-check"></i><b>2.2.3</b> Using the Central Limit Theorem</a></li>
<li class="chapter" data-level="2.2.4" data-path="FPSI.html"><a href="FPSI.html#sec:resamp"><i class="fa fa-check"></i><b>2.2.4</b> Using resampling methods</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Methods of Causal Inference</b></span></li>
<li class="chapter" data-level="3" data-path="RCT.html"><a href="RCT.html"><i class="fa fa-check"></i><b>3</b> Randomized Controlled Trials</a>
<ul>
<li class="chapter" data-level="3.1" data-path="RCT.html"><a href="RCT.html#sec:design1"><i class="fa fa-check"></i><b>3.1</b> Brute Force Design</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="RCT.html"><a href="RCT.html#identification"><i class="fa fa-check"></i><b>3.1.1</b> Identification</a></li>
<li class="chapter" data-level="3.1.2" data-path="RCT.html"><a href="RCT.html#estimating-ate"><i class="fa fa-check"></i><b>3.1.2</b> Estimating ATE</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="RCT.html"><a href="RCT.html#sec:design2"><i class="fa fa-check"></i><b>3.2</b> Self-Selection design</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="RCT.html"><a href="RCT.html#identification-1"><i class="fa fa-check"></i><b>3.2.1</b> Identification</a></li>
<li class="chapter" data-level="3.2.2" data-path="RCT.html"><a href="RCT.html#estimating-tt"><i class="fa fa-check"></i><b>3.2.2</b> Estimating TT</a></li>
<li class="chapter" data-level="3.2.3" data-path="RCT.html"><a href="RCT.html#estimating-sampling-noise-1"><i class="fa fa-check"></i><b>3.2.3</b> Estimating Sampling Noise</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="RCT.html"><a href="RCT.html#sec:design3"><i class="fa fa-check"></i><b>3.3</b> Eligibility design</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="RCT.html"><a href="RCT.html#identification-2"><i class="fa fa-check"></i><b>3.3.1</b> Identification</a></li>
<li class="chapter" data-level="3.3.2" data-path="RCT.html"><a href="RCT.html#estimating-the-ite-and-the-tt"><i class="fa fa-check"></i><b>3.3.2</b> Estimating the ITE and the TT</a></li>
<li class="chapter" data-level="3.3.3" data-path="RCT.html"><a href="RCT.html#estimating-sampling-noise-2"><i class="fa fa-check"></i><b>3.3.3</b> Estimating sampling noise</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="RCT.html"><a href="RCT.html#sec:design4"><i class="fa fa-check"></i><b>3.4</b> Encouragement Design</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="RCT.html"><a href="RCT.html#identification-3"><i class="fa fa-check"></i><b>3.4.1</b> Identification</a></li>
<li class="chapter" data-level="3.4.2" data-path="RCT.html"><a href="RCT.html#IVRCT"><i class="fa fa-check"></i><b>3.4.2</b> Estimating the Local Average Treatment Effect and the Intention to Treat Effect</a></li>
<li class="chapter" data-level="3.4.3" data-path="RCT.html"><a href="RCT.html#estimating-sampling-noise-3"><i class="fa fa-check"></i><b>3.4.3</b> Estimating sampling noise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="NE.html"><a href="NE.html"><i class="fa fa-check"></i><b>4</b> Natural Experiments</a>
<ul>
<li class="chapter" data-level="4.1" data-path="NE.html"><a href="NE.html#instrumental-variables"><i class="fa fa-check"></i><b>4.1</b> Instrumental Variables</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="NE.html"><a href="NE.html#an-example-where-monotonicity-does-not-hold"><i class="fa fa-check"></i><b>4.1.1</b> An example where Monotonicity does not hold</a></li>
<li class="chapter" data-level="4.1.2" data-path="NE.html"><a href="NE.html#identification-4"><i class="fa fa-check"></i><b>4.1.2</b> Identification</a></li>
<li class="chapter" data-level="4.1.3" data-path="NE.html"><a href="NE.html#estimation"><i class="fa fa-check"></i><b>4.1.3</b> Estimation</a></li>
<li class="chapter" data-level="4.1.4" data-path="NE.html"><a href="NE.html#estimation-of-sampling-noise"><i class="fa fa-check"></i><b>4.1.4</b> Estimation of sampling noise</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="NE.html"><a href="NE.html#regression-discontinuity-designs"><i class="fa fa-check"></i><b>4.2</b> Regression Discontinuity Designs</a></li>
<li class="chapter" data-level="4.3" data-path="NE.html"><a href="NE.html#difference-in-differences"><i class="fa fa-check"></i><b>4.3</b> Difference In Differences</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="NE.html"><a href="NE.html#sec:DIDbasic"><i class="fa fa-check"></i><b>4.3.1</b> Difference In Differences with two time periods</a></li>
<li class="chapter" data-level="4.3.2" data-path="NE.html"><a href="NE.html#sec:DIDr"><i class="fa fa-check"></i><b>4.3.2</b> Reverse Difference In Differences designs with two time periods</a></li>
<li class="chapter" data-level="4.3.3" data-path="NE.html"><a href="NE.html#difference-in-differences-with-multiple-time-periods"><i class="fa fa-check"></i><b>4.3.3</b> Difference In Differences with multiple time periods</a></li>
<li class="chapter" data-level="4.3.4" data-path="NE.html"><a href="NE.html#difference-in-differences-with-instrumental-variables"><i class="fa fa-check"></i><b>4.3.4</b> Difference In Differences with Instrumental Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sec:OM.html"><a href="sec:OM.html"><i class="fa fa-check"></i><b>5</b> Observational Methods</a>
<ul>
<li class="chapter" data-level="5.1" data-path="sec:OM.html"><a href="sec:OM.html#imputation-methods"><i class="fa fa-check"></i><b>5.1</b> Imputation methods</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="threats.html"><a href="threats.html"><i class="fa fa-check"></i><b>6</b> Threats to the validity of Causal Inference</a>
<ul>
<li class="chapter" data-level="6.1" data-path="threats.html"><a href="threats.html#threats-to-internal-validity"><i class="fa fa-check"></i><b>6.1</b> Threats to internal validity</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="threats.html"><a href="threats.html#survey-bias"><i class="fa fa-check"></i><b>6.1.1</b> Survey bias</a></li>
<li class="chapter" data-level="6.1.2" data-path="threats.html"><a href="threats.html#experimenter-bias"><i class="fa fa-check"></i><b>6.1.2</b> Experimenter bias</a></li>
<li class="chapter" data-level="6.1.3" data-path="threats.html"><a href="threats.html#substitution-bias"><i class="fa fa-check"></i><b>6.1.3</b> Substitution bias</a></li>
<li class="chapter" data-level="6.1.4" data-path="threats.html"><a href="threats.html#diffusion-bias"><i class="fa fa-check"></i><b>6.1.4</b> Diffusion bias</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="threats.html"><a href="threats.html#threats-to-the-measurement-of-precision"><i class="fa fa-check"></i><b>6.2</b> Threats to the measurement of precision</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="threats.html"><a href="threats.html#insufficient-precision"><i class="fa fa-check"></i><b>6.2.1</b> Insufficient precision</a></li>
<li class="chapter" data-level="6.2.2" data-path="threats.html"><a href="threats.html#clustering"><i class="fa fa-check"></i><b>6.2.2</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="threats.html"><a href="threats.html#threats-to-external-validity"><i class="fa fa-check"></i><b>6.3</b> Threats to external validity</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="threats.html"><a href="threats.html#randomization-bias"><i class="fa fa-check"></i><b>6.3.1</b> Randomization bias</a></li>
<li class="chapter" data-level="6.3.2" data-path="threats.html"><a href="threats.html#equilibrium-effects"><i class="fa fa-check"></i><b>6.3.2</b> Equilibrium effects</a></li>
<li class="chapter" data-level="6.3.3" data-path="threats.html"><a href="threats.html#context-effects"><i class="fa fa-check"></i><b>6.3.3</b> Context effects</a></li>
<li class="chapter" data-level="6.3.4" data-path="threats.html"><a href="threats.html#site-selection-bias"><i class="fa fa-check"></i><b>6.3.4</b> Site selection bias</a></li>
<li class="chapter" data-level="6.3.5" data-path="threats.html"><a href="threats.html#publication-bias"><i class="fa fa-check"></i><b>6.3.5</b> Publication bias</a></li>
<li class="chapter" data-level="6.3.6" data-path="threats.html"><a href="threats.html#ethical-and-political-issues"><i class="fa fa-check"></i><b>6.3.6</b> Ethical and political issues</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Additional Topics</b></span></li>
<li class="chapter" data-level="7" data-path="Power.html"><a href="Power.html"><i class="fa fa-check"></i><b>7</b> Power Analysis</a></li>
<li class="chapter" data-level="8" data-path="sec:placebo.html"><a href="sec:placebo.html"><i class="fa fa-check"></i><b>8</b> Placebo Tests</a></li>
<li class="chapter" data-level="9" data-path="cluster.html"><a href="cluster.html"><i class="fa fa-check"></i><b>9</b> Clustering</a></li>
<li class="chapter" data-level="10" data-path="LaLonde.html"><a href="LaLonde.html"><i class="fa fa-check"></i><b>10</b> LaLonde Tests</a></li>
<li class="chapter" data-level="11" data-path="Diffusion.html"><a href="Diffusion.html"><i class="fa fa-check"></i><b>11</b> Diffusion effects</a></li>
<li class="chapter" data-level="12" data-path="Distribution.html"><a href="Distribution.html"><i class="fa fa-check"></i><b>12</b> Distributional effects</a></li>
<li class="chapter" data-level="13" data-path="meta.html"><a href="meta.html"><i class="fa fa-check"></i><b>13</b> Meta-analysis and Publication Bias</a>
<ul>
<li class="chapter" data-level="13.1" data-path="meta.html"><a href="meta.html#meta-analysis"><i class="fa fa-check"></i><b>13.1</b> Meta-analysis</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="meta.html"><a href="meta.html#basic-setting"><i class="fa fa-check"></i><b>13.1.1</b> Basic setting</a></li>
<li class="chapter" data-level="13.1.2" data-path="meta.html"><a href="meta.html#why-vote-counting-does-not-work"><i class="fa fa-check"></i><b>13.1.2</b> Why vote-counting does not work</a></li>
<li class="chapter" data-level="13.1.3" data-path="meta.html"><a href="meta.html#MetaWA"><i class="fa fa-check"></i><b>13.1.3</b> Meta-analysis when treatment effects are homogeneous: the fixed effects approach</a></li>
<li class="chapter" data-level="13.1.4" data-path="meta.html"><a href="meta.html#meta-analysis-when-treatment-effects-are-heterogeneous-the-random-effects-approach"><i class="fa fa-check"></i><b>13.1.4</b> Meta-analysis when treatment effects are heterogeneous: the random effects approach</a></li>
<li class="chapter" data-level="13.1.5" data-path="meta.html"><a href="meta.html#meta-regression"><i class="fa fa-check"></i><b>13.1.5</b> Meta-regression</a></li>
<li class="chapter" data-level="13.1.6" data-path="meta.html"><a href="meta.html#constantly-updated-meta-analysis"><i class="fa fa-check"></i><b>13.1.6</b> Constantly updated meta-analysis</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="meta.html"><a href="meta.html#publication-bias-and-site-selection-bias"><i class="fa fa-check"></i><b>13.2</b> Publication bias and site selection bias</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="meta.html"><a href="meta.html#sources-of-publication-bias-and-of-site-selection-bias-and-questionable-research-practices"><i class="fa fa-check"></i><b>13.2.1</b> Sources of publication bias and of site selection bias and Questionable Research Practices</a></li>
<li class="chapter" data-level="13.2.2" data-path="meta.html"><a href="meta.html#detection-of-and-correction-for-publication-bias"><i class="fa fa-check"></i><b>13.2.2</b> Detection of and correction for publication bias</a></li>
<li class="chapter" data-level="13.2.3" data-path="meta.html"><a href="meta.html#getting-rid-of-publication-bias-registered-reports-and-pre-analysis-plans"><i class="fa fa-check"></i><b>13.2.3</b> Getting rid of publication bias: registered reports and pre-analysis plans</a></li>
<li class="chapter" data-level="13.2.4" data-path="meta.html"><a href="meta.html#detection-of-and-correction-for-site-selection-bias"><i class="fa fa-check"></i><b>13.2.4</b> Detection of and correction for site selection bias</a></li>
<li class="chapter" data-level="13.2.5" data-path="meta.html"><a href="meta.html#vote-counting-and-publication-bias"><i class="fa fa-check"></i><b>13.2.5</b> Vote counting and publication bias</a></li>
<li class="chapter" data-level="13.2.6" data-path="meta.html"><a href="meta.html#the-value-of-a-statistically-significant-result"><i class="fa fa-check"></i><b>13.2.6</b> The value of a statistically significant result</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Bounds.html"><a href="Bounds.html"><i class="fa fa-check"></i><b>14</b> Bounds</a></li>
<li class="chapter" data-level="15" data-path="mediation-analysis.html"><a href="mediation-analysis.html"><i class="fa fa-check"></i><b>15</b> Mediation Analysis</a>
<ul>
<li class="chapter" data-level="15.1" data-path="mediation-analysis.html"><a href="mediation-analysis.html#mediation-analysis-a-framework"><i class="fa fa-check"></i><b>15.1</b> Mediation analysis: a framework</a></li>
<li class="chapter" data-level="15.2" data-path="mediation-analysis.html"><a href="mediation-analysis.html#the-fundamental-problem-of-mediation-analysis"><i class="fa fa-check"></i><b>15.2</b> The Fundamental Problem of Mediation Analysis</a></li>
<li class="chapter" data-level="15.3" data-path="mediation-analysis.html"><a href="mediation-analysis.html#mediation-analysis-under-unconfoundedness"><i class="fa fa-check"></i><b>15.3</b> Mediation analysis under unconfoundedness</a></li>
<li class="chapter" data-level="15.4" data-path="mediation-analysis.html"><a href="mediation-analysis.html#mediation-analysis-with-panel-data"><i class="fa fa-check"></i><b>15.4</b> Mediation analysis with panel data</a></li>
<li class="chapter" data-level="15.5" data-path="mediation-analysis.html"><a href="mediation-analysis.html#mediation-analysis-with-instruments"><i class="fa fa-check"></i><b>15.5</b> Mediation analysis with instruments</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="proofs.html"><a href="proofs.html"><i class="fa fa-check"></i><b>A</b> Proofs</a>
<ul>
<li class="chapter" data-level="A.1" data-path="proofs.html"><a href="proofs.html#proofs-of-results-in-chapter-reffpsi"><i class="fa fa-check"></i><b>A.1</b> Proofs of results in Chapter @ref(FPSI)</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="proofs.html"><a href="proofs.html#proofcheb"><i class="fa fa-check"></i><b>A.1.1</b> Proof of Theorem @ref(thm:uppsampnoise)</a></li>
<li class="chapter" data-level="A.1.2" data-path="proofs.html"><a href="proofs.html#proofCLT"><i class="fa fa-check"></i><b>A.1.2</b> Proof of Theorem @ref(thm:asympnoiseWW)</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="proofs.html"><a href="proofs.html#proofs-of-results-in-chapter-refrct"><i class="fa fa-check"></i><b>A.2</b> Proofs of results in Chapter @ref(RCT)</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="proofs.html"><a href="proofs.html#proofIdentLATE"><i class="fa fa-check"></i><b>A.2.1</b> Proof of Theorem @ref(thm:IdentLATE)</a></li>
<li class="chapter" data-level="A.2.2" data-path="proofs.html"><a href="proofs.html#proofWaldIV"><i class="fa fa-check"></i><b>A.2.2</b> Proof of Theorem @ref(thm:WaldIV)</a></li>
<li class="chapter" data-level="A.2.3" data-path="proofs.html"><a href="proofs.html#ProofAsymWald"><i class="fa fa-check"></i><b>A.2.3</b> Proof of Theorem @ref(thm:asymWald)</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="proofs.html"><a href="proofs.html#proofs-of-results-in-chapter-refne"><i class="fa fa-check"></i><b>A.3</b> Proofs of results in Chapter @ref(NE)</a>
<ul>
<li class="chapter" data-level="A.3.1" data-path="proofs.html"><a href="proofs.html#proofEstimDID"><i class="fa fa-check"></i><b>A.3.1</b> Proof of Theorem @ref(thm:EstimDID)</a></li>
<li class="chapter" data-level="A.3.2" data-path="proofs.html"><a href="proofs.html#proofasympnoiseDIDCross"><i class="fa fa-check"></i><b>A.3.2</b> Proof of Theorem @ref(thm:asympnoiseDIDCross)</a></li>
<li class="chapter" data-level="A.3.3" data-path="proofs.html"><a href="proofs.html#proofEquivDIDSApop"><i class="fa fa-check"></i><b>A.3.3</b> Proof of Theorem @ref(thm:EquivDIDSApop)</a></li>
<li class="chapter" data-level="A.3.4" data-path="proofs.html"><a href="proofs.html#proofEquivDIDSAsamp"><i class="fa fa-check"></i><b>A.3.4</b> Proof of Theorem @ref(thm:EquivDIDSAsamp)</a></li>
<li class="chapter" data-level="A.3.5" data-path="proofs.html"><a href="proofs.html#proofasympnoiseSACross"><i class="fa fa-check"></i><b>A.3.5</b> Proof of Theorem @ref(thm:asympnoiseSACross)</a></li>
<li class="chapter" data-level="A.3.6" data-path="proofs.html"><a href="proofs.html#proofasympnoiseSATTCross"><i class="fa fa-check"></i><b>A.3.6</b> Proof of Theorem @ref(thm:asympnoiseSATTCross)</a></li>
<li class="chapter" data-level="A.3.7" data-path="proofs.html"><a href="proofs.html#proofasympnoiseSAPanel"><i class="fa fa-check"></i><b>A.3.7</b> Proof of Theorem @ref(thm:asympnoiseSAPanel)</a></li>
<li class="chapter" data-level="A.3.8" data-path="proofs.html"><a href="proofs.html#proofasympnoiseSATTPanel"><i class="fa fa-check"></i><b>A.3.8</b> Proof of Theorem @ref(thm:asympnoiseSATTPanel)</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Tools for Causal Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="proofs" class="section level1 hasAnchor" number="16">
<h1><span class="header-section-number">A</span> Proofs<a href="proofs.html#proofs" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="proofs-of-results-in-chapter-reffpsi" class="section level2 hasAnchor" number="16.1">
<h2><span class="header-section-number">A.1</span> Proofs of results in Chapter <a href="FPSI.html#FPSI">2</a><a href="proofs.html#proofs-of-results-in-chapter-reffpsi" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="proofcheb" class="section level3 hasAnchor" number="16.1.1">
<h3><span class="header-section-number">A.1.1</span> Proof of Theorem <a href="FPSI.html#thm:uppsampnoise">2.3</a><a href="proofs.html#proofcheb" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In order to use Theorem <a href="FPSI.html#thm:cheb">2.2</a> for studying the behavior of <span class="math inline">\(\hat{\Delta^Y_{WW}}\)</span>, we have to prove that it is unbiased and we have to compute <span class="math inline">\(\var{\hat{\Delta^Y_{WW}}}\)</span>.
Letâs first prove that the <span class="math inline">\(WW\)</span> estimator is an unbiased estimator of <span class="math inline">\(TT\)</span>:</p>
<div class="lemma">
<p><span id="lem:unbiasww" class="lemma"><strong>Lemma A.1  (Unbiasedness of $\hat{\Delta^Y_{WW}}$) </strong></span>Under Assumptions <a href="FPCI.html#def:noselb">1.7</a>, <a href="FPSI.html#hyp:fullrank">2.1</a> and <a href="FPSI.html#hyp:iid">2.2</a>,</p>
<p><span class="math display">\[\begin{align*}
\esp{\hat{\Delta^Y_{WW}}}&amp; = \Delta^Y_{TT}.
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-182" class="proof"><em>Proof</em>. </span>In order to prove Lemma <a href="proofs.html#lem:unbiasww">A.1</a>, we are going to use a trick.
We are going to compute the expectation of the <span class="math inline">\(WW\)</span> estimator conditional on a given treatment allocation.
Because the resulting estimate is independent of treatment allocation, we will have our proof.
This trick simplifies derivations a lot and is really natural: think first of all the samples with the same treatment allocation, then average your results over all possible treatment allocations.</p>
<p><span class="math display">\[\begin{align*}
\esp{\hat{\Delta^Y_{WW}}} &amp; = \esp{\esp{\hat{\Delta^Y_{WW}}|\mathbf{D}}}\\
                          &amp; = \esp{\esp{\frac{1}{\sum_{i=1}^N D_i}\sum_{i=1}^N Y_iD_i-\frac{1}{\sum_{i=1}^N (1-D_i)}\sum_{i=1}^N Y_i(1-D_i)|\mathbf{D}}}\\
                          &amp; = \esp{\esp{\frac{1}{\sum_{i=1}^N D_i}\sum_{i=1}^N Y_iD_i|\mathbf{D}}-\esp{\frac{1}{\sum_{i=1}^N (1-D_i)}\sum_{i=1}^N Y_i(1-D_i)|\mathbf{D}}}\\
                          &amp; = \esp{\frac{1}{\sum_{i=1}^N D_i}\esp{\sum_{i=1}^N Y_iD_i|\mathbf{D}}-\frac{1}{\sum_{i=1}^N (1-D_i)}\esp{\sum_{i=1}^N Y_i(1-D_i)|\mathbf{D}}}\\
                          &amp; = \esp{\frac{1}{\sum_{i=1}^N D_i}\sum_{i=1}^N \esp{Y_iD_i|\mathbf{D}}-\frac{1}{\sum_{i=1}^N (1-D_i)}\sum_{i=1}^N \esp{Y_i(1-D_i)|\mathbf{D}}}\\
                          &amp; = \esp{\frac{1}{\sum_{i=1}^N D_i}\sum_{i=1}^N \esp{Y_iD_i|D_i}-\frac{1}{\sum_{i=1}^N (1-D_i)}\sum_{i=1}^N \esp{Y_i(1-D_i)|D_i}}\\
                          &amp; = \esp{\frac{1}{\sum_{i=1}^N D_i}\sum_{i=1}^N D_i\esp{Y_i|D_i=1}-\frac{1}{\sum_{i=1}^N (1-D_i)}\sum_{i=1}^N(1-D_i)\esp{Y_i|D_i=0}}\\
                          &amp; = \esp{\frac{\sum_{i=1}^N D_i}{\sum_{i=1}^N D_i}\esp{Y_i|D_i=1}-\frac{\sum_{i=1}^N(1-D_i)}{\sum_{i=1}^N (1-D_i)}\esp{Y_i|D_i=0}}\\
                          &amp; = \esp{\esp{Y_i|D_i=1}-\esp{Y_i|D_i=0}}\\
                          &amp; = \esp{Y_i|D_i=1}-\esp{Y_i|D_i=0} \\
                          &amp; = \Delta^Y_{TT}.
\end{align*}\]</span></p>
<p>The first equality uses the Law of Iterated Expectations (LIE).
The second and fourth equalities use the linearity of conditional expectations.
The third equality uses the fact that, conditional on <span class="math inline">\(\mathbf{D}\)</span>, the number of treated and untreated is a constant.
The fifth equality uses Assumption <a href="FPSI.html#hyp:iid">2.2</a>.
The sixth equality uses the fact that <span class="math inline">\(\esp{Y_iD_i|D_i}=D_i\esp{Y_i*1|D_i=1}+(1-D_i)\esp{Y_i*0|D_i=0}\)</span>.
The seventh and ninth equalities use the fact that <span class="math inline">\(\esp{Y_i|D_i=1}\)</span> is a constant.
The last equality uses Assumption <a href="FPCI.html#def:noselb">1.7</a>.</p>
</div>
<p>Letâs now compute the variance of the <span class="math inline">\(WW\)</span> estimator:</p>
<div class="lemma">
<p><span id="lem:varww" class="lemma"><strong>Lemma A.2  (Variance of $\hat{\Delta^Y_{WW}}$) </strong></span>Under Assumptions <a href="FPCI.html#def:noselb">1.7</a>, <a href="#def:fullrank"><strong>??</strong></a> and <a href="FPSI.html#hyp:iid">2.2</a>,</p>
<p><span class="math display">\[\begin{align*}
\var{{\hat{\Delta^Y_{WW}}}} &amp; = \frac{1-(1-\Pr(D_i=1))^N}{N\Pr(D_i=1)}\var{Y_i^1|D_i=1}+\frac{1-\Pr(D_i=1)^N}{N(1-\Pr(D_i=1))}\var{Y_i^0|D_i=0}.
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-183" class="proof"><em>Proof</em>. </span>Same trick as before, but now using the Law of Total Variance (LTV):</p>
<p><span class="math display">\[\begin{align*}
\var{{\hat{\Delta^Y_{WW}}}} &amp; = \esp{\var{\hat{\Delta^Y_{WW}}|\mathbf{D}}}+\var{\esp{\hat{\Delta^Y_{WW}}|\mathbf{D}}}\\
                            &amp; = \esp{\var{\frac{1}{\sum_{i=1}^N D_i}\sum_{i=1}^N Y_iD_i-\frac{1}{\sum_{i=1}^N (1-D_i)}\sum_{i=1}^N Y_i(1-D_i)|\mathbf{D}}} \\
                            &amp; = \esp{\var{\frac{1}{\sum_{i=1}^N D_i}\sum_{i=1}^N Y_iD_i|\mathbf{D}}}+\esp{\var{\frac{1}{\sum_{i=1}^N (1-D_i)}\sum_{i=1}^N Y_i(1-D_i)|\mathbf{D}}}\\
                            &amp; \phantom{=}+\esp{\cov{\frac{1}{\sum_{i=1}^N D_i}\sum_{i=1}^N Y_iD_i,\frac{1}{\sum_{i=1}^N (1-D_i)}\sum_{i=1}^N Y_i(1-D_i)|\mathbf{D}}} \\
                            &amp; = \esp{\frac{1}{(\sum_{i=1}^N D_i)^2}\var{\sum_{i=1}^N Y_iD_i|\mathbf{D}}}+\esp{\frac{1}{(\sum_{i=1}^N (1-D_i))^2}\var{\sum_{i=1}^N Y_i(1-D_i)|\mathbf{D}}} \\
                            &amp; = \esp{\frac{1}{(\sum_{i=1}^N D_i)^2}\var{\sum_{i=1}^N Y_iD_i|D_i}}+\esp{\frac{1}{(\sum_{i=1}^N (1-D_i))^2}\var{\sum_{i=1}^N Y_i(1-D_i)|D_i}} \\
                            &amp; = \esp{\frac{1}{(\sum_{i=1}^N D_i)^2}\sum_{i=1}^ND_i\var{Y_i|D_i=1}}+\esp{\frac{1}{(\sum_{i=1}^N (1-D_i))^2}\sum_{i=1}^N(1-D_i)\var{Y_i|D_i=0}} \\
                            &amp; = \var{Y_i|D_i=1}\esp{\frac{1}{\sum_{i=1}^N D_i}}+\var{Y_i|D_i=0}\esp{\frac{1}{\sum_{i=1}^N (1-D_i)}} \\
                            &amp; = \frac{1-(1-\Pr(D_i=1))^N}{N\Pr(D_i=1)}\var{Y_i^1|D_i=1}+\frac{1-\Pr(D_i=1)^N}{N(1-\Pr(D_i=1))}\var{Y_i^0|D_i=0}.
\end{align*}\]</span></p>
<p>The first equality stems from the LTV.
The second and third equalities stems from the definition of the <span class="math inline">\(WW\)</span> estimator and of the variance of a sum of random variables.
The fourth equality stems from Assumption <a href="FPSI.html#hyp:iid">2.2</a>, which means that the covariance across observations is zero, and from the formula for a variance of a random variable multiplied by a constant.
The fifth and sixth equalities stems from Assumption <a href="FPSI.html#hyp:iid">2.2</a> and from <span class="math inline">\(\var{Y_iD_i|D_i}=D_i\var{Y_i*1|D_i=1}+(1-D_i)\var{Y_i*0|D_i=0}\)</span>.
The seventh equality stems from <span class="math inline">\(\var{Y_i|D_i=1}\)</span> and <span class="math inline">\(\var{Y_i|D_i=0}\)</span> being constant.
The last equality stems from the formula for the expectation of the inverse of a sum of Bernoulli random variables with at least one of them taking value one which is the case under Assumption <a href="FPSI.html#hyp:fullrank">2.1</a>.</p>
</div>
<p>Using Theorem <a href="FPSI.html#thm:cheb">2.2</a>, we have:</p>
<p><span class="math display">\[\begin{align*}
2\epsilon &amp; \leq 2\sqrt{\frac{1}{N(1-\delta)}\left(\frac{1-(1-\Pr(D_i=1))^N}{\Pr(D_i=1)}\var{Y_i^1|D_i=1}+\frac{1-\Pr(D_i=1)^N}{(1-\Pr(D_i=1))}\var{Y_i^0|D_i=0}\right)}\\
          &amp; \leq 2\sqrt{\frac{1}{N(1-\delta)}\left(\frac{\var{Y_i^1|D_i=1}}{\Pr(D_i=1)}+\frac{\var{Y_i^0|D_i=0}}{(1-\Pr(D_i=1))}\right)},
\end{align*}\]</span></p>
<p>where the second equality stems from the fact that <span class="math inline">\(\frac{(1-\Pr(D_i=1))^N}{\Pr(D_i=1)}\var{Y_i^1|D_i=1}+\frac{\Pr(D_i=1)^N}{(1-\Pr(D_i=1))}\var{Y_i^0|D_i=0}\geq0\)</span>.
This proves the result.</p>
</div>
<div id="proofCLT" class="section level3 hasAnchor" number="16.1.2">
<h3><span class="header-section-number">A.1.2</span> Proof of Theorem <a href="FPSI.html#thm:asympnoiseWW">2.5</a><a href="proofs.html#proofCLT" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before proving Theorem <a href="FPSI.html#thm:asympnoiseWW">2.5</a>, let me state a very useful result: <span class="math inline">\(\hat{WW}\)</span> can be computed using OLS:</p>
<div class="lemma">
<p><span id="lem:WWOLS" class="lemma"><strong>Lemma A.3  (WW is OLS) </strong></span>Under Assumption <a href="FPSI.html#hyp:fullrank">2.1</a>, the OLS coefficient <span class="math inline">\(\beta\)</span> in the following regression:</p>
<p><span class="math display">\[\begin{align*}
        Y_i &amp;  = \alpha +  \beta D_i + U_i
    \end{align*}\]</span></p>
<p>is the WW estimator:</p>
<p><span class="math display">\[\begin{align*}
\hat{\beta}_{OLS} &amp; = \frac{\frac{1}{N}\sum_{i=1}^N\left(Y_i-\frac{1}{N}\sum_{i=1}^NY_i\right)\left(D_i-\frac{1}{N}\sum_{i=1}^ND_i\right)}{\frac{1}{N}\sum_{i=1}^N\left(D_i-\frac{1}{N}\sum_{i=1}^ND_i\right)^2} \\
                                &amp; = \hat{\Delta^Y_{WW}}.
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-184" class="proof"><em>Proof</em>. </span>In matrix notation, we have:</p>
<p><span class="math display">\[\begin{align*}
  \underbrace{\left(\begin{array}{c}  Y_1 \\    \vdots \\   Y_N \end{array}\right)}_{Y} &amp; = 
  \underbrace{\left(\begin{array}{cc}   1 &amp; D_1\\   \vdots &amp; \vdots\\   1 &amp; D_N\end{array}\right)}_{X}
  \underbrace{\left(\begin{array}{c}    \alpha \\   \beta \end{array}\right)}_{\Theta}+
  \underbrace{\left(\begin{array}{c}    U_1 \\  \vdots \\   U_N \end{array}\right)}_{U}
\end{align*}\]</span></p>
<p>The OLS estimator is:</p>
<p><span class="math display">\[\begin{align*}
    \hat{\Theta}_{OLS} &amp;  = (X&#39;X)^{-1}X&#39;Y
\end{align*}\]</span></p>
<p>Under the Full Rank Assumption, <span class="math inline">\(X&#39;X\)</span> is invertible and we have:</p>
<p><span class="math display">\[\begin{align*}
(X&#39;X)^{-1} &amp;  = \left(\begin{array}{cc} N &amp; \sum_{i=1}^ND_i \\ \sum_{i=1}^ND_i &amp; \sum_{i=1}^ND_i^2 \end{array}\right)^{-1} \\
                &amp; = \frac{1}{N\sum_{i=1}^ND_i^2-\left(\sum_{i=1}^ND_i\right)^2}\left(\begin{array}{cc} \sum_{i=1}^ND_i^2 &amp; -\sum_{i=1}^ND_i \\ -\sum_{i=1}^ND_i &amp; N \end{array}\right)
\end{align*}\]</span></p>
<p>For simplicity, I omit the summation index:</p>
<p><span class="math display">\[\begin{align*}
  \hat{\Theta}_{OLS} &amp;  = \frac{1}{N\sum D_i^2-\left(\sum D_i\right)^2}
                          \left(\begin{array}{cc} \sum D_i^2 &amp; -\sum D_i \\ -\sum D_i &amp; N \end{array}\right)
                          \left(\begin{array}{c} \sum Y_i \\  \sum Y_iD_i \end{array}\right) \\
                    &amp; = \frac{1}{N\sum D_i^2-\left(\sum D_i\right)^2}
                        \left(\begin{array}{c} \sum D_i^2\sum Y_i-\sum D_i\sum_{i=1}^NY_iD_i \\
                                              -\sum D_i\sum Y_i+ N\sum Y_iD_i \end{array}\right) \\
\end{align*}\]</span></p>
<p>Using <span class="math inline">\(D_i^2=D_i\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
  \hat{\Theta}_{OLS} &amp;  =  \left(\begin{array}{c} 
          \frac{\left(\sum D_i\right)\left(\sum Y_i-\sum Y_iD_i\right)}{\left(\sum D_i\right)\left(N-\sum D_i\right)} \\
          \frac{N\sum Y_iD_i-\sum D_i\sum Y_i}{N\sum D_i-\left(\sum D_i\right)^2} 
                            \end{array}\right) 
                        =     \left(\begin{array}{c} 
          \frac{\sum (Y_iD_i+Y_i(1-D_i))-\sum Y_iD_i}{\sum(1-D_i)} \\
          \frac{N^2}{N^2}\frac{\frac{1}{N}\sum Y_iD_i-\frac{1}{N}\sum D_i\frac{1}{N}\sum Y_i+\frac{1}{N}\sum D_i\frac{1}{N}\sum Y_i-\frac{1}{N}\sum D_i\frac{1}{N}\sum Y_i}{\frac{1}{N}\sum D_i-2\left(\frac{1}{N}\sum D_i\right)^2+\left(\frac{1}{N}\sum D_i\right)^2} 
                            \end{array}\right) \\
                      &amp;  =     \left(\begin{array}{c} 
          \frac{\sum Y_i(1-D_i)}{\sum(1-D_i)} \\
          \frac{\frac{1}{N}\sum \left(Y_iD_i-D_i\frac{1}{N}\sum Y_i-Y_i\frac{1}{N}\sum D_i+\frac{1}{N}\sum D_i\frac{1}{N}\sum Y_i\right)}{\frac{1}{N}\sum\left(D_i-2D_i\frac{1}{N}\sum D_i+\left(\frac{1}{N}\sum D_i\right)^2\right)} 
                            \end{array}\right) 
                      =     \left(\begin{array}{c} 
          \frac{\sum Y_i(1-D_i)}{\sum(1-D_i)} \\
    \frac{\frac{1}{N}\sum\left(Y_i-\frac{1}{N}\sum Y_i\right)\left(D_i-\frac{1}{N}\sum D_i\right)}{\frac{1}{N}\sum \left(D_i-\frac{1}{N}\sum D_i\right)^2} 
                            \end{array}\right), 
\end{align*}\]</span></p>
<p>which proves the first part of the lemma.
Now for the second part of the lemma:</p>
<p><span class="math display">\[\begin{align*}
  \hat{\beta}_{OLS} &amp;  = \frac{\sum Y_iD_i-\frac{1}{N}\sum D_i\sum Y_i}{\sum D_i\left(1-\frac{1}{N}\sum D_i\right)}
                       = \frac{\sum Y_iD_i-\frac{1}{N}\sum D_i\sum\left(Y_iD_i+(1-D_i)Y_i\right)}{\sum D_i\left(1-\frac{1}{N}\sum D_i\right)}\\
                    &amp;  = \frac{\sum Y_iD_i\left(1-\frac{1}{N}\sum D_i\right)-\frac{1}{N}\sum D_i\sum(1-D_i)Y_i}{\sum D_i\left(1-\frac{1}{N}\sum D_i\right)}\\
                    &amp;  = \frac{\sum Y_iD_i}{\sum D_i}-\frac{\frac{1}{N}\sum(1-D_i)Y_i}{\left(1-\frac{1}{N}\sum D_i\right)}\\
                    &amp;  = \frac{\sum Y_iD_i}{\sum D_i}-\frac{\frac{1}{N}\sum(1-D_i)Y_i}{\frac{1}{N}\sum\left(1-D_i\right)}\\
                     &amp;  = \frac{\sum Y_iD_i}{\sum D_i}-\frac{\sum(1-D_i)Y_i}{\sum\left(1-D_i\right)}\\
                     &amp; = \hat{\Delta^Y_{WW}},
\end{align*}\]</span></p>
<p>which proves the result.</p>
</div>
<p>Now, let me state the most important lemma behind the result in Theorem <a href="FPSI.html#thm:asympnoiseWW">2.5</a>:</p>
<div class="lemma">
<p><span id="lem:asympOLS" class="lemma"><strong>Lemma A.4  (Asymptotic Distribution of the OLS Estimator) </strong></span>Under Assumptions <a href="FPCI.html#def:noselb">1.7</a>, <a href="FPSI.html#hyp:fullrank">2.1</a>, <a href="FPSI.html#hyp:iid">2.2</a> and <a href="FPSI.html#hyp:finitevar">2.3</a>, we have:</p>
<p><span class="math display">\[\begin{align*}
  \sqrt{N}(\hat{\Theta}_{OLS}-\Theta) &amp;  \stackrel{d}{\rightarrow}
  \mathcal{N}\left(\begin{array}{c} 0\\ 0\end{array},
  \sigma_{XX}^{-1}\mathbf{V_{xu}}\sigma_{XX}^{-1}\right), 
\end{align*}\]</span></p>
<p>with
<span class="math display">\[\begin{align*}
\sigma_{XX}^{-1}&amp; = \left(\begin{array}{cc} \frac{\Pr(D_i=1)}{\Pr(D_i=1)(1-\Pr(D_i=1))} &amp; -\frac{\Pr(D_i=1)}{\Pr(D_i=1)(1-\Pr(D_i=1))}\\
                                          -\frac{\Pr(D_i=1)}{\Pr(D_i=1)(1-\Pr(D_i=1))} &amp; \frac{1}{\Pr(D_i=1)(1-\Pr(D_i=1))} 
                          \end{array}\right)\\
\mathbf{V_{xu}}&amp;= \esp{U_i^2\left(\begin{array}{cc}  1 &amp; D_i\\  D_i &amp; D_i\end{array}\right)}                        
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-185" class="proof"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
\sqrt{N}(\hat{\Theta}_{OLS}-\Theta) &amp; = \sqrt{N}((X&#39;X)^{-1}X&#39;Y-\Theta) \\
                                    &amp; = \sqrt{N}((X&#39;X)^{-1}X&#39;(X\Theta+U)-\Theta) \\
                                    &amp; = \sqrt{N}((X&#39;X)^{-1}X&#39;X\Theta+(X&#39;X)^{-1}X&#39;U)-\Theta) \\
                                    &amp; = \sqrt{N}(X&#39;X)^{-1}X&#39;U \\
                                    &amp; = N(X&#39;X)^{-1}\frac{\sqrt{N}}{N}X&#39;U
\end{align*}\]</span></p>
<p>Using Slutskyâs Theorem, we can study both terms separately.</p>
<p>Before stating Slutskyâs Theorem, we need to define a new term: convergence in probability (this is a simpler version of convergence in distribution).
We say that a sequence <span class="math inline">\(X_N\)</span> converges in probability to the constant <span class="math inline">\(x\)</span> if, <span class="math inline">\(\forall\epsilon&gt;0\)</span>, <span class="math inline">\(\lim_{N\rightarrow\infty}\Pr(|X_N-x|&gt;\epsilon)=0\)</span>.<br />
We denote <span class="math inline">\(X_N\stackrel{p}{\rightarrow}x\)</span> or <span class="math inline">\(\text{plim}(X_N)=x\)</span>.</p>
<p>Slutskyâs Theorem states that if <span class="math inline">\(Y_N\stackrel{d}{\rightarrow}y\)</span> and <span class="math inline">\(\text{plim}(X_N)=x\)</span>, then:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(X_N+Y_N\stackrel{d}{\rightarrow}x+y\)</span></li>
<li><span class="math inline">\(X_NY_N\stackrel{d}{\rightarrow}xy\)</span></li>
<li><span class="math inline">\(\frac{Y_N}{X_N}\stackrel{d}{\rightarrow}\frac{x}{y}\)</span> if <span class="math inline">\(x\neq0\)</span></li>
</ol>
<p>Using this theorem, we have:</p>
<p><span class="math display">\[\begin{align*}
\sqrt{N}(\hat{\Theta}_{OLS}-\Theta) &amp; \stackrel{d}{\rightarrow} \sigma_{XX}^{-1}xu,
\end{align*}\]</span></p>
<p>Where <span class="math inline">\(\sigma_{XX}^{-1}\)</span> is a matrix of constants and <span class="math inline">\(xu\)</span> is a random variable.</p>
<p>Letâs begin with <span class="math inline">\(\frac{\sqrt{N}}{N}X&#39;U\stackrel{d}{\rightarrow}xu\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\frac{\sqrt{N}}{N}X&#39;U &amp; = \sqrt{N}\left(\begin{array}{c}  \frac{1}{N}\sum_{i=1}^{N}U_i\\  \frac{1}{N}\sum_{i=1}^{N}D_iU_i\end{array}\right)
\end{align*}\]</span></p>
<p>In order to determine the asymptotic distribution of <span class="math inline">\(\frac{\sqrt{N}}{N}X&#39;U\)</span>, we are going to use the vector version of the CLT:</p>
<p>If <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span> are two i.i.d. random variables with finite first and second moments, we have:</p>
<p><span class="math display">\[\begin{align*}
    \sqrt{N}
  \left(
      \begin{array}{c}  
       \frac{1}{N}\sum_{i=1}^NX_i-\esp{X_i}\\   
       \frac{1}{N}\sum_{i=1}^NY_i-\esp{Y_i}
       \end{array}
     \right) 
      &amp;
  \stackrel{d}{\rightarrow}
  \mathcal{N}
  \left(
    \begin{array}{c}    
    0\\
    0
    \end{array},
  \mathbf{V}
  \right),
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\mathbf{V}\)</span> is the population covariance matrix of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span>.</p>
<p>We know that, under Assumption <a href="FPCI.html#def:noselb">1.7</a>, both random variables have mean zero:</p>
<p><span class="math display">\[\begin{align*}
\esp{U_i}&amp; = \esp{U_i|D_i=1}\Pr(D_i=1)+\esp{U_i|D_i=0}\Pr(D_i=0)=0 \\
\esp{U_iD_i}&amp; = \esp{U_i|D_i=1}\Pr(D_i=1)=0
\end{align*}\]</span></p>
<p>Their covariance matrix <span class="math inline">\(\mathbf{V_{xu}}\)</span> can be computed as follows:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{V_{xu}} &amp; = \esp{\left(\begin{array}{c}  U_i\\  UiD_i\end{array}\right)\left(\begin{array}{cc}  U_i&amp;    UiD_i\end{array}\right)}
                  - \esp{\left(\begin{array}{c} U_i\\   UiD_i\end{array}\right)}\esp{\left(\begin{array}{cc}    U_i&amp;    UiD_i\end{array}\right)}\\
                &amp; = \esp{\left(\begin{array}{cc}    U_i^2 &amp; U_i^2D_i\\  Ui^2D_i &amp; U_i^2D_i^2\end{array}\right)} 
                  = \esp{U_i^2\left(\begin{array}{cc}   1 &amp; D_i\\   D_i &amp; D_i^2\end{array}\right)} 
                  = \esp{U_i^2\left(\begin{array}{cc}   1 &amp; D_i\\   D_i &amp; D_i\end{array}\right)} 
\end{align*}\]</span></p>
<p>Using the Vector CLT, we have that <span class="math inline">\(\frac{\sqrt{N}}{N}X&#39;U\stackrel{d}{\rightarrow}\mathcal{N}\left(\begin{array}{c} 0\\ 0\end{array},\mathbf{V_{xu}}\right)\)</span>.</p>
<p>Letâs show now that <span class="math inline">\(\plims N(X&#39;X)^{-1}=\sigma_{XX}^{-1}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
N(X&#39;X)^{-1} &amp; = \frac{N}{N\sum_{i=1}^ND_i-\left(\sum_{i=1}^ND_i\right)^2}
                \left(\begin{array}{cc} \sum_{i=1}^ND_i &amp; -\sum_{i=1}^ND_i \\ -\sum_{i=1}^ND_i &amp; N \end{array}\right) \\
            &amp; = \frac{1}{N}\frac{1}{\frac{1}{N}\sum_{i=1}^ND_i-\left(\frac{1}{N}\sum_{i=1}^ND_i\right)^2}
                \left(\begin{array}{cc} \sum_{i=1}^ND_i &amp; -\sum_{i=1}^ND_i \\ -\sum_{i=1}^ND_i &amp; N \end{array}\right)\\
            &amp; = \frac{1}{\frac{1}{N}\sum_{i=1}^ND_i-\left(\frac{1}{N}\sum_{i=1}^ND_i\right)^2}
                \left(\begin{array}{cc} \frac{1}{N}\sum_{i=1}^ND_i &amp; -\frac{1}{N}\sum_{i=1}^ND_i \\ -\frac{1}{N}\sum_{i=1}^ND_i &amp; 1 \end{array}\right)\\
\plims N(X&#39;X)^{-1} &amp; = \frac{1}{\plims\frac{1}{N}\sum_{i=1}^ND_i-\left(\plims\frac{1}{N}\sum_{i=1}^ND_i\right)^2}
                \left(\begin{array}{cc} \plims\frac{1}{N}\sum_{i=1}^ND_i &amp; -\plims\frac{1}{N}\sum_{i=1}^ND_i \\ -\plims\frac{1}{N}\sum_{i=1}^ND_i &amp; 1 \end{array}\right)\\
                  &amp; = \frac{1}{\Pr(D_i=1)-\Pr(D_i=1)^2}
                \left(\begin{array}{cc} \Pr(D_i=1) &amp; -\Pr(D_i=1) \\ -\Pr(D_i=1) &amp; 1 \end{array}\right)\\
                 &amp; = \sigma_{XX}^{-1}
\end{align*}\]</span></p>
<p>The fourth equality uses Slutskyâs Theorem.
The fifth equality uses the Law of Large Numbers (LLN): if <span class="math inline">\(Y_i\)</span> are i.i.d. variables with finite first and second moments, <span class="math inline">\(\plim{N}\frac{1}{N}\sum_{i=1}^NY_i = \esp{Y_i}\)</span>.</p>
<p>In order to complete the proof, we have to use the Delta Method Theorem.
This theorem states that:</p>
<p><span class="math display">\[\begin{gather*}
  \sqrt{N}(\begin{array}{c} \bar{X}_N-\esp{X_i}\\   \bar{Y}_N-\esp{Y_i}\end{array})  \stackrel{d}{\rightarrow}\mathcal{N}(\begin{array}{c}  0\\ 0\end{array},\mathbf{V}) \\
\Rightarrow \sqrt{N}(g(\bar{X}_N,\bar{Y}_N)-g(\esp{X_i},\esp{Y_i})  \stackrel{d}{\rightarrow}\mathcal{N}(0,G&#39;\mathbf{V}G)
\end{gather*}\]</span></p>
<p>where <span class="math inline">\(G(u)=\partder{g(u)}{u}\)</span> and <span class="math inline">\(G=G(\esp{X_i},\esp{Y_i})\)</span>.</p>
<p>In our case, <span class="math inline">\(g(xu)=\sigma_{XX}^{-1}xu\)</span>, so <span class="math inline">\(G(xu)=\sigma_{XX}^{-1}\)</span>.
The results follows from that and from the symmetry of <span class="math inline">\(\sigma_{XX}^{-1}\)</span>.</p>
</div>
<p>A last lemma uses the previous result to derive the asymptotic distribution of <span class="math inline">\(\hat{WW}\)</span>:</p>
<div class="lemma">
<p><span id="lem:asymWW" class="lemma"><strong>Lemma A.5  (Asymptotic Distribution of $\hat{WW}$) </strong></span>Under Assumptions <a href="FPCI.html#def:noselb">1.7</a>, <a href="FPSI.html#hyp:fullrank">2.1</a>, <a href="FPSI.html#hyp:iid">2.2</a> and <a href="FPSI.html#hyp:finitevar">2.3</a>, we have:</p>
<p><span class="math display">\[\begin{align*}
  \sqrt{N}(\hat{\Delta^Y_{WW}}-\Delta^Y_{TT}) &amp;  \stackrel{d}{\rightarrow}
  \mathcal{N}\left(0,\frac{\var{Y_i^1|D_i=1}}{\Pr(D_i=1)}+\frac{\var{Y_i^0|D_i=0}}{1-\Pr(D_i=1)}\right).
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-186" class="proof"><em>Proof</em>. </span>In order to derive the asymptotic distribution of WW, I use first Lemma <a href="proofs.html#lem:WWOLS">A.3</a> which implies that the asymptotic distribution of WW is the same as that of <span class="math inline">\(\hat{\beta}_{OLS}\)</span>.
Now, from Lemma <a href="proofs.html#lem:asympOLS">A.4</a>, we know that <span class="math inline">\(\sqrt{N}(\hat{\beta}_{OLS}-\beta)\stackrel{d}{\rightarrow}\mathcal{N}(0,\sigma^2_{\beta})\)</span>, where <span class="math inline">\(\sigma^2_{\beta}\)</span> is the lower diagonal term of <span class="math inline">\(\sigma_{XX}^{-1}\mathbf{V_{xu}}\sigma_{XX}^{-1}\)</span>.
Using the convention <span class="math inline">\(p=\Pr(D_i=1)\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
\sigma_{XX}^{-1}\mathbf{V_{xu}}\sigma_{XX}^{-1} 
                  &amp; = \left(\begin{array}{cc}  
                                          \frac{p}{p(1-p)} &amp; -\frac{p}{p(1-p)}\\
                                          -\frac{p}{p(1-p)} &amp; \frac{1}{p(1-p)} 
                          \end{array}\right)
                          \esp{U_i^2\left(\begin{array}{cc}  1 &amp; D_i\\  D_i &amp; D_i\end{array}\right)}        
                          \left(\begin{array}{cc}  
                                          \frac{p}{p(1-p)} &amp; -\frac{p}{p(1-p)}\\
                                          -\frac{p}{p(1-p)} &amp; \frac{1}{p(1-p)} 
                          \end{array}\right)\\
                  &amp; = \frac{1}{(p(1-p))^2}
                          \left(\begin{array}{cc}  
                                          p\esp{U_i^2}-p\esp{U_i^2D_i} &amp; p\esp{U_i^2D_i}-p\esp{U_i^2D_i}\\
                                          -p\esp{U_i^2}+\esp{U_i^2D_i} &amp;  -p\esp{U_i^2D_i}+\esp{U_i^2D_i}
                          \end{array}\right)
                         \left(\begin{array}{cc}  
                                          p &amp; -p\\
                                          -p &amp; 1 
                          \end{array}\right)\\
                 &amp; = \frac{1}{(p(1-p))^2}
                          \left(\begin{array}{cc}  
                                          p^2(\esp{U_i^2}-\esp{U_i^2D_i}) &amp; p^2(\esp{U_i^2D_i}-\esp{U_i^2})\\
                                          p^2(\esp{U_i^2D_i}-\esp{U_i^2}) &amp;  p^2\esp{U_i^2}+(1-2p)\esp{U_i^2D_i}
                          \end{array}\right)
 \end{align*}\]</span></p>
<p>The final result comes from the fact that:</p>
<p><span class="math display">\[\begin{align*}
\esp{U_i^2} &amp; = \esp{U_i^2|D_i=1}p + (1-p)\esp{U_i^2|D_i=0}\\
            &amp; = p\var{Y_i^1|D_i=1}+(1-p)\var{Y_i^0|D_i=0} \\
\esp{U_i^2D_i}  &amp; = \esp{U_i^2|D_i=1}p  \\
                &amp; = p\var{Y_i^1|D_i=1}.
 \end{align*}\]</span></p>
<p>As a consequence:</p>
<p><span class="math display">\[\begin{align*}
\sigma^2_{\beta} &amp;= \frac{1}{(p(1-p))^2}\left(\var{Y_i^1|D_i=1}p(p^2-2p+1) + p^2(1-p)\var{Y_i^0|D_i=0}\right) \\
                  &amp;= \frac{1}{(p(1-p))^2}\left(\var{Y_i^1|D_i=1}p(1-p)^2 + p^2(1-p)\var{Y_i^0|D_i=0}\right)\\
                  &amp; = \frac{\var{Y_i^1|D_i=1}}{p}+\frac{\var{Y_i^0|D_i=0}}{1-p}.
 \end{align*}\]</span></p>
</div>
<p>Using the previous lemma, we can now approximate the confidence level of <span class="math inline">\(\hat{WW}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\Pr&amp;(|\hat{\Delta^Y_{WW}}-\Delta^Y_{TT}|\leq\epsilon) = \Pr(-\epsilon\leq\hat{\Delta^Y_{WW}}-\Delta^Y_{TT}\leq\epsilon) \\
&amp; = \Pr\left(-\frac{\epsilon}{\frac{1}{\sqrt{N}}\sqrt{\frac{\var{Y_i^1|D_i=1}}{\Pr(D_i=1)}+\frac{\var{Y_i^0|D_i=0}}{1-\Pr(D_i=1)}}}\leq\frac{\hat{\Delta^Y_{WW}}-\Delta^Y_{TT}}{\frac{1}{\sqrt{N}}\sqrt{\frac{\var{Y_i^1|D_i=1}}{\Pr(D_i=1)}+\frac{\var{Y_i^0|D_i=0}}{1-\Pr(D_i=1)}}}\leq\frac{\epsilon}{\frac{1}{\sqrt{N}}\sqrt{\frac{\var{Y_i^1|D_i=1}}{\Pr(D_i=1)}+\frac{\var{Y_i^0|D_i=0}}{1-\Pr(D_i=1)}}}\right)\\
&amp; \approx \Phi\left(\frac{\epsilon}{\frac{1}{\sqrt{N}}\sqrt{\frac{\var{Y_i^1|D_i=1}}{\Pr(D_i=1)}+\frac{\var{Y_i^0|D_i=0}}{1-\Pr(D_i=1)}}}\right)-
\Phi\left(-\frac{\epsilon}{\frac{1}{\sqrt{N}}\sqrt{\frac{\var{Y_i^1|D_i=1}}{\Pr(D_i=1)}+\frac{\var{Y_i^0|D_i=0}}{1-\Pr(D_i=1)}}}\right)\\
&amp; = \Phi\left(\frac{\epsilon}{\frac{1}{\sqrt{N}}\sqrt{\frac{\var{Y_i^1|D_i=1}}{\Pr(D_i=1)}+\frac{\var{Y_i^0|D_i=0}}{1-\Pr(D_i=1)}}}\right)- 1 + \Phi\left(\frac{\epsilon}{\frac{1}{\sqrt{N}}\sqrt{\frac{\var{Y_i^1|D_i=1}}{\Pr(D_i=1)}+\frac{\var{Y_i^0|D_i=0}}{1-\Pr(D_i=1)}}}\right)\\
&amp; = 2\Phi\left(\frac{\epsilon}{\frac{1}{\sqrt{N}}\sqrt{\frac{\var{Y_i^1|D_i=1}}{\Pr(D_i=1)}+\frac{\var{Y_i^0|D_i=0}}{1-\Pr(D_i=1)}}}\right)-1.
\end{align*}\]</span></p>
<p>As a consequence,</p>
<p><span class="math display">\[\begin{align*}
\delta &amp; \approx 2\Phi\left(\frac{\epsilon}{\frac{1}{\sqrt{N}}\sqrt{\frac{\var{Y_i^1|D_i=1}}{\Pr(D_i=1)}+\frac{\var{Y_i^0|D_i=0}}{1-\Pr(D_i=1)}}}\right)-1.
\end{align*}\]</span></p>
<p>Hence the result.</p>
</div>
</div>
<div id="proofs-of-results-in-chapter-refrct" class="section level2 hasAnchor" number="16.2">
<h2><span class="header-section-number">A.2</span> Proofs of results in Chapter <a href="RCT.html#RCT">3</a><a href="proofs.html#proofs-of-results-in-chapter-refrct" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="proofIdentLATE" class="section level3 hasAnchor" number="16.2.1">
<h3><span class="header-section-number">A.2.1</span> Proof of Theorem <a href="RCT.html#thm:IdentLATE">3.9</a><a href="proofs.html#proofIdentLATE" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In order to prove the theorem, it is going to be very helpful to prove the following lemma:</p>
<div class="lemma">
<p><span id="lem:UnconfTypes" class="lemma"><strong>Lemma A.6  (Unconfounded Types) </strong></span>Under Assumptions <a href="RCT.html#def:RandEncouragValid">3.9</a> and <a href="RCT.html#def:IndepEncourag">3.10</a>, the types <span class="math inline">\(T_i\)</span> are independent of the allocation of the treatment:</p>
<p><span class="math display">\[\begin{align*}
(Y_i^{1,1},Y_i^{0,1},Y_i^{0,0},Y_i^{1,0},T_i)\Ind R_i|E_i=1.
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-187" class="proof"><em>Proof</em>. </span>Lemma 4.2 in <a href="https://www.jstor.org/stable/2984718">Dawid (1979)</a> shows that if <span class="math inline">\(X\Ind Y|Z\)</span> and <span class="math inline">\(U\)</span> is a function of <span class="math inline">\(X\)</span> then <span class="math inline">\(U\Ind Y|Z\)</span>.
The fact that <span class="math inline">\(T_i\)</span> is a function of <span class="math inline">\((D_i^1,D^0_i)\)</span> proves the result.</p>
</div>
<p>The four sets defined by <span class="math inline">\(T_i\)</span> are a partition of the sample space.
As a consequence, we have (ommitting the conditioning on <span class="math inline">\(E_i=1\)</span> all along for simplicity):</p>
<p><span class="math display">\[\begin{align*}
\esp{Y_i|R_i=1} &amp; = \esp{Y_i|T_i=a,R_i=1}\Pr(T_i=a|R_i=1)\\
                &amp; \phantom{=}+ \esp{Y_i|T_i=c,R_i=1}\Pr(T_i=c|R_i=1) \\
                            &amp; \phantom{=} + \esp{Y_i|T_i=d,R_i=1}\Pr(T_i=d|R_i=1)\\
                            &amp; \phantom{=} + \esp{Y_i|T_i=n,R_i=1}\Pr(T_i=n|R_i=1)\\
\esp{Y_i|R_i=0} &amp; = \esp{Y_i|T_i=a,R_i=0}\Pr(T_i=a|R_i=0)\\
                &amp; \phantom{=} + \esp{Y_i|T_i=c,R_i=0}\Pr(T_i=c|R_i=0) \\
                            &amp; \phantom{=} + \esp{Y_i|T_i=d,R_i=0}\Pr(T_i=d|R_i=0)\\
                            &amp; \phantom{=}+ \esp{Y_i|T_i=n,R_i=0}\Pr(T_i=n|R_i=0).
\end{align*}\]</span></p>
<p>Letâs look at all these terms in turn:</p>
<p><span class="math display">\[\begin{align*}
  \esp{Y_i|T_i=a,R_i=1} &amp; =   \esp{Y_i^{1,1}D_iR_i+Y_i^{1,0}D_i(1-R_i)+Y_i^{0,1}(1-D_i)R_i+Y_i^{0,0}(1-D_i)(1-R_i)|T_i=a,R_i=1} \\
   &amp; =   \esp{Y_i^{1,1}(D^1_iR_i+D_i^0(1-R_i))R_i+Y_i^{0,1}(1-(D^1_iR_i+D_i^0(1-R_i)))R_i|T_i=a,R_i=1} \\
   &amp; =   \esp{Y_i^{1,1}D^1_iR_i^2+Y_i^{0,1}(1-D^1_iR_i)R_i|D_i^1=D_i^0=1,R_i=1} \\
   &amp; =   \esp{Y_i^{1,1}|T_i=a,R_i=1} \\
   &amp; =   \esp{Y_i^{1,1}|T_i=a}, \\
\end{align*}\]</span></p>
<p>where the first equality uses Assumption <a href="RCT.html#def:RandEncouragValid">3.9</a>, the second equality uses the fact that <span class="math inline">\(R_i=1\)</span> in the conditional expectation and Assumption <a href="RCT.html#def:RandEncouragValid">3.9</a>, the third equality uses the fact that <span class="math inline">\(R_i=1\)</span>, the fourth equality uses the fact that <span class="math inline">\(T_i=a \Leftrightarrow D_i^1=D_i^0=1\)</span> and the last equality uses Lemma <a href="proofs.html#lem:UnconfTypes">A.6</a>.</p>
<p>Using a similar reasoning, we have:</p>
<p><span class="math display">\[\begin{align*}
  \esp{Y_i|T_i=c,R_i=1} &amp; = \esp{Y_i^{1,1}|T_i=c} \\
  \esp{Y_i|T_i=d,R_i=1} &amp; = \esp{Y_i^{0,1}|T_i=d} \\
  \esp{Y_i|T_i=n,R_i=1} &amp; = \esp{Y_i^{0,1}|T_i=n} \\
  \esp{Y_i|T_i=a,R_i=0} &amp; = \esp{Y_i^{1,0}|T_i=c} \\
  \esp{Y_i|T_i=c,R_i=0} &amp; = \esp{Y_i^{0,0}|T_i=c} \\
  \esp{Y_i|T_i=d,R_i=0} &amp; = \esp{Y_i^{1,0}|T_i=d} \\
  \esp{Y_i|T_i=n,R_i=0} &amp; = \esp{Y_i^{0,0}|T_i=n}.
\end{align*}\]</span></p>
<p>Also, Lemma <a href="proofs.html#lem:UnconfTypes">A.6</a> implies that <span class="math inline">\(\Pr(T_i=a|R_i)=\Pr(T_i=a)\)</span>, and the same is true for all other types.
As a consequence, we have:</p>
<p><span class="math display">\[\begin{align*}
\esp{Y_i|R_i=1} &amp; = \esp{Y_i^{1,1}|T_i=a}\Pr(T_i=a)\\
                &amp; \phantom{=} + \esp{Y_i^{1,1}|T_i=c}\Pr(T_i=c) \\
                            &amp; \phantom{=} + \esp{Y_i^{0,1}|T_i=d}\Pr(T_i=d)\\
                            &amp; \phantom{=} + \esp{Y_i^{0,1}|T_i=n}\Pr(T_i=n)\\
\esp{Y_i|R_i=0} &amp; = \esp{Y_i^{1,0}|T_i=a}\Pr(T_i=a)\\
                &amp; \phantom{=} + \esp{Y_i^{0,0}|T_i=c}\Pr(T_i=c) \\                      
                                &amp; \phantom{=} + \esp{Y_i^{1,0}|T_i=d}\Pr(T_i=d)\\
                                &amp; \phantom{=} + \esp{Y_i^{0,0}|T_i=n}\Pr(T_i=n).
\end{align*}\]</span></p>
<p>And thus:</p>
<p><span class="math display">\[\begin{align*}
\esp{Y_i|R_i=1}-\esp{Y_i|R_i=0} &amp; = (\esp{Y_i^{1,1}|T_i=a}-\esp{Y_i^{1,0}|T_i=a})\Pr(T_i=a)\\
                                &amp; \phantom{=}+ (\esp{Y_i^{1,1}|T_i=c}-\esp{Y_i^{0,0}|T_i=c})\Pr(T_i=c) \\
                                            &amp; \phantom{=} - (\esp{Y_i^{1,0}|T_i=d}-\esp{Y_i^{0,1}|T_i=d})\Pr(T_i=d)\\
                                            &amp; \phantom{=} + (\esp{Y_i^{0,1}|T_i=n}-\esp{Y_i^{0,0}|T_i=n})\Pr(T_i=n).
\end{align*}\]</span></p>
<p>Using Assumption <a href="RCT.html#def:ExclRestr">3.11</a>, we have:</p>
<p><span class="math display">\[\begin{align*}
\esp{Y_i|R_i=1}-\esp{Y_i|R_i=0} &amp; = (\esp{Y_i^{1}|T_i=a}-\esp{Y_i^{1}|T_i=a})\Pr(T_i=a)\\
                                &amp; \phantom{=}+ (\esp{Y_i^{1}|T_i=c}-\esp{Y_i^{0}|T_i=c})\Pr(T_i=c) \\
                                            &amp; \phantom{=} - (\esp{Y_i^{1}|T_i=d}-\esp{Y_i^{0}|T_i=d})\Pr(T_i=d)\\
                                            &amp; \phantom{=} + (\esp{Y_i^{0}|T_i=n}-\esp{Y_i^{0}|T_i=n})\Pr(T_i=n)\\
                                            &amp; = \esp{Y_i^{1}-Y_i^{0}|T_i=c}\Pr(T_i=c) \\
                                            &amp; \phantom{=} - \esp{Y_i^{1}-Y_i^{0}|T_i=d}\Pr(T_i=d).
\end{align*}\]</span></p>
<p>Under Assumption <a href="RCT.html#def:Mono">3.13</a>, we have:</p>
<p><span class="math display">\[\begin{align*}
\esp{Y_i|R_i=1}-\esp{Y_i|R_i=0} &amp;  = \esp{Y_i^{1}-Y_i^{0}|T_i=c}\Pr(T_i=c)\\
                                 &amp; = \Delta^Y_{LATE}\Pr(T_i=c).
\end{align*}\]</span></p>
<p>We also have:</p>
<p><span class="math display">\[\begin{align*}
\Pr(D_i=1|R_i=1) &amp; = \Pr(D^1_i=1|R_i=1)\\
                &amp; =  \Pr(D^1_i=1\cap (D_i^0=1\cup D_i^0=0) |R_i=1)\\
                &amp; =  \Pr(D^1_i=1\cap D_i^0=1\cup D^1_i=1\cap D_i^0=0 |R_i=1)\\
                &amp; =  \Pr(D^1_i=D_i^0=1\cup D^1_i-D_i^0=0 |R_i=1)\\
                &amp; =  \Pr(T_i=a\cup T_i=c |R_i=1)\\
               &amp; =  \Pr(T_i=a|R_i=1)+\Pr(T_i=c|R_i=1)\\
               &amp; =  \Pr(T_i=a)+\Pr(T_i=c),
\end{align*}\]</span></p>
<p>where the first equality follows from Assumption <a href="RCT.html#def:RandEncouragValid">3.9</a> and the fact that <span class="math inline">\(D_i=R_iD_i^1+(1-R_i)D_i^0\)</span>, so that <span class="math inline">\(D_i|R_i=1=D_i^1\)</span>.
The second equality follows from the fact that <span class="math inline">\(\left\{ D_i^0=1,D_i^0=0\right\}\)</span> is a partition of the sample space.
The third equality follows from usual rules of logic and the fourth equality from the fact that <span class="math inline">\(D_i^1\)</span> and <span class="math inline">\(D_i^0\)</span> can only take values zero and one.
The fifth equality follows from the definition of <span class="math inline">\(T_i\)</span>.
The sixth equaity follows from the rule of addition in probability and the fact that <span class="math inline">\(T_i=a\)</span> and <span class="math inline">\(T_i=c\)</span> are disjoint.
The final equality follows from Lemma <a href="proofs.html#lem:UnconfTypes">A.6</a>.</p>
<p>Using a similar reasoning, we have:</p>
<p><span class="math display">\[\begin{align*}
\Pr(D_i=1|R_i=0) &amp; = \Pr(T_i=a)+ \Pr(T_i=d).
\end{align*}\]</span></p>
<p>As a consequence, under Assumption <a href="RCT.html#def:Mono">3.13</a>, we have:</p>
<p><span class="math display">\[\begin{align*}
\Pr(D_i=1|R_i=1)-\Pr(D_i=1|R_i=0) &amp; = \Pr(T_i=c).
\end{align*}\]</span></p>
<p>Using Assumption <a href="RCT.html#def:Fstage">3.12</a> proves the result.</p>
</div>
<div id="proofWaldIV" class="section level3 hasAnchor" number="16.2.2">
<h3><span class="header-section-number">A.2.2</span> Proof of Theorem <a href="RCT.html#thm:WaldIV">3.15</a><a href="proofs.html#proofWaldIV" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In matrix notation, we have:</p>
<p><span class="math display">\[\begin{align*}
  \underbrace{\left(\begin{array}{c}  Y_1 \\    \vdots \\   Y_N \end{array}\right)}_{Y} &amp; =
  \underbrace{\left(\begin{array}{cc}   1 &amp; D_1\\   \vdots &amp; \vdots\\   1 &amp; D_N\end{array}\right)}_{X}
  \underbrace{\left(\begin{array}{c}    \alpha \\   \beta \end{array}\right)}_{\Theta}+
  \underbrace{\left(\begin{array}{c}    U_1 \\  \vdots \\   U_N \end{array}\right)}_{U}
\end{align*}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{align*}
  \left(\begin{array}{c}  D_1 \\    \vdots \\   D_N \end{array}\right) &amp; =
  \underbrace{\left(\begin{array}{cc}   1 &amp; R_1\\   \vdots &amp; \vdots\\   1 &amp; R_N\end{array}\right)}_{R}
  \left(\begin{array}{c}    \gamma \\   \tau \end{array}\right)+
  \left(\begin{array}{c}    V_1 \\  \vdots \\   V_N \end{array}\right)
\end{align*}\]</span></p>
<p>The IV estimator is:</p>
<p><span class="math display">\[\begin{align*}
    \hat{\Theta}_{IV} &amp;  = (R&#39;X)^{-1}R&#39;Y
\end{align*}\]</span></p>
<p>If there is at least one observation with <span class="math inline">\(R_i=1\)</span> and <span class="math inline">\(D_i=1\)</span>, <span class="math inline">\(R&#39;X\)</span> is invertible (its determinant is non null) and we have (ommitting the summation index for simplicity):</p>
<p><span class="math display">\[\begin{align*}
(R&#39;X)^{-1} &amp;  = \left(\begin{array}{cc} N &amp; \sum D_i \\ \sum R_i &amp; \sum D_iR_i \end{array}\right)^{-1} \\
                &amp; = \frac{1}{N\sum D_iR_i-\sum D_i\sum R_i}\left(\begin{array}{cc} \sum D_iR_i &amp; -\sum D_i \\ -\sum R_i &amp; N \end{array}\right)
\end{align*}\]</span></p>
<p>Since:</p>
<p><span class="math display">\[\begin{align*}
R&#39;Y &amp;  = \left(\begin{array}{c} \sum Y_i \\ \sum Y_iR_i \end{array}\right),
\end{align*}\]</span></p>
<p>we have:</p>
<p><span class="math display">\[\begin{align*}
  \hat{\Theta}_{IV} &amp;  =  \left(
                              \begin{array}{c}
                                \frac{\sum Y_i\sum D_iR_i-\sum D_i\sum Y_iR_i}{N\sum D_iR_i -\sum D_iR_i}\\
                                \frac{N\sum Y_iR_i-\sum R_i\sum Y_i}{N\sum D_iR_i-\sum D_iR_i}
                              \end{array}
                          \right)
\end{align*}\]</span></p>
<p>As a consequence, <span class="math inline">\(\hat{\beta}_{IV}\)</span> is equal to the ratio of two OLS estimators (<span class="math inline">\(Y_i\)</span> on <span class="math inline">\(R_i\)</span> and a constant and <span class="math inline">\(D_i\)</span> on the same regressors) (see the proof of Lemma <a href="proofs.html#lem:WWOLS">A.3</a> in section <a href="proofs.html#proofCLT">A.1.2</a>, just after âUsing <span class="math inline">\(D_i^2=D_i\)</span>â).
We can use Lemma <a href="proofs.html#lem:WWOLS">A.3</a> stating that the OLS estimator is the WW estimator to prove the result.</p>
</div>
<div id="ProofAsymWald" class="section level3 hasAnchor" number="16.2.3">
<h3><span class="header-section-number">A.2.3</span> Proof of Theorem <a href="RCT.html#thm:asymWald">3.16</a><a href="proofs.html#ProofAsymWald" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In order to derive the asymptotic distribution of the Wald estimator, I first use Theorem <a href="RCT.html#thm:WaldIV">3.15</a> which implies that the asymptotic distribution of Wald is the same as that of <span class="math inline">\(\hat{\beta}_{IV}\)</span>.
Now, Iâm going to derive the asymptotic distribution of the IV estimator.</p>
<div class="lemma">
<p><span id="lem:asympIV" class="lemma"><strong>Lemma A.7  (Asymptotic Distribution of the IV Estimator) </strong></span>Under Independence and Validity of the Instrument, Exclusion Restriction and Full Rank, we have:</p>
<p><span class="math display">\[\begin{align*}
  \sqrt{N}(\hat{\Theta}_{IV}-\Theta) &amp;  \stackrel{d}{\rightarrow}
  \mathcal{N}\left(\begin{array}{c} 0\\ 0\end{array},
  (\sigma_{RX}^{-1})&#39;\mathbf{V_{ru}}\sigma_{RX}^{-1}\right), 
\end{align*}\]</span></p>
<p>with
<span class="math display">\[\begin{align*}
\sigma_{RX}^{-1}&amp; = \frac{\left(\begin{array}{cc}   \esp{D_iR_i} &amp; -\Pr(D_i=1)\\
                                          -\Pr(R_i=1) &amp; 1 
                          \end{array}\right)}{(\Pr(D_i=1|R_i=1)-\Pr(D_i=1|R_i=0))\Pr(R_i=1)(1-\Pr(R_i=1))}
                          \\
\mathbf{V_{ru}}&amp;= \esp{U_i^2\left(\begin{array}{cc}  1 &amp; R_i\\  R_i &amp; R_i\end{array}\right)}                                                                
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-188" class="proof"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
\sqrt{N}(\hat{\Theta}_{IV}-\Theta) &amp; = \sqrt{N}((R&#39;X)^{-1}R&#39;Y-\Theta) \\
                                    &amp; = \sqrt{N}((R&#39;X)^{-1}R&#39;(X\Theta+U)-\Theta) \\
                                    &amp; = \sqrt{N}((R&#39;X)^{-1}R&#39;X\Theta+(X&#39;X)^{-1}X&#39;U)-\Theta) \\
                                    &amp; = \sqrt{N}(R&#39;X)^{-1}R&#39;U \\
                                    &amp; = N(R&#39;X)^{-1}\frac{\sqrt{N}}{N}R&#39;U
\end{align*}\]</span></p>
<p>Using Slutskyâs Theorem, we have:</p>
<p><span class="math display">\[\begin{align*}
\sqrt{N}(\hat{\Theta}_{IV}-\Theta) &amp; \stackrel{d}{\rightarrow} \sigma_{RX}^{-1}ru,
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\sigma_{RX}^{-1}\)</span> is a matrix of constants and <span class="math inline">\(ru\)</span> is a random variable.</p>
<p>We know that <span class="math inline">\(\plims N(R&#39;X)^{-1}=\sigma_{RX}^{-1}\)</span>.
So:</p>
<p><span class="math display">\[\begin{align*}
N(R&#39;X)^{-1} &amp; = \frac{N}{N\sum D_iR_i-\sum D_i\sum R_i}\left(\begin{array}{cc} \sum D_iR_i &amp; -\sum D_i \\ -\sum R_i &amp; N \end{array}\right) \\
            &amp; = \frac{1}{\frac{\sum D_iR_i}{N}-\frac{\sum D_i}{N}\frac{\sum R_i}{N}} 
                  \left(\begin{array}{cc} 
                          \frac{\sum D_iR_i}{N} &amp; -\frac{\sum D_i}{N} \\ -\frac{\sum R_i}{N} &amp; 1 
                        \end{array}
                  \right) 
\end{align*}\]</span></p>
<p><span class="math inline">\(\frac{\sum D_iR_i}{N}-\frac{\sum D_i}{N}\frac{\sum R_i}{N}\)</span> is equal to the numerator of the OLS coefficient of a regression of <span class="math inline">\(D_i\)</span> on <span class="math inline">\(R_i\)</span> and a constant (Proof of Lemma 3 in Lecture 0).
As a consequence of Lemma 3 in Lecture 0, it can be written as the With/Without estimator multiplied by the denominator of the OLS estimator, which is simply the variance of <span class="math inline">\(R_i\)</span>.</p>
<p>Letâs turn to <span class="math inline">\(\frac{\sqrt{N}}{N}R&#39;U\stackrel{d}{\rightarrow}xu\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\frac{\sqrt{N}}{N}R&#39;U &amp; = \sqrt{N}\left(\begin{array}{c}  \frac{1}{N}\sum^{i=1}_{N}U_i\\  \frac{1}{N}\sum^{i=1}_{N}R_iU_i\end{array}\right)
\end{align*}\]</span></p>
<p>We know that, under Validity of Randomization, both random variables have mean zero:</p>
<p><span class="math display">\[\begin{align*}
\esp{U_i}&amp; = \esp{U_i|R_i=1}\Pr(R_i=1)+\esp{U_i|R_i=0}\Pr(R_i=0)=0 \\
\esp{U_iR_i}&amp; = \esp{U_i|R_i=1}\Pr(R_i=1)=0
\end{align*}\]</span></p>
<p>Their covariance matrix <span class="math inline">\(\mathbf{V_{ru}}\)</span> can be computed as follows:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{V_{ru}} &amp; = \esp{\left(\begin{array}{c}  U_i\\  UiR_i\end{array}\right)\left(\begin{array}{cc}  U_i&amp;    UiR_i\end{array}\right)}
                  - \esp{\left(\begin{array}{c} U_i\\   UiR_i\end{array}\right)}\esp{\left(\begin{array}{cc}    U_i&amp;    UiR_i\end{array}\right)}\\
                &amp; = \esp{\left(\begin{array}{cc}    U_i^2 &amp; U_i^2R_i\\  Ui^2R_i &amp; U_i^2R_i^2\end{array}\right)} 
                  = \esp{U_i^2\left(\begin{array}{cc}   1 &amp; R_i\\   R_i &amp; R_i^2\end{array}\right)} 
                  = \esp{U_i^2\left(\begin{array}{cc}   1 &amp; R_i\\   R_i &amp; R_i\end{array}\right)} 
\end{align*}\]</span></p>
<p>Using the Vector CLT, we have that <span class="math inline">\(\frac{\sqrt{N}}{N}R&#39;U\stackrel{d}{\rightarrow}\mathcal{N}\left(\begin{array}{c} 0\\ 0\end{array},\mathbf{V_{ru}}\right)\)</span>.
Using Slutskyâs theorem and the LLN gives the result.</p>
</div>
<p>From Lemma <a href="proofs.html#lem:asympIV">A.7</a>, we know that <span class="math inline">\(\sqrt{N}(\hat{\beta}_{IV}-\beta)\stackrel{d}{\rightarrow}\mathcal{N}(0,\sigma^2_{\beta})\)</span>, where <span class="math inline">\(\sigma^2_{\beta}\)</span> is the lower diagonal term of <span class="math inline">\((\sigma_{RX}^{-1})&#39;\mathbf{V_{ru}}\sigma_{RX}^{-1}\)</span>.
Using the convention <span class="math inline">\(p^R=\Pr(R_i=1)\)</span>, <span class="math inline">\(p^D=\Pr(D_i=1)\)</span>, <span class="math inline">\(p^D_1=\Pr(D_i=1|R_i=1)\)</span>, <span class="math inline">\(p^D_0=\Pr(D_i=1|R_i=0)\)</span> and <span class="math inline">\(p^{DR}=\esp{D_iR_i}\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
(&amp;\sigma_{RX}^{-1})&#39;\mathbf{V_{ru}}\sigma_{RX}^{-1} \\
                   &amp; = \frac{1}{((p^D_1-p^D_0)p^R(1-p^R))^2}
                  \left(\begin{array}{cc}
                         p^{DR}  &amp; -p^R\\
                        -p^D &amp; 1
                          \end{array}\right)
                          \esp{U_i^2\left(\begin{array}{cc}  1 &amp; R_i\\  R_i &amp; R_i\end{array}\right)}
                        \left(\begin{array}{cc}
                                         p^{DR}  &amp; -p^D\\
                                          -p^R &amp; 1
                          \end{array}\right)\\
                    &amp; = \frac{1}{((p^D_1-p^D_0)p^R(1-p^R))^2}
                         \left(\begin{array}{cc}
                         p^{DR}\esp{U_i^2}-p^R\esp{U_i^2R_i} &amp; \esp{U_i^2R_i}(p^{DR}-p^R)\\
                        \esp{U_i^2R_i}-p^D\esp{U_i^2} &amp; \esp{U_i^2R_i}(1-p^D)
                          \end{array}\right)
                          \left(\begin{array}{cc}
                                         p^{DR}  &amp; -p^D\\
                                          -p^R &amp; 1
                          \end{array}\right)\\
                    &amp; = \frac{\left(\begin{array}{cc}
                          p^{DR}(p^{DR}\esp{U_i^2}-p^R\esp{U_i^2R_i})- p^R\esp{U_i^2R_i}(p^{DR}-p^R)
                            &amp; \esp{U_i^2R_i}(p^{DR}-p^R)-p^{D}(p^{DR}\esp{U_i^2}-p^R\esp{U_i^2R_i})\\
                          p^{DR}(\esp{U_i^2R_i}-p^D\esp{U_i^2})-p^R\esp{U_i^2R_i}(1-p^D)
                            &amp; \esp{U_i^2R_i}(1-p^D) - p^{D}(\esp{U_i^2R_i}-p^D\esp{U_i^2})
                          \end{array}\right)}{((p^D_1-p^D_0)p^R(1-p^R))^2}
 \end{align*}\]</span></p>
<p>As a consequence:</p>
<p><span class="math display">\[\begin{align*}
\sigma^2_{\beta} &amp; = \frac{\esp{U_i^2R_i}(1-p^D) - p^{D}(\esp{U_i^2R_i}-p^D\esp{U_i^2})}{((p^D_1-p^D_0)p^R(1-p^R))^2} \\
                  &amp; = \frac{(p^D)^2\esp{U_i^2}+(1-2p^D)\esp{U_i^2R_i}}{((p^D_1-p^D_0)p^R(1-p^R))^2}\\
                  &amp; = \frac{(p^D)^2(\esp{U_i^2|R_i=1}p^R+\esp{U_i^2|R_i=0}(1-p^R))+(1-2p^D)\esp{U_i^2|R_i=1}p^R}{((p^D_1-p^D_0)p^R(1-p^R))^2}\\
                  &amp; = \frac{(p^D)^2\esp{U_i^2|R_i=0}(1-p^R)+(1-2p^D+(p^D)^2)\esp{U_i^2|R_i=1}p^R}{((p^D_1-p^D_0)p^R(1-p^R))^2}\\
                  &amp; = \frac{(p^D)^2\esp{U_i^2|R_i=0}(1-p^R)+(1-p^D)^2\esp{U_i^2|R_i=1}p^R}{((p^D_1-p^D_0)p^R(1-p^R))^2}\\
                  &amp; = \frac{1}{(p^D_1-p^D_0)^2}\left[\left(\frac{p^D}{p^R}\right)^2\frac{\esp{U_i^2|R_i=0}}{1-p^R}+\left(\frac{1-p^D}{1-p^R}\right)^2\frac{\esp{U_i^2|R_i=1}}{p^R}\right].
\end{align*}\]</span></p>
<p>Note that, under monotonicity, <span class="math inline">\(p^C=p^D_1-p^D_0\)</span> and:</p>
<p><span class="math display">\[\begin{align*}
\esp{U_i^2|R_i=1} &amp; = p^{AT}\var{Y_i^1|T_i=AT}+p^C\var{Y_i^1|T_i=C}+p^{NT}\var{Y_i^0|T_i=NT} \\
\esp{U_i^2|R_i=0}  &amp; = p^{AT}\var{Y_i^1|T_i=AT}+p^C\var{Y_i^0|T_i=C}+p^{NT}\var{Y_i^0|T_i=NT}.
 \end{align*}\]</span></p>
<p>The final result comes from the fact that:</p>
<p><span class="math display">\[\begin{align*}
\frac{1}{(p^C)^2} &amp;  \left[\left(\frac{p^D}{p^R}\right)^2\frac{1}{1-p^R}+\left(\frac{1-p^D}{1-p^R}\right)^2\frac{1}{p^R}\right]\\
  &amp; = \frac{(p^D)^2(1-p^R)+(1-p^D)^2p^R}{(p^Cp^R(1-p^R))^2} \\
  &amp; = \frac{(p^D)^2-(p^D)^2p^R+p^R-2p^Dp^R+(p^D)^2p^R}{(p^Cp^R(1-p^R))^2} \\
  &amp; = \frac{(p^D)^2+p^R-2p^Dp^R}{(p^Cp^R(1-p^R))^2} \\
  &amp; = \frac{(p^D-p^R)^2+p^R-(p^R)^2}{(p^Cp^R(1-p^R))^2} \\
  &amp; = \frac{(p^D-p^R)^2+p^R(1-p^R)}{(p^Cp^R(1-p^R))^2} \\
  &amp; = \frac{(p^{AT}+p^Cp^R-p^R)^2+p^R(1-p^R)}{(p^Cp^R(1-p^R))^2} \\
  &amp; = \frac{(p^{AT}+(1-p^{AT}-p^{NT})p^R-p^R)^2+p^R(1-p^R)}{(p^Cp^R(1-p^R))^2} \\
  &amp; = \frac{(p^{AT}+(1-p^{AT}-p^{NT})p^R-p^R)^2+p^R(1-p^R)}{(p^Cp^R(1-p^R))^2} \\
  &amp; = \frac{(p^{AT}+p^R-p^{AT}p^R-p^{NT}p^R-p^R)^2+p^R(1-p^R)}{(p^Cp^R(1-p^R))^2} \\
  &amp; = \frac{(p^{AT}(1-p^R)-p^{NT}p^R)^2+p^R(1-p^R)}{(p^Cp^R(1-p^R))^2},
\end{align*}\]</span></p>
<p>where the seventh equality uses the fact that <span class="math inline">\(p^C+p^{AT}+p^{NT}=1\)</span>.</p>
</div>
</div>
<div id="proofs-of-results-in-chapter-refne" class="section level2 hasAnchor" number="16.3">
<h2><span class="header-section-number">A.3</span> Proofs of results in Chapter <a href="NE.html#NE">4</a><a href="proofs.html#proofs-of-results-in-chapter-refne" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="proofEstimDID" class="section level3 hasAnchor" number="16.3.1">
<h3><span class="header-section-number">A.3.1</span> Proof of Theorem <a href="NE.html#thm:EstimDID">4.5</a><a href="proofs.html#proofEstimDID" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us start with the proof that <span class="math inline">\(\hat{\beta}^{FD}=\hat{\Delta}^Y_{DID}\)</span>.
Using Lemma <a href="proofs.html#lem:WWOLS">A.3</a>, we have that <span class="math inline">\(\hat{\beta}^{FD}=\hat{\Delta}^{Y_A-Y_B}_{WW}\)</span>.
From there, since <span class="math inline">\(\sum_{i=1}^N(Y_{i,A}-Y_{i,B})D_i= \sum_{i=1}^NY_{i,A}D_i- \sum_{i=1}^NY_{i,B}D_i\)</span>, we have <span class="math inline">\(\hat{\beta}^{FD}=\hat{\Delta}^Y_{DID}\)</span>.</p>
<p>In order to prove the result for the OLS DID estimator, it is convenient to write the model in matrix form (where we rank all the observations from the first period in the first lines of each matrix and vector):</p>
<p><span class="math display">\[\begin{align*}
  \underbrace{\left(\begin{array}{c}  Y_{1,B} \\    \vdots \\   Y_{N,B} \\Y_{1,A} \\    \vdots \\   Y_{N,A} \end{array}\right)}_{Y} &amp; =
    \underbrace{\left(\begin{array}{cccc}   1 &amp; D_1 &amp; T_{1,B} &amp; D_1T_{1,B}\\    \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ 1 &amp; D_N &amp; T_{N,B} &amp; D_NT_{N,B} \\
                                        1 &amp; D_1 &amp; T_{1,A} &amp; D_1T_{1,A}\\    \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ 1 &amp; D_N &amp; T_{N,A} &amp; D_NT_{N,A}\end{array}\right)}_{X}
  \underbrace{\left(\begin{array}{c}    \alpha \\ \mu \\ \delta \\  \beta \end{array}\right)}_{\Theta} +
  \underbrace{\left(\begin{array}{c}    \epsilon_{1,B} \\   \vdots \\   \epsilon_{N,B} \\ \epsilon_{1,A} \\ \vdots \\   \epsilon_{N,A} \end{array}\right)}_{\epsilon}
\end{align*}\]</span></p>
<p>Now, using the fact that <span class="math inline">\(T_{i,B}=0\)</span> and <span class="math inline">\(T_{i,A}=1\)</span>, <span class="math inline">\(\forall i\)</span>, we can write matrix <span class="math inline">\(X\)</span> as follows:</p>
<p><span class="math display">\[\begin{align*}
  X &amp; = \left(\begin{array}{cccc}   1 &amp; D_1 &amp; 0 &amp; 0\\   \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ 1 &amp; D_N &amp; 0 &amp; 0 \\
                                        1 &amp; D_1 &amp; 1 &amp; D_1\\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ 1 &amp; D_N &amp; 1 &amp; D_N\end{array}\right)
\end{align*}\]</span></p>
<p>Doing some matrix multiplication and factoring <span class="math inline">\(N\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
  X&#39;X &amp; = N\underbrace{\left(\begin{array}{cccc}
                    2 &amp; 2\bar{D} &amp; 1 &amp; \bar{D}\\
                    2\bar{D} &amp; 2\bar{D} &amp; \bar{D} &amp; \bar{D} \\
                    1 &amp; \bar{D} &amp; 1 &amp; \bar{D}\\
                    \bar{D} &amp; \bar{D} &amp; \bar{D} &amp; \bar{D}
            \end{array}\right)}_{x&#39;x}
\end{align*}\]</span></p>
<p>with <span class="math inline">\(\bar{D}=\frac{1}{N}\sum_{i=1}^ND_i\)</span>, and using the fact that <span class="math inline">\(D_i^2=D_i\)</span> since <span class="math inline">\(D_i\in\left\{0,1\right\}\)</span>.
Using results on the inverse of a 4 by 4 matrix presented <a href="https://semath.info/src/inverse-cofactor-ex4.html">here</a> and collecting terms patiently, we find that the determinant of <span class="math inline">\(xx\)</span> is equal to:</p>
<p><span class="math display">\[\begin{align*}
  \det(x&#39;x) &amp; = \bar{D}^2(1-\bar{D})^2
\end{align*}\]</span></p>
<p>and its adjugate is equal to:</p>
<p><span class="math display">\[\begin{align*}
  \tilde{x&#39;x} &amp; = \bar{D}(1-\bar{D})
                  \left(\begin{array}{cccc}
                    \bar{D} &amp; -\bar{D} &amp; -\bar{D} &amp; \bar{D}\\
                    -\bar{D} &amp; 1 &amp; \bar{D} &amp; -1 \\
                    -\bar{D} &amp; \bar{D} &amp; 2\bar{D} &amp; -2\bar{D}\\
                    \bar{D} &amp; -1 &amp; -2\bar{D} &amp; 2
            \end{array}\right)
\end{align*}\]</span></p>
<p>We also have that:</p>
<p><span class="math display">\[\begin{align*}
  X&#39;Y &amp; = N\left(\begin{array}{c}
              \bar{Y}_B+\bar{Y}_A \\
              \bar{D}(\bar{Y}^1_B+\bar{Y}^1_A)\\
              \bar{Y}_A \\
              \bar{D}\bar{Y}^1_A
            \end{array}\right)
\end{align*}\]</span></p>
<p>with <span class="math inline">\(\bar{Y}_t=\frac{1}{N}\sum_{i=1}^NY_{i,t}\)</span> and <span class="math inline">\(\bar{Y}^1_t=\frac{1}{\sum_{i=1}^ND_i}\sum_{i=1}^ND_iY_{i,t}\)</span> and <span class="math inline">\(\bar{Y}^0_t=\frac{1}{\sum_{i=1}^N(1-D_i)}\sum_{i=1}^N(1-D_i)Y_{i,t}\)</span> and using the fact that <span class="math inline">\(\sum_{i=1}^ND_iY_{i,t}=N\bar{D}\bar{Y}^1_t\)</span>.
Using the fact that <span class="math inline">\(Y_{i,t}=D_iY_{i,t}+(1-D_i)Y_{i,t}\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
  \bar{Y}_t &amp; = \frac{\sum_{i=1}^ND_i}{N}\frac{\sum_{i=1}^ND_iY_{i,t}}{\sum_{i=1}^ND_i}+\frac{\sum_{i=1}^N(1-D_i)}{N}\frac{\sum_{i=1}^N(1-D_i)Y_{i,t}}{\sum_{i=1}^N(1-D_i)} \\
            &amp; = \bar{D}\bar{Y}^1_t+(1-\bar{D})\bar{Y}^0_t.
 \end{align*}\]</span></p>
<p>We thus have:</p>
<p><span class="math display">\[\begin{align*}
  X&#39;Y &amp; = N\left(\begin{array}{c}
              \underbrace{\bar{Y}^0_B+\bar{Y}^0_A+\bar{D}(\bar{Y}^1_B-\bar{Y}^0_B+\bar{Y}^1_A-\bar{Y}^0_A)}_{\mathbf{A}} \\
              \underbrace{\bar{D}(\bar{Y}^1_B+\bar{Y}^1_A)}_{\mathbf{B}}\\
              \underbrace{\bar{Y}^0_A+\bar{D}(\bar{Y}^1_A-\bar{Y}^0_A)}_{\mathbf{C}} \\
              \underbrace{\bar{D}\bar{Y}^1_A}_{\mathbf{D}}
            \end{array}\right)
\end{align*}\]</span></p>
<p>Using the fact that <span class="math inline">\((X&#39;X)^{-1}=(Nx&#39;x)^{-1}=\frac{1}{N}(x&#39;x)^{-1}=\frac{1}{N}\frac{\tilde{x&#39;x}}{\det(x&#39;x)}\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
  \hat{\Theta}^{OLS} &amp; = (X&#39;X)^{-1}X&#39;Y \\
         &amp; = \frac{1}{\bar{D}(1-\bar{D})}
            \left(\begin{array}{c}
              \bar{D}(\mathbf{A}-\mathbf{B}-\mathbf{C}+\mathbf{D}) \\
              -\bar{D}\mathbf{A}+\mathbf{B}+\bar{D}\mathbf{C}-\mathbf{D} \\
              \bar{D}(-\mathbf{A}+\mathbf{B}+2\mathbf{C}-2\mathbf{D})\\
              \bar{D}\mathbf{A}-\mathbf{B}-2\bar{D}\mathbf{C}+2\mathbf{D}
            \end{array}\right)
\end{align*}\]</span></p>
<p>Letâs take each term in turn:</p>
<p><span class="math display">\[\begin{align*}
  \hat{\alpha}^{OLS} &amp; = \frac{1}{1-\bar{D}}
            \left(\bar{Y}^0_B+\bar{Y}^0_A+\bar{D}(\bar{Y}^1_B-\bar{Y}^0_B+\bar{Y}^1_A-\bar{Y}^0_A)
            -\bar{D}(\bar{Y}^1_B+\bar{Y}^1_A)
            -(\bar{Y}^0_A+\bar{D}(\bar{Y}^1_A-\bar{Y}^0_A))
            +\bar{D}\bar{Y}^1_A\right)\\
        &amp; = \frac{1}{1-\bar{D}}
              \left(\bar{Y}^0_B(1-\bar{D})
                    +\bar{Y}^0_A(1-\bar{D}-1+\bar{D})
                    +\bar{Y}^1_B(\bar{D}-\bar{D})
                    +\bar{Y}^1_A(\bar{D}-\bar{D}-\bar{D}+\bar{D})\right)\\
        &amp; = \bar{Y}^0_B
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
  \hat{\mu}^{OLS} &amp; = \frac{1}{\bar{D}(1-\bar{D})}\left(
            -\bar{D}(\bar{Y}^0_B+\bar{Y}^0_A+\bar{D}(\bar{Y}^1_B-\bar{Y}^0_B+\bar{Y}^1_A-\bar{Y}^0_A))
            +\bar{D}(\bar{Y}^1_B+\bar{Y}^1_A)
            +\bar{D}(\bar{Y}^0_A+\bar{D}(\bar{Y}^1_A-\bar{Y}^0_A))
            -\bar{D}\bar{Y}^1_A\right)\\
      &amp; =  \frac{1}{1-\bar{D}}\left(
            -\bar{Y}^0_B(1-\bar{D})
            +\bar{Y}^0_A(-1+\bar{D}+1-\bar{D})
            +\bar{Y}^1_B(1-\bar{D})
            +\bar{Y}^1_A(-\bar{D}+1+\bar{D}-1)\right) \\
      &amp; = \bar{Y}^1_B-\bar{Y}^0_B
 \end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
  \hat{\delta}^{OLS} &amp; =\frac{1}{1-\bar{D}}\left(
            -(\bar{Y}^0_B+\bar{Y}^0_A+\bar{D}(\bar{Y}^1_B-\bar{Y}^0_B+\bar{Y}^1_A-\bar{Y}^0_A))
            +(\bar{Y}^1_B+\bar{Y}^1_A)
            +2(\bar{Y}^0_A+\bar{D}(\bar{Y}^1_A-\bar{Y}^0_A))
            -2\bar{D}\bar{Y}^1_A\right)\\
      &amp; =  \frac{1}{1-\bar{D}}\left(
            -\bar{Y}^0_B(1-\bar{D})
            +\bar{Y}^0_A(2(1-\bar{D})-(1-\bar{D}))
            +\bar{Y}^1_B(\bar{D}-\bar{D})
            +\bar{Y}^1_A(\bar{D}-\bar{D}+2\bar{D}-2\bar{D})\right) \\
      &amp; = \bar{Y}^0_A-\bar{Y}^0_B
 \end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
  \hat{\beta}^{OLS} &amp; =\frac{1}{\bar{D}(1-\bar{D})}\left(
            \bar{D}(\bar{Y}^0_B+\bar{Y}^0_A+\bar{D}(\bar{Y}^1_B-\bar{Y}^0_B+\bar{Y}^1_A-\bar{Y}^0_A))
            -\bar{D}(\bar{Y}^1_B+\bar{Y}^1_A)
            -2 \bar{D}(\bar{Y}^0_A+\bar{D}(\bar{Y}^1_A-\bar{Y}^0_A))
            +2\bar{D}\bar{Y}^1_A\right)\\
      &amp; =  \frac{1}{1-\bar{D}}\left(
            \bar{Y}^0_B(1-\bar{D})
            +\bar{Y}^0_A((1-\bar{D})-2(1-\bar{D}))
            +\bar{Y}^1_B(\bar{D}-1)
            +\bar{Y}^1_A(\bar{D}-1-2\bar{D}+2)\right) \\
      &amp; = \bar{Y}^1_A-\bar{Y}^1_B-(\bar{Y}^0_A-\bar{Y}^0_B)
 \end{align*}\]</span></p>
<p>This last results proves that <span class="math inline">\(\hat{\beta}^{OLS}=\hat{\Delta}^Y_{DID}\)</span>.</p>
<p>For the within estimator, it can be written in matrix form as follows:</p>
<p><span class="math display">\[\begin{align*}
  \underbrace{\left(\begin{array}{c}  Y_{1,B}-\bar{Y}_1 \\  \vdots \\   Y_{N,B}-\bar{Y}_N \\Y_{1,A}-\bar{Y}_1 \\    \vdots \\   Y_{N,A}-\bar{Y}_N \end{array}\right)}_{Y^W} &amp; =
    \underbrace{\left(\begin{array}{ccc}    1  &amp; 0 &amp; -\bar{D}_1\\   \vdots &amp; \vdots &amp; \vdots \\ 1 &amp; 0 &amp; -\bar{D}_N \\
                                        1 &amp; 1 &amp; D_1-\bar{D}_1\\ \vdots &amp; \vdots &amp; \vdots \\ 1 &amp; 1 &amp; D_N-\bar{D}_N\end{array}\right)}_{X^W}
  \underbrace{\left(\begin{array}{c}    \alpha^W \\ \delta^W \\ \beta^W \end{array}\right)}_{\Theta^{W}} +
  \underbrace{\left(\begin{array}{c}    \epsilon^W_{1,B} \\ \vdots \\   \epsilon^W_{N,B} \\ \epsilon^W_{1,A} \\ \vdots \\   \epsilon^W_{N,A} \end{array}\right)}_{\epsilon^W}
\end{align*}\]</span></p>
<p>We have:</p>
<p><span class="math display">\[\begin{align*}
  {X^W}&#39;X^W &amp; = N\underbrace{\left(\begin{array}{ccc}
                    2 &amp;  1 &amp; 0\\
                    1 &amp; 1  &amp; \frac{\bar{D}}{2} \\
                    0 &amp; \frac{\bar{D}}{2} &amp; \frac{\bar{D}}{2}
            \end{array}\right)}_{{x^W}&#39;x^W}
\end{align*}\]</span></p>
<p>This is because:</p>
<p><span class="math display">\[\begin{align*}
  {X^W}&#39;X^W &amp; = \left(\begin{array}{ccc}
                    2N &amp; N &amp; -\sum_{i=1}^N\bar{D}_i+\sum_{i=1}^N(D_i-\bar{D}_i)\\
                    N &amp; N  &amp; \sum_{i=1}^N(D_i-\bar{D}_i) \\
                    -\sum_{i=1}^N\bar{D}_i+\sum_{i=1}^N(D_i-\bar{D}_i) &amp; \sum_{i=1}^N(D_i-\bar{D}_i) &amp; \sum_{i=1}^N\bar{D}_i^2+\sum_{i=1}^N(D_i-\bar{D}_i)^2
            \end{array}\right)
\end{align*}\]</span></p>
<p>and:</p>
<p><span class="math display">\[\begin{align*}
  \sum_{i=1}^N\bar{D}_i &amp; = \frac{1}{2}\sum_{i=1}^N(D_{i,B}+D_{i,A}) \\
                        &amp; = \frac{1}{2}\sum_{i=1}^ND_{i} \\
                        &amp; = \frac{1}{2}N\bar{D}\\
  \sum_{i=1}^N(D_i-\bar{D}_i) &amp; = N\bar{D}-\frac{1}{2}N\bar{D} \\
                            &amp; = \frac{1}{2}N\bar{D}\\
  \sum_{i=1}^N\bar{D}^2_i &amp; = \frac{1}{4}\sum_{i=1}^N(D_{i,B}+D_{i,A})^2\\
                          &amp; = \frac{1}{4}\sum_{i=1}^ND^2_{i} \\
                          &amp; = \frac{1}{4}N\bar{D} \\
  \sum_{i=1}^N(D_i-\bar{D}_i)^2 &amp; = \sum_{i=1}^N(D_{i}-\frac{1}{2}D_{i})^2\\
                                &amp; = \frac{1}{4}N\bar{D}
\end{align*}\]</span></p>
<p>Now we can use the results <a href="https://study.com/academy/lesson/finding-the-inverse-of-a-3x3-matrix.html">here</a> and <a href="https://metric.ma.ic.ac.uk/metric_public/matrices/inverses/inverses2.html">here</a> to compute the inverse of the <span class="math inline">\({x^W}&#39;x^W\)</span> matrix.
Let us first compute the determinant:</p>
<p><span class="math display">\[\begin{align*}
  \det({x^W}&#39;x^W) &amp; = 2(\frac{\bar{D}}{2}-\frac{\bar{D}^2}{4}) - \frac{\bar{D}}{2}\\
                  &amp; = \frac{1}{2}\bar{D}(1-\bar{D}).
\end{align*}\]</span></p>
<p>And then the adjugate:</p>
<p><span class="math display">\[\begin{align*}
  \tilde{{x^W}&#39;x^W} &amp; = \left(\begin{array}{ccc}
                    \frac{\bar{D}}{2}(1-\frac{\bar{D}}{2}) &amp; -\frac{\bar{D}}{2} &amp; \frac{\bar{D}}{2}\\
                    -\frac{\bar{D}}{2} &amp; \bar{D}  &amp; -\bar{D}\\
                    \frac{\bar{D}}{2} &amp; -\bar{D} &amp; 1
            \end{array}\right)
\end{align*}\]</span></p>
<p>Let us now examine <span class="math inline">\({X^W}&#39;Y^W\)</span>:</p>
<p><span class="math display">\[\begin{align*}
  {X^W}&#39;Y^W &amp; = \left(\begin{array}{c}
                \sum_{i=1}^N(Y_{i,B}-\bar{Y}_i)+\sum_{i=1}^N(Y_{i,A}-\bar{Y}_i)\\
                \sum_{i=1}^N(Y_{i,A}-\bar{Y}_i) \\
                -\sum_{i=1}^N\bar{D}_i(Y_{i,B}-\bar{Y}_i)+\sum_{i=1}^N(D_i-\bar{D}_i)(Y_{i,A}-\bar{Y}_i)
            \end{array}\right)
\end{align*}\]</span></p>
<p>We have:</p>
<p><span class="math display">\[\begin{align*}
  \sum_{i=1}^N(Y_{i,B}-\bar{Y}_i) &amp; = N\bar{Y}_B-\frac{1}{2}N(\bar{Y}_B+\bar{Y}_A)\\
                                  &amp; = \frac{1}{2}N(\bar{Y}_B-\bar{Y}_A)\\
  \sum_{i=1}^N(Y_{i,A}-\bar{Y}_i) &amp; = \frac{1}{2}N(\bar{Y}_A-\bar{Y}_B)\\
  \sum_{i=1}^N\bar{D}_i(Y_{i,B}-\bar{Y}_i) &amp; = \sum_{i=1}^N\frac{1}{2}D_i(Y_{i,B}-\frac{1}{2}\sum_{i=1}^N(Y_{i,B}+Y_{i,A}))\\
                                    &amp; = \sum_{i=1}^N\frac{1}{2}D_i\frac{1}{2}(Y_{i,B}-Y_{i,A})\\
                                    &amp; = \frac{1}{4}\sum_{i=1}^ND_i(Y_{i,B}-Y_{i,A})\\
                                    &amp; = \frac{1}{4}N\bar{D}(\bar{Y}^1_B-\bar{Y}^1_A)\\
  \sum_{i=1}^N(D_i-\bar{D}_i)(Y_{i,A}-\bar{Y}_i) &amp; = \sum_{i=1}^N(D_i-\frac{1}{2}D_i)(Y_{i,A}-\frac{1}{2}\sum_{i=1}^N(Y_{i,B}+Y_{i,A}))\\
                                                &amp; = \frac{1}{4}\sum_{i=1}^ND_i(Y_{i,A}-Y_{i,B})\\
                                                &amp; = \frac{1}{4}N\bar{D}(\bar{Y}^1_A-\bar{Y}^1_B).
\end{align*}\]</span></p>
<p>So, we have:</p>
<p><span class="math display">\[\begin{align*}
  ({X^W}&#39;X^W)^{-1}{X^W}&#39;Y^W  &amp; = \frac{2}{N\bar{D}(1-\bar{D})}
            \left(\begin{array}{ccc}
                    \frac{\bar{D}}{2}(1-\frac{\bar{D}}{2}) &amp; -\frac{\bar{D}}{2} &amp; \frac{\bar{D}}{2}\\
                    -\frac{\bar{D}}{2} &amp; \bar{D}  &amp; -\bar{D}\\
                    \frac{\bar{D}}{2} &amp; -\bar{D} &amp; 1
            \end{array}\right)
            \left(\begin{array}{c}
                0\\
                \frac{N}{2}(\bar{Y}_A-\bar{Y}_B)\\
                \frac{N}{2}\bar{D}(\bar{Y}^1_A-\bar{Y}^1_B)
            \end{array}\right)
\end{align*}\]</span></p>
<p>We thus have:</p>
<p><span class="math display">\[\begin{align*}
  \hat{\beta}^W &amp; = \frac{2}{N\bar{D}(1-\bar{D})}\left(-\bar{D}\frac{N}{2}(\bar{Y}_A-\bar{Y}_B)+\frac{N}{2}\bar{D}(\bar{Y}^1_A-\bar{Y}^1_B)\right)\\
                &amp; = \frac{1}{1-\bar{D}}\left(\bar{Y}^1_A-\bar{Y}^1_B-(\bar{Y}_A-\bar{Y}_B)\right)\\
\end{align*}\]</span></p>
<p>Using the fact that <span class="math inline">\(\bar{Y}_t=\bar{D}\bar{Y}_t^1+(1-\bar{D})\bar{Y}^0_t\)</span>, we have <span class="math inline">\(\bar{Y}_A-\bar{Y}_B=(1-\bar{D})(\bar{Y}^0_A-\bar{Y}^0_B)+\bar{D}(\bar{Y}_A^1-\bar{Y}_B^1)\)</span>.</p>
<p>As a consequence:</p>
<p><span class="math display">\[\begin{align*}
  \hat{\beta}^W &amp; = \frac{1-\bar{D}}{1-\bar{D}}\left(\bar{Y}^1_A-\bar{Y}^1_B-(\bar{Y}^0_A-\bar{Y}^0_B)\right)\\
                &amp; =  \bar{Y}^1_A-\bar{Y}^1_B-(\bar{Y}^0_A-\bar{Y}^0_B),
\end{align*}\]</span></p>
<p>which proves that <span class="math inline">\(\hat{\beta}^{W}=\hat{\Delta}^Y_{DID}\)</span>.</p>
<p>Now for <span class="math inline">\(\hat{\beta}^{LSDV}\)</span>, the estimator can be written in matrix form as follows:</p>
<p><span class="math display">\[\begin{align*}
  \underbrace{\left(\begin{array}{c}  Y_{1,B} \\    \vdots \\   Y_{N,B} \\Y_{1,A} \\    \vdots \\   Y_{N,A} \end{array}\right)}_{Y} &amp; =
    \underbrace{\left(\begin{array}{ccccccc}
    1 &amp; 0 &amp; \dots &amp; 0 &amp; 1 &amp; 0 &amp; D_{1,B}\\
    0 &amp; 1 &amp; \dots &amp; 0 &amp; 1 &amp; 0 &amp; D_{2,B}\\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots  \\
    0 &amp; 0 &amp; \dots &amp; 1 &amp; 1 &amp; 0 &amp; D_{N,B}\\
    1 &amp; 0 &amp; \dots &amp; 0 &amp; 0 &amp; 1 &amp; D_{1,A}\\
    0 &amp; 1 &amp; \dots &amp; 0 &amp; 0 &amp; 1 &amp; D_{2,A}\\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots  \\
    0 &amp; 0 &amp; \dots &amp; 1 &amp; 0 &amp; 1 &amp; D_{N,A}\\
    \end{array}\right)}_{X^{LSDV}}
  \underbrace{\left(\begin{array}{c}  \mu^{LSDV}_1\\ \vdots \\ \mu^{LSDV}_N \\ \delta^{LSDV}_B \\ \delta^{LSDV}_A \\    \beta^{LSDV} \end{array}\right)}_{\Theta^{LSDV}} +
  \underbrace{\left(\begin{array}{c}    \epsilon^{LSDV}_{1,B} \\    \vdots \\   \epsilon^{LSDV}_{N,B} \\ \epsilon^{LSDV}_{1,A} \\   \vdots \\   \epsilon^{LSDV}_{N,A} \end{array}\right).}_{\epsilon^{LSDV}}
\end{align*}\]</span></p>
<p>In order to prove the result, it is going to be very convenient to use <a href="https://bookdown.org/ts_robinson1994/10_fundamental_theorems_for_econometrics/frisch.html">Frish-Waugh-Lovell Theorem</a>.
It can be stated as follows:</p>
<div class="theorem">
<p><span id="thm:FWL" class="theorem"><strong>Theorem A.1  (Frish-Waugh-Lovell) </strong></span>The coefficients on a set of variables <span class="math inline">\(X_2\)</span> estimated by OLS in a linear regression with another set of control variables <span class="math inline">\(X_1\)</span> is equal to the coefficients on the same set of variables estimated by OLS in a linear model where the outcome variable is the residual of regressing <span class="math inline">\(Y\)</span> on <span class="math inline">\(X_1\)</span> by OLS and the explanatory variables are the residuals of regressing <span class="math inline">\(X_2\)</span> on <span class="math inline">\(X_1\)</span>. More formally: <span class="math inline">\(\hat{\beta}_2^{OLS}=\hat{\beta}_2^{OLS(MX_1)}\)</span> where:
<span class="math display">\[\begin{align*}
  Y &amp; = X_1\beta_1 + X_2\beta_2 + \epsilon \\
  M_1Y &amp; = M_1X_2\beta_2 + \epsilon^* \\
  M_1 &amp; = I - X_1(X_1&#39;X_1)^{-1}X_1&#39;.
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-189" class="proof"><em>Proof</em>. </span>See Section 8.2.2 <a href="https://bookdown.org/ts_robinson1994/10_fundamental_theorems_for_econometrics/frisch.html">here</a>.</p>
</div>
<p><span class="math inline">\(M_1\)</span> is called the <strong>prediction</strong> or the <strong>residualizing</strong> matrix.</p>
<p>In our case, let us call <span class="math inline">\(X^{LSDV}_{\mu}\)</span> the first <span class="math inline">\(N\)</span> columns of <span class="math inline">\(X^{LSDV}\)</span>.
<span class="math inline">\(X^{LSDV}_{\mu}\)</span> is going to play the role of <span class="math inline">\(X_1\)</span> in Theorem <a href="proofs.html#thm:FWL">A.1</a>.
Let us call <span class="math inline">\(X^{LSDV}_{\delta,D}\)</span> the matrix made of the last three columns of <span class="math inline">\(X^{LSDV}\)</span>.
<span class="math inline">\(X^{LSDV}_{\delta,D}\)</span> is going to play the role of <span class="math inline">\(X_2\)</span> in Theorem <a href="proofs.html#thm:FWL">A.1</a>.</p>
<p>Let us first note that <span class="math inline">\({X^{LSDV}_{\mu}}&#39;X^{LSDV}_{\mu}=2I_{N}\)</span>, where <span class="math inline">\(I_{N}\)</span> is the identity matrix of dimension <span class="math inline">\(N\)</span>.
As a consequence, <span class="math inline">\(({X^{LSDV}_{\mu}}&#39;X^{LSDV}_{\mu})^{-1}=\frac{1}{2}I_N\)</span>.
Now, let us compute <span class="math inline">\({X^{LSDV}_{\mu}}&#39;Y\)</span>:</p>
<p><span class="math display">\[\begin{align*}
 {X^{LSDV}_{\mu}}&#39;Y  &amp; = \left(\begin{array}{c}
                            Y_{1,B}+Y_{1,A} \\
                            \vdots \\
                            Y_{N,B}+Y_{N,A}
                          \end{array}\right).
\end{align*}\]</span></p>
<p>As a consequence, we have:</p>
<p><span class="math display">\[\begin{align*}
 M^{LSDV}_{\mu}Y  &amp; = Y - X^{LSDV}_{\mu}({X^{LSDV}_{\mu}}&#39;X^{LSDV}_{\mu})^{-1}{X^{LSDV}_{\mu}}&#39;Y \\
                  &amp; = Y-\frac{1}{2} X^{LSDV}_{\mu}I_N
                  \left(\begin{array}{c}
                            Y_{1,B}+Y_{1,A} \\
                            \vdots \\
                            Y_{N,B}+Y_{N,A}
                          \end{array}\right) \\
                  &amp; = \left(\begin{array}{c}
                           Y_{1,B} - \frac{1}{2}(Y_{1,B}+Y_{1,A}) \\
                            \vdots \\
                            Y_{N,B} - \frac{1}{2}(Y_{N,B}+Y_{N,A})\\
                           Y_{1,A} - \frac{1}{2}(Y_{1,B}+Y_{1,A}) \\
                            \vdots \\
                            Y_{N,A} - \frac{1}{2}(Y_{N,B}+Y_{N,A})
                          \end{array}\right).
\end{align*}\]</span></p>
<p>And finally:</p>
<p><span class="math display">\[\begin{align*}
 M^{LSDV}_{\mu}X^{LSDV}_{\delta,D}  &amp; = X^{LSDV}_{\delta,D} - X^{LSDV}_{\mu}({X^{LSDV}_{\mu}}&#39;X^{LSDV}_{\mu})^{-1}{X^{LSDV}_{\mu}}&#39;X^{LSDV}_{\delta,D} \\
                  &amp; = \left(\begin{array}{ccc}
                          \frac{1}{2} &amp; -\frac{1}{2} &amp;  D_{1,B}-\frac{1}{2}(D_{1,B}+D_{1,A}) \\
                          \vdots &amp; \vdots &amp;  \vdots \\
                          \frac{1}{2} &amp; -\frac{1}{2} &amp;  D_{N,B}-\frac{1}{2}(D_{1,B}+D_{1,A}) \\
                          -\frac{1}{2} &amp; \frac{1}{2} &amp;  D_{1,A}-\frac{1}{2}(D_{1,B}+D_{1,A}) \\
                          \vdots &amp; \vdots &amp;  \vdots \\
                          -\frac{1}{2} &amp; \frac{1}{2} &amp;  D_{N,A}-\frac{1}{2}(D_{1,B}+D_{1,A}) \\
                          \end{array}\right).
\end{align*}\]</span></p>
<p>Using Theorem <a href="proofs.html#thm:FWL">A.1</a>, we can rewrite the LSDV version of the TWFE model as follows:</p>
<p><span class="math display">\[\begin{align*}
     M^{LSDV}_{\mu}Y &amp; =  M^{LSDV}_{\mu}X^{LSDV}_{\delta,D}
                            \left(\begin{array}{c}
                            \delta^{LSDV}_B \\ \delta^{LSDV}_A \\   \beta^{LSDV}
                            \end{array}\right) + M^{LSDV}_{\mu}\epsilon^{LSDV}
\end{align*}\]</span></p>
<p>In a more compact notation, we have, <span class="math inline">\(\forall i\in\left[1,N\right]\)</span> and <span class="math inline">\(\forall t\in\left\{B,A\right\}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
    Y_{i,t} - \bar{Y}_i  &amp;  = \frac{1}{2}(\delta^{LSDV}_A-\delta^{LSDV}_B)(\uns{t=A}-\uns{t=B}) + \beta^{LSDV}(D_{i,t}-\bar{D}_{i}) + \epsilon^{LSDV}_{i,t}-\bar{\epsilon}^{LSDV}_{i},
\end{align*}\]</span></p>
<p>which we can rewrite, for simplicity, as:</p>
<p><span class="math display">\[\begin{align*}
    Y_{i,t} - \bar{Y}_i  &amp;  = \tilde{\delta}^{LSDV}_t + \beta^{LSDV}(D_{i,t}-\bar{D}_{i}) + \epsilon^{LSDV}_{i,t}-\bar{\epsilon}^{LSDV}_{i},
\end{align*}\]</span></p>
<p>with <span class="math inline">\(\tilde{\delta}^{LSDV}_A=-\tilde{\delta}_B^{LSDV}=\bar{\delta}^{LSDV}\)</span> and <span class="math inline">\(\bar{\delta}^{LSDV}=\frac{1}{2}(\delta^{LSDV}_A-\delta^{LSDV}_B)\)</span>.</p>
<p>In matrix form, we can thus rewrite the LSDV model transformed by the application of the Frich-Waugh theorem as follows:</p>
<p><span class="math display">\[\begin{align*}
  \underbrace{\left(\begin{array}{c}  Y_{1,B}-\bar{Y}_1 \\  \vdots \\   Y_{N,B}-\bar{Y}_N \\Y_{1,A}-\bar{Y}_1 \\    \vdots \\   Y_{N,A}-\bar{Y}_N \end{array}\right)}_{Y^{LSDV}_r} &amp; =
    \underbrace{\left(\begin{array}{ccc}
      1  &amp; 0 &amp; -\bar{D}_1\\
      \vdots &amp; \vdots &amp; \vdots \\
      1 &amp; 0 &amp; -\bar{D}_N \\
      0 &amp; 1 &amp; D_1-\bar{D}_1\\
      \vdots &amp; \vdots &amp; \vdots \\
      0 &amp; 1 &amp; D_N-\bar{D}_N
    \end{array}\right)}_{X^{LSDV}_r}
  \underbrace{\left(\begin{array}{c}    \tilde{\delta}^{LSDV}_B \\ \tilde{\delta}^{LSDV}_A \\   \beta^{LSDV} \end{array}\right)}_{\Theta^{LSDV}_r} +
  \underbrace{\left(\begin{array}{c}    \epsilon^{LSDV}_{1,B}-\bar{\epsilon}^{LSDV}_{1} \\  \vdots \\   \epsilon^{LSDV}_{N,B}-\bar{\epsilon}^{LSDV}_{N} \\ \epsilon^{LSDV}_{1,A}-\bar{\epsilon}^{LSDV}_{1} \\   \vdots \\   \epsilon^{LSDV}_{N,A}-\bar{\epsilon}^{LSDV}_{N} \end{array}\right)}_{\epsilon^{LSDV}_r}
\end{align*}\]</span></p>
<p>This is very close to the formula for the Within estimator we have seen above.
The only difference is that we have two time fixed effects instead of a constant and the <strong>After</strong> time fixed effect.
We are going to solve for the estimator in a very similar way.
First:</p>
<p><span class="math display">\[\begin{align*}
  {X^{LSDV}_r}&#39;X^{LSDV}_r &amp; = N\underbrace{\left(\begin{array}{ccc}
                    1 &amp;  0 &amp; -\frac{\bar{D}}{2}\\
                    0 &amp; 1  &amp; \frac{\bar{D}}{2} \\
                    -\frac{\bar{D}}{2} &amp; \frac{\bar{D}}{2} &amp; \frac{\bar{D}}{2}
            \end{array}\right)}_{{x^{LSDV}_r}&#39;x^{LSDV}_r}
\end{align*}\]</span></p>
<p>The determinant of <span class="math inline">\({x^{LSDV}_r}&#39;x^{LSDV}_r\)</span> is:</p>
<p><span class="math display">\[\begin{align*}
  \det({x^{LSDV}_r}&#39;x^{LSDV}_r) &amp; = \frac{1}{2}\bar{D}(1-\bar{D})
\end{align*}\]</span></p>
<p>and its adjoint matrix is:</p>
<p><span class="math display">\[\begin{align*}
  \tilde{{x^{LSDV}_r}&#39;x^{LSDV}_r} &amp; = \left(\begin{array}{ccc}
                    \frac{1}{2}\bar{D}(1-\frac{1}{2}\bar{D}) &amp; - \frac{1}{4}\bar{D}^2 &amp; \frac{1}{2}\bar{D}\\
                    - \frac{1}{4}\bar{D}^2 &amp; \frac{1}{2}\bar{D}(1-\frac{1}{2}\bar{D})  &amp; -\frac{1}{2}\bar{D} \\
                    \frac{1}{2}\bar{D} &amp; -\frac{1}{2}\bar{D} &amp; 1
            \end{array}\right).
\end{align*}\]</span></p>
<p>Finally, we have:</p>
<p><span class="math display">\[\begin{align*}
  {X^{LSDV}_r}&#39;Y^{LSDV}_r &amp; = \left(\begin{array}{c}
                \sum_{i=1}^N(Y_{i,B}-\bar{Y}_i)\\
                \sum_{i=1}^N(Y_{i,A}-\bar{Y}_i) \\
                -\sum_{i=1}^N\bar{D}_i(Y_{i,B}-\bar{Y}_i)+\sum_{i=1}^N(D_i-\bar{D}_i)(Y_{i,A}-\bar{Y}_i)
            \end{array}\right) \\
                          &amp; = \left(\begin{array}{c}
               -\frac{1}{2}N(\bar{Y}_A-\bar{Y}_B)\\
               \frac{1}{2}N(\bar{Y}_A-\bar{Y}_B)\\
               \frac{1}{2}N\bar{D}(\bar{Y}_A^1-\bar{Y}_B^1)
            \end{array}\right)
\end{align*}\]</span></p>
<p>Using the fact that <span class="math inline">\(\hat{\Theta}^{LSDV}_r=({X^{LSDV}_r}&#39;X^{LSDV}_r)^{-1}{X^{LSDV}_r}&#39;Y^{LSDV}_r\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
  \hat{\beta}^{LSDV} &amp; = \frac{2}{N\bar{D}(1-\bar{D})}\left[-\frac{\bar{D}N}{2}(\bar{Y}_A-\bar{Y}_B)+\frac{\bar{D}N}{2}(\bar{Y}_A^1-\bar{Y}_B^1)\right] \\
                    &amp; =\frac{1}{1-\bar{D}}\left[\bar{Y}_A^1-\bar{Y}_B^1-(1-\bar{D})(\bar{Y}^0_A-\bar{Y}^0_B)-\bar{D}(\bar{Y}_A^1-\bar{Y}_B^1)\right]\\
                    &amp; =\frac{1}{1-\bar{D}}\left[(1-\bar{D})(\bar{Y}_A^1-\bar{Y}_B^1)-(1-\bar{D})(\bar{Y}^0_A-\bar{Y}^0_B)\right]\\
                    &amp; =\bar{Y}_A^1-\bar{Y}_B^1-(\bar{Y}^0_A-\bar{Y}^0_B).
\end{align*}\]</span></p>
<p>The second equality uses the fact that <span class="math inline">\(\bar{Y}_A-\bar{Y}_B=(1-\bar{D})(\bar{Y}^0_A-\bar{Y}^0_B)+\bar{D}(\bar{Y}_A^1-\bar{Y}_B^1)\)</span>.
This proves the result.</p>
<p><strong>To Do: the AP and LC estimators</strong></p>
</div>
<div id="proofasympnoiseDIDCross" class="section level3 hasAnchor" number="16.3.2">
<h3><span class="header-section-number">A.3.2</span> Proof of Theorem <a href="NE.html#thm:asympnoiseDIDCross">4.7</a><a href="proofs.html#proofasympnoiseDIDCross" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The DID model in repeated cross sections can be written in the following matrix form:</p>
<p><span class="math display">\[\begin{align*}
  \underbrace{\left(\begin{array}{c}  Y_{1,B} \\    \vdots \\   Y_{N_B,B} \\Y_{1,A} \\  \vdots \\   Y_{N_A,A} \end{array}\right)}_{Y} &amp; =
    \underbrace{\left(\begin{array}{cccc}   1 &amp; D^B_{1} &amp; T_{1,B} &amp; D^B_{1}T_{1,B}\\    \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ 1 &amp; D^B_{N_B} &amp; T_{N_B,B} &amp; D^B_{N_B}T_{N_B,B} \\
                                        1 &amp; D^A_{1} &amp; T_{1,A} &amp; D^A_{1}T_{1,A}\\    \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ 1 &amp; D^A_{N_A} &amp; T_{N_A,A} &amp; D^A_{N_A}T_{N_A,A}\end{array}\right)}_{X}
  \underbrace{\left(\begin{array}{c}    \alpha \\ \mu \\ \delta \\  \beta \end{array}\right)}_{\Theta} +
  \underbrace{\left(\begin{array}{c}    \epsilon_{1,B} \\   \vdots \\   \epsilon_{N_B,B} \\ \epsilon_{1,A} \\   \vdots \\   \epsilon_{N_A,A} \end{array}\right),}_{\epsilon}
\end{align*}\]</span></p>
<p>where <span class="math inline">\(D^B_{i}\)</span> and <span class="math inline">\(D^A_{i}\)</span> denote the actual treatment status in period <span class="math inline">\(A\)</span> of individuals observed in periods <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span> respectively and <span class="math inline">\(N_B\)</span> and <span class="math inline">\(N_A\)</span> are the numbers of units observed in periods <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span> respectively.</p>
<p>Using the beginning of the proof of Lemma <a href="proofs.html#lem:asympOLS">A.4</a>, we know that: <span class="math inline">\(\sqrt{N}(\hat{\Theta}_{OLS}-\Theta)=N(X&#39;X)^{-1}\frac{\sqrt{N}}{N}X&#39;\epsilon\)</span>.
Using Slutskyâs Theorem, we know that we can study both terms separately (see the same proof of Lemma <a href="proofs.html#lem:asympOLS">A.4</a>).
Letâs start with <span class="math inline">\(N(X&#39;X)^{-1}\)</span>.
Using the fact that <span class="math inline">\(T_{i,B}=0\)</span> and <span class="math inline">\(T_{i,A}=1\)</span>, <span class="math inline">\(\forall i\)</span>, we can write matrix <span class="math inline">\(X\)</span> as follows:</p>
<p><span class="math display">\[\begin{align*}
  X &amp; = \left(\begin{array}{cccc}   1 &amp; D^B_{1} &amp; 0 &amp; 0\\   \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ 1 &amp; D^B_{N_B} &amp; 0 &amp; 0 \\
                                        1 &amp; D^A_{1} &amp; 1 &amp; D^A_{1}\\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ 1 &amp; D^A_{N_A} &amp; 1 &amp; D^A_{N_A}\end{array}\right)
\end{align*}\]</span></p>
<p>Doing some matrix multiplication, we have:</p>
<p><span class="math display">\[\begin{align*}
  X&#39;X &amp; = N_A\underbrace{\left(\begin{array}{cccc}
                    k+1 &amp; k\bar{D}_B+\bar{D}_A &amp; 1 &amp; \bar{D}_A\\
                    k\bar{D}_B+\bar{D}_A &amp; k\bar{D}_B+\bar{D}_A &amp; \bar{D}_A &amp; \bar{D}_A \\
                    1 &amp; \bar{D}_A &amp; 1 &amp; \bar{D}_A\\
                    \bar{D}_A &amp; \bar{D}_A &amp; \bar{D}_A &amp; \bar{D}_A
            \end{array}\right)}_{x&#39;x}
\end{align*}\]</span></p>
<p>with <span class="math inline">\(\bar{D}_t=\frac{1}{N_t}\sum_{i=1}^{N_t}D^t_i\)</span>, <span class="math inline">\(k=\frac{N_B}{N_A}\)</span>, and using the fact that <span class="math inline">\((D^t_i)^2=D^t_i\)</span> since <span class="math inline">\(D^t_i\in\left\{0,1\right\}\)</span>.
Using results on the inverse of a 4 by 4 matrix presented <a href="https://semath.info/src/inverse-cofactor-ex4.html">here</a> and collecting terms patiently, we find that the determinant of <span class="math inline">\(x&#39;x\)</span> is equal to:</p>
<p><span class="math display">\[\begin{align*}
  \det(x&#39;x) &amp; = k^2\pi\bar{D}_A^2(1-\bar{D}_A)(1-\pi\bar{D}_A),
\end{align*}\]</span>
with <span class="math inline">\(\pi=\frac{\bar{D}_B}{\bar{D}_A}\)</span>, and its adjugate is equal to:</p>
<p><span class="math display">\[\begin{align*}
  \tilde{x&#39;x} &amp; = k\pi\bar{D}_A(1-\bar{D}_A)
                  \left(\begin{array}{cccc}
                    \bar{D}_A &amp; -\bar{D}_A &amp; -\bar{D}_A &amp; \bar{D}_A\\
                    -\bar{D}_A &amp; \frac{1}{\pi} &amp; \bar{D}_A &amp; -\frac{1}{\pi} \\
                    -\bar{D}_A &amp; \bar{D}_A &amp; \bar{D}_A\frac{k+1-\bar{D}_A(k\pi+1)}{1-\bar{D}_A} &amp; -\bar{D}_A\frac{k+1-\bar{D}_A(k\pi+1)}{1-\bar{D}_A}\\
                    \bar{D}_A &amp; -\frac{1}{\pi} &amp; -\bar{D}_A\frac{k+1-\bar{D}_A(k\pi+1)}{1-\bar{D}_A} &amp; k\frac{1-\pi\bar{D}_A}{1-\bar{D}_A}+\frac{1}{\pi}
            \end{array}\right)
\end{align*}\]</span></p>
<p>We finally have that <span class="math inline">\(N_A(X&#39;X)^{-1}=\frac{1}{\det(x&#39;x)}\tilde{x&#39;x}\)</span>.
Taking the <span class="math inline">\(\text{plim}\)</span> with respect to <span class="math inline">\(N_A\)</span>, we have that:</p>
<p><span class="math display">\[\begin{align*}
  \text{plim}N_A(X&#39;X)^{-1} &amp; = \frac{1}{kp(1-p)}
                  \left(\begin{array}{cccc}
                    p &amp; -p &amp; -p &amp; p\\
                    -p &amp; 1 &amp; p &amp; -1 \\
                    -p &amp; p &amp; p(k+1) &amp; -p(k+1)\\
                    p &amp; -1 &amp; -p(k+1) &amp; k+1
            \end{array}\right)
\end{align*}\]</span></p>
<p>The result comes from <span class="math inline">\(\text{plim}\bar{D}_B=\text{plim}\bar{D}_A=\Pr(D_i=1)=p\)</span>, according to the Law of Large Numbers, and thus, using Slutskyâs Theorem, <span class="math inline">\(\text{plim}\pi=1\)</span>.</p>
<p>Let us now derive the asymptotic distribution of <span class="math inline">\(\frac{\sqrt{N}}{N}X&#39;\epsilon\)</span>.
In order to do that, we need to know the coefficients of the OLS DID model in repeated cross sections of different sizes.
They probably are the same that with a panel, but we still need to check.
We have that:</p>
<p><span class="math display">\[\begin{align*}
  X&#39;Y &amp; = N_A\left(\begin{array}{c}
              k\bar{Y}_B+\bar{Y}_A \\
              \bar{D}_A(k\pi\bar{Y}^1_B+\bar{Y}^1_A)\\
              \bar{Y}_A \\
              \bar{D}_A\bar{Y}^1_A
            \end{array}\right)
\end{align*}\]</span></p>
<p>Using the fact that <span class="math inline">\(\bar{Y}_t = \bar{D}_t\bar{Y}^1_t+(1-\bar{D}_t)\bar{Y}^0_t\)</span>, we have that:</p>
<p><span class="math display">\[\begin{align*}
  X&#39;Y &amp; = N_A\left(\begin{array}{c}
              \underbrace{k\bar{Y}^0_B+\bar{Y}^0_A+\bar{D}_A(k\pi(\bar{Y}^1_B-\bar{Y}^0_B)+\bar{Y}^1_A-\bar{Y}^0_A)}_{\mathbf{A}} \\
              \underbrace{\bar{D}_A(k\pi\bar{Y}^1_B+\bar{Y}^1_A)}_{\mathbf{B}}\\
              \underbrace{\bar{Y}^0_A+\bar{D}_A(\bar{Y}^1_A-\bar{Y}^0_A)}_{\mathbf{C}} \\
              \underbrace{\bar{D}_A\bar{Y}^1_A}_{\mathbf{D}}
            \end{array}\right)
\end{align*}\]</span></p>
<p>Using the fact that <span class="math inline">\((X&#39;X)^{-1}=\frac{1}{N_A}\frac{\tilde{x&#39;x}}{\det(x&#39;x)}\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
  \hat{\Theta}^{OLS} &amp; = (X&#39;X)^{-1}X&#39;Y \\
         &amp; = \frac{1}{\bar{D}_Ak(1-\pi\bar{D}_A)}
            \left(\begin{array}{c}
              \bar{D}_A(\mathbf{A}-\mathbf{B}-\mathbf{C}+\mathbf{D}) \\
              -\bar{D}_A\mathbf{A}+\frac{\mathbf{B}}{\pi}+\bar{D}_A\mathbf{C}-\frac{\mathbf{D}}{\pi} \\
              \bar{D}_A(-\mathbf{A}+\mathbf{B}+\frac{k+1-\bar{D}_A(k\pi+1)}{1-\bar{D}_A}\mathbf{C}-\frac{k+1-\bar{D}_A(k\pi+1)}{1-\bar{D}_A}\mathbf{D})\\
              \bar{D}_A\mathbf{A}-\frac{\mathbf{B}}{\pi}-\frac{k+1-\bar{D}_A(k\pi+1)}{1-\bar{D}_A}\bar{D}_A\mathbf{C}+(k\frac{1-\pi\bar{D}_A}{1-\bar{D}_A}+\frac{1}{\pi})\mathbf{D}
            \end{array}\right)
\end{align*}\]</span></p>
<p>Letâs take each term in turn:</p>
<p><span class="math display">\[\begin{align*}
  \hat{\alpha}^{OLS} &amp; = \frac{1}{k(1-\pi\bar{D}_A)}
            \left(k\bar{Y}^0_B+\bar{Y}^0_A+\bar{D}_A(k\pi(\bar{Y}^1_B-\bar{Y}^0_B)+\bar{Y}^1_A-\bar{Y}^0_A) - \bar{D}_A(k\pi\bar{Y}^1_B+\bar{Y}^1_A)\right.\\
                  &amp; \phantom{\frac{1}{k(1-\pi\bar{D}_A)}\left(\right.}
                  \left. - \bar{Y}^0_A-\bar{D}_A(\bar{Y}^1_A-\bar{Y}^0_A)+\bar{D}_A\bar{Y}^1_A\right)\\
                  &amp; = \frac{1}{k(1-\pi\bar{D}_A)}
            \left(\bar{Y}^0_B(k-k\pi\bar{D}_A)+
                  \bar{Y}^0_A(1-\bar{D}_A-1+\bar{D}_A)+
                  \bar{Y}^1_B(k\pi\bar{D}_A-k\pi\bar{D}_A)+ \right.\\
                  &amp; \phantom{\frac{1}{k(1-\pi\bar{D}_A)}\left(\right.}\left. \bar{Y}^1_A(\bar{D}_A-\bar{D}_A-\bar{D}_A+\bar{D}_A)\right) \\
                  &amp; = \bar{Y}^0_Bk\frac{1-\pi\bar{D}_A}{k(1-\pi\bar{D}_A)}\\
                  &amp; = \bar{Y}^0_B
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
  \hat{\mu}^{OLS} &amp; = \frac{1}{k\bar{D}_A(1-\pi\bar{D}_A)}
            \left(-\bar{D}_A(k\bar{Y}^0_B+\bar{Y}^0_A+\bar{D}_A(k\pi(\bar{Y}^1_B-\bar{Y}^0_B)+\bar{Y}^1_A-\bar{Y}^0_A))\right.\\
            &amp; \phantom{= \frac{1}{k\bar{D}_A(1-\pi\bar{D}_A)}}
            \left.+\frac{\bar{D}_A(k\pi\bar{Y}^1_B+\bar{Y}^1_A)}{\pi}+\bar{D}_A(\bar{Y}^0_A+\bar{D}_A(\bar{Y}^1_A-\bar{Y}^0_A))-\frac{\bar{D}_A\bar{Y}^1_A}{\pi}\right)\\
            &amp; = \frac{1}{k\bar{D}_A(1-\pi\bar{D}_A)}
            \left(-\bar{D}_Ak\bar{Y}^0_B(1-\pi\bar{D}_A)-\bar{Y}^0_A\bar{D}_A(1-\bar{D}_A-1+\bar{D}_A)\right.\\
            &amp; \phantom{= \frac{1}{k\bar{D}_A(1-\pi\bar{D}_A)}}
            \left.+\bar{D}_Ak\bar{Y}^1_B(1-\pi\bar{D}_A)+\bar{Y}^1_A(-\bar{D}_A^2+\frac{\bar{D}_A}{\pi}+\bar{D}_A^2-\frac{\bar{D}_A}{\pi})\right) \\
            &amp; = \frac{k\bar{D}_A(1-\pi\bar{D}_A)}{k\bar{D}_A(1-\pi\bar{D}_A)}(\bar{Y}^1_B-\bar{Y}^0_B)\\
            &amp; = \bar{Y}^1_B-\bar{Y}^0_B
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
  \hat{\delta}^{OLS} &amp; = \frac{1}{k(1-\pi\bar{D}_A)}\left(-(k\bar{Y}^0_B+\bar{Y}^0_A+\bar{D}_A(k\pi(\bar{Y}^1_B-\bar{Y}^0_B)+\bar{Y}^1_A-\bar{Y}^0_A))\right. \\
                    &amp; \phantom{=\frac{1}{k(1-\pi\bar{D}_A)}}+\bar{D}_A(k\pi\bar{Y}^1_B+\bar{Y}^1_A)+\frac{k+1-\bar{D}_A(k\pi+1)}{1-\bar{D}_A}(\bar{Y}^0_A+\bar{D}_A(\bar{Y}^1_A-\bar{Y}^0_A))\\
                    &amp; \phantom{=\frac{1}{k(1-\pi\bar{D}_A)}}\left.-\frac{k+1-\bar{D}_A(k\pi+1)}{1-\bar{D}_A}\bar{D}_A\bar{Y}^1_A\right) \\
                    &amp; = \frac{1}{k(1-\pi\bar{D}_A)}\left(-\bar{Y}^0_Bk(1-\pi\bar{D}_A)-\bar{Y}^0_A(1-\bar{D}_A-(k+1-\bar{D}_A(k\pi+1)))\right.\\
                    &amp; \phantom{=\frac{1}{k(1-\pi\bar{D}_A)}}\left.+\bar{Y}^1_B(-k\pi\bar{D}_A+k\pi\bar{D}_A)+\bar{Y}^1_A(-\bar{D}_A+\bar{D}_A+\frac{k+1-\bar{D}_A(k\pi+1)}{1-\bar{D}_A}(\bar{D}_A-\bar{D}_A))\right)\\
                    &amp; = \frac{1}{k(1-\pi\bar{D}_A)}\left(-\bar{Y}^0_Bk(1-\pi\bar{D}_A)+\bar{Y}^0_Ak(1-\pi\bar{D}_A)\right)\\
                    &amp; = \bar{Y}^0_A-\bar{Y}^0_B
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
  \hat{\beta}^{OLS} &amp; =\frac{1}{\bar{D}_Ak(1-\pi\bar{D}_A)}\left(\bar{D}_A(k\bar{Y}^0_B+\bar{Y}^0_A+\bar{D}_A(k\pi(\bar{Y}^1_B-\bar{Y}^0_B)+\bar{Y}^1_A-\bar{Y}^0_A))-\frac{\bar{D}_A(k\pi\bar{Y}^1_B+\bar{Y}^1_A)}{\pi}\right.\\
                    &amp; \phantom{ =\frac{1}{\bar{D}_Ak(1-\pi\bar{D}_A)}}\left.-\frac{k+1-\bar{D}_A(k\pi+1)}{1-\bar{D}_A}\bar{D}_A(\bar{Y}^0_A+\bar{D}_A(\bar{Y}^1_A-\bar{Y}^0_A))+(k\frac{1-\pi\bar{D}_A}{1-\bar{D}_A}+\frac{1}{\pi})\bar{D}_A\bar{Y}^1_A\right) \\
                    &amp; = \frac{1}{\bar{D}_Ak(1-\pi\bar{D}_A)}\left(\bar{Y}^0_B\bar{D}_Ak(1-\pi\bar{D}_A)+\bar{Y}^0_A\bar{D}_A(1-\bar{D}_A-(k+1-\bar{D}_A(k\pi+1)))-\bar{Y}^1_B\bar{D}_Ak(1-\pi\bar{D}_A)\right.\\
                    &amp; \phantom{= \frac{1}{\bar{D}_Ak(1-\pi\bar{D}_A)}}\left.+\bar{Y}^1_A\bar{D}_A(\bar{D}_A-\frac{1}{\pi}-\bar{D}_A\frac{k+1-\bar{D}_A(k\pi+1)}{1-\bar{D}_A}+k\frac{1-\pi\bar{D}_A}{1-\bar{D}_A}+\frac{1}{\pi})\right)\\
                    &amp; = \frac{1}{k(1-\pi\bar{D}_A)}\left(-k(1-\pi\bar{D}_A)(\bar{Y}^1_B-\bar{Y}^0_B)-k(1-\pi\bar{D}_A)\bar{Y}^0_A+  k(1-\pi\bar{D}_A)\bar{Y}^1_A\right)\\
                    &amp; = \bar{Y}^1_A-\bar{Y}^0_A-(\bar{Y}^1_B-\bar{Y}^0_B).
\end{align*}\]</span></p>
<p>So it is confirmed that OLS estimation of the DID model in repeated cross sections of different sizes estimates the same parameters than in panel data.
Thanks to the Law of Large Numbers, we know that:</p>
<p><span class="math display">\[\begin{align*}
  \text{plim}\hat\Theta^{OLS} &amp; =\left(\begin{array}{c}
              \esp{Y^0_{i,B}|D_i=0}\\
              \esp{Y^0_{i,B}|D_i=1}-\esp{Y^0_{i,B}|D_i=1}\\
              \esp{Y^0_{i,A}-Y^0_{i,B}|D_i=0}\\
              \esp{Y^1_{i,A}-Y^0_{i,B}|D_i=1}-\esp{Y^0_{i,A}-Y^0_{i,B}|D_i=0}
            \end{array}\right),
\end{align*}\]</span></p>
<p>where the <span class="math inline">\(\text{plim}\)</span> is taken over <span class="math inline">\(N=N_A+N_B\)</span>.</p>
<p>In order to study more easily the DID model as estimated by OLS, we are going to rewrite it as a pure cross sectional model:</p>
<p><span class="math display">\[\begin{align*}
  Y_j &amp; = \alpha + \mu D_j + \delta T_j + \beta D_jT_j + \epsilon_j,
\end{align*}\]</span></p>
<p>where <span class="math inline">\(j=i\)</span> when <span class="math inline">\(t=B\)</span> and <span class="math inline">\(j=N_B+i\)</span> when <span class="math inline">\(t=A\)</span>, <span class="math inline">\(D_j=D_i^t\)</span>, <span class="math inline">\(T_j=T_{i,t}\)</span> and <span class="math inline">\(Y_j=Y_{i,t}\)</span>.
In that case, we are assuming that <span class="math inline">\(T_i\)</span> is a random variable, whereas in real life, the sample is stratified with respect to <span class="math inline">\(T_i\)</span>.
We will treat this case in the stratification section.
For now, assuming that time is sampled as a usual random variable is a useful simplification.</p>
<p>From what we have proven above, we know that:</p>
<p><span class="math display">\[\begin{align*}
  \epsilon_{j} &amp; = Y_{j}-\left(\esp{Y^0_{j}|D_j=0,T_j=0}+D_j(\esp{Y^0_{j}|D_j=1,T_j=0}-\esp{Y^0_{j}|D_j=0,T_j=0})\right.\\
                &amp; \phantom{=Y_{j}-\left(\right.}+T_j(\esp{Y^0_{j}|D_j=0,T_j=1}-\esp{Y^0_{j}|D_j=0,T_j=0})\\
                  &amp; \phantom{=Y_{j}-\left(\right.}+D_jT_j(\esp{Y^1_{j}|D_j=1,T_j=1}-\esp{Y^0_{j}|D_j=1,T_j=0}\\
                  &amp; \phantom{=Y_{j}-\left(\right.+D_jT_j}\left.-(\esp{Y^0_{j}|D_j=0,T_j=1}-\esp{Y^0_{j}|D_j=0,T_j=0}))\right)
\end{align*}\]</span></p>
<p>With this notation, and <span class="math inline">\(N=N_A+N_B\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
  \frac{\sqrt{N}}{N}X&#39;\epsilon &amp; =\sqrt{N}\left(\begin{array}{c}
              \frac{1}{N}\sum_{i=1}^N\epsilon_{j} \\
              \frac{1}{N}\sum_{i=1}^ND_j\epsilon_{j} \\
              \frac{1}{N}\sum_{i=1}^NT_j\epsilon_{j} \\
              \frac{1}{N}\sum_{i=1}^ND_jT_j\epsilon_{j} 
            \end{array}\right).
\end{align*}\]</span></p>
<p>In order to be able to use the vector CLT in order to study the distribution of these quantities, we need first to compute the expectation of these variables.
Let us first start with <span class="math inline">\(\esp{D_jT_j\epsilon_{j}}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
  \esp{D_jT_j\epsilon_{j}} &amp; =\esp{\epsilon_{j}|D_j=1,T_j=1}\Pr(D_j=1|T_j=1)\Pr(T_j=1)\\
                          &amp; = 0,
\end{align*}\]</span></p>
<p>where the first equality follows from Bayesâ Law and the second equality from the definition of <span class="math inline">\(\epsilon_{j}\)</span>.
Using the same reasoning, we have:</p>
<p><span class="math display">\[\begin{align*}
  \esp{D_j\epsilon_{j}} &amp; =\esp{\epsilon_{j}|D_j=1,T_j=1}\Pr(T_j=1|D_j=1)+\esp{\epsilon_{j}|D_j=1,T_j=0}\Pr(T_j=0|D_j=1)\\
                          &amp; = 0\\
  \esp{T_j\epsilon_{j}} &amp; =\esp{\epsilon_{j}|D_j=1,T_j=1}\Pr(D_j=1|T_j=1)+\esp{\epsilon_{j}|D_j=0,T_j=1}\Pr(D_j=0|T_j=1)\\
                          &amp; = 0\\
  \esp{\epsilon_{j}} &amp; =\esp{\epsilon_{j}|T_j=1}\Pr(T_j=1)+\esp{\epsilon_{j}|T_j=0}\Pr(T_j=0)\\
                          &amp; = (\esp{\epsilon_{j}|T_j=0,D_j=1}\Pr(D_j=1|T_j=0)+\esp{\epsilon_{j}|T_j=0,D_j=0}\Pr(D_j=0|T_j=0))\Pr(T_j=0)\\
                          &amp; = 0.
\end{align*}\]</span></p>
<p>Using the vector version of the CLT that we have already invoked in the proof of Lemma <a href="proofs.html#lem:asympOLS">A.4</a>, we have that <span class="math inline">\(\sqrt{N}\frac{X&#39;\epsilon}{N}\sim\mathcal{N}((0,0,0,0),\mathbf{V_{x\epsilon}})\)</span> with:</p>
<p><span class="math display">\[\begin{align*}
  \mathbf{V_{x\epsilon}} &amp; = \esp{\left(\begin{array}{c}  \epsilon_i  \\    \epsilon_iD_i \\ \epsilon_iT_i \\ \epsilon_iD_iT_i \end{array}\right)
                                  \left(\begin{array}{cccc} \epsilon_i &amp;    \epsilon_iD_i &amp;  \epsilon_iT_i &amp;  \epsilon_iD_iT_i \end{array}\right)}
- \esp{\left(\begin{array}{c}  \epsilon_i  \\   \epsilon_iD_i \\ \epsilon_iT_i \\ \epsilon_iD_iT_i \end{array}\right)}
                    \esp{\left(\begin{array}{cccc}  \epsilon_i &amp;    \epsilon_iD_i &amp;  \epsilon_iT_i &amp;  \epsilon_iD_iT_i \end{array}\right)}\\
                &amp; = \esp{\epsilon_i^2\left(\begin{array}{cccc}
                                              1 &amp; D_i &amp; T_i &amp; D_iT_i \\
                                              D_i &amp; D_i &amp;   D_iT_i &amp; D_iT_i \\
                                              T_i &amp; D_iT_i &amp;    T_i &amp; D_iT_i \\
                                              D_iT_i &amp; D_iT_i &amp; D_iT_i &amp; D_iT_i
                                            \end{array}\right)},
\end{align*}\]</span></p>
<p>where we made use of the fact that <span class="math inline">\(T_i^2=T_i\)</span> and <span class="math inline">\(D_i^2=D_i\)</span>.</p>
<p>Before we can use the Delta Method to derive the distribution of the OLS DID estimator, we need to compute <span class="math inline">\(\text{plim}N(X&#39;X)-1\)</span> as a function of <span class="math inline">\(N\)</span> and not <span class="math inline">\(N_A\)</span>, as we did before.
In order to obtain that without redoing the matrix inversion all over again (which is pretty awful without the trick of factoring <span class="math inline">\(N_A\)</span>), we are going to use the fact that the proportion of observations belonging to period <span class="math inline">\(A\)</span> is equal to <span class="math inline">\(\bar{T}_A=\frac{N_A}{N}=\sum_{j=1}^NT_j\)</span>, and the proportion of observations belonging to period <span class="math inline">\(B\)</span> is equal to <span class="math inline">\(1-\bar{T}_A\)</span>.
We also have that <span class="math inline">\(k=\frac{N_B}{N_A}=\frac{(1-\bar{T}_A)N}{\bar{T}_AN}=\frac{1-\bar{T}_A}{\bar{T}_A}\)</span>.
Finally, note that <span class="math inline">\(\pi=\frac{\bar{D}_B}{\bar{D}_A}\)</span>.
As a consequence of that and of our previous computations, we have that:</p>
<p><span class="math display">\[\begin{align*}
  (X&#39;X)^{-1} &amp; = \frac{1}{N_A}\frac{1}{k\bar{D}_A(1-\pi\bar{D}_A)}
                  \left(\begin{array}{cccc}
                    \bar{D}_A &amp; -\bar{D}_A &amp; -\bar{D}_A &amp; \bar{D}_A\\
                    -\bar{D}_A &amp; \frac{1}{\pi} &amp; \bar{D}_A &amp; -\frac{1}{\pi} \\
                    -\bar{D}_A &amp; \bar{D}_A &amp; \bar{D}_A\frac{k+1-\bar{D}_A(k\pi+1)}{1-\bar{D}_A} &amp; -\bar{D}_A\frac{k+1-\bar{D}_A(k\pi+1)}{1-\bar{D}_A}\\
                    \bar{D}_A &amp; -\frac{1}{\pi} &amp; -\bar{D}_A\frac{k+1-\bar{D}_A(k\pi+1)}{1-\bar{D}_A} &amp; k\frac{1-\pi\bar{D}_A}{1-\bar{D}_A}+\frac{1}{\pi}
            \end{array}\right) \\
          &amp; =   \frac{1}{N\bar{T}_A}\frac{1}{\frac{1-\bar{T}_A}{\bar{T}_A}\bar{D}_A(1-\frac{\bar{D}_B}{\bar{D}_A}\bar{D}_A)}\\
          &amp; \phantom{=}
                  \left(\begin{array}{cccc}
                    \bar{D}_A &amp; -\bar{D}_A &amp; -\bar{D}_A &amp; \bar{D}_A\\
                    -\bar{D}_A &amp; \frac{\bar{D}_A}{\bar{D}_B} &amp; \bar{D}_A &amp; -\frac{\bar{D}_A}{\bar{D}_B} \\
                    -\bar{D}_A &amp; \bar{D}_A &amp; \bar{D}_A\frac{\frac{1-\bar{T}_A}{\bar{T}_A}+1-\bar{D}_A(\frac{1-\bar{T}_A}{\bar{T}_A}\frac{\bar{D}_B}{\bar{D}_A}+1)}{1-\bar{D}_A} &amp; -\bar{D}_A\frac{\frac{1-\bar{T}_A}{\bar{T}_A}+1-\bar{D}_A(\frac{1-\bar{T}_A}{\bar{T}_A}\frac{\bar{D}_B}{\bar{D}_A}+1)}{1-\bar{D}_A}\\
                    \bar{D}_A &amp; -\frac{\bar{D}_A}{\bar{D}_B} &amp; -\bar{D}_A\frac{\frac{1-\bar{T}_A}{\bar{T}_A}+1-\bar{D}_A(\frac{1-\bar{T}_A}{\bar{T}_A}\frac{\bar{D}_B}{\bar{D}_A}+1)}{1-\bar{D}_A} &amp; \frac{1-\bar{T}_A}{\bar{T}_A}\frac{1-\frac{\bar{D}_B}{\bar{D}_A}\bar{D}_A}{1-\bar{D}_A}+\frac{\bar{D}_A}{\bar{D}_B}
            \end{array}\right) \\
           &amp; =   \frac{1}{N}\frac{1}{(1-\bar{T}_A)(1-\bar{D}_B)}\\
          &amp; \phantom{=}
                  \left(\begin{array}{cccc}
                    1 &amp; -1 &amp; -1 &amp; 1\\
                    -1 &amp; \frac{1}{\bar{D}_B} &amp; 1 &amp; -\frac{1}{\bar{D}_B} \\
                    -1 &amp; 1 &amp; \frac{1-\bar{D}_B+\bar{T}_A(\bar{D}_B-\bar{D}_A)}{\bar{T}_A(1-\bar{D}_A)} &amp; -\frac{1-\bar{D}_B+\bar{T}_A(\bar{D}_B-\bar{D}_A)}{\bar{T}_A(1-\bar{D}_A)}\\
                    1 &amp; -\frac{1}{\bar{D}_B} &amp; -\frac{1-\bar{D}_B+\bar{T}_A(\bar{D}_B-\bar{D}_A)}{\bar{T}_A(1-\bar{D}_A)} &amp; \frac{1}{\bar{D}_A}\frac{1-\bar{T}_A}{\bar{T}_A}\frac{1-\bar{D}_B}{1-\bar{D}_A}+\frac{1}{\bar{D}_B}
            \end{array}\right),
\end{align*}\]</span></p>
<p>because:</p>
<p><span class="math display">\[\begin{align*}
\frac{\frac{1-\bar{T}_A}{\bar{T}_A}+1-\bar{D}_A(\frac{1-\bar{T}_A}{\bar{T}_A}\frac{\bar{D}_B}{\bar{D}_A}+1)}{1-\bar{D}_A} &amp; = \frac{1+\frac{1-\bar{T}_A}{\bar{T}_A}-\frac{1-\bar{T}_A}{\bar{T}_A}\bar{D}_B+ \bar{D}_A}{1-\bar{D}_A} \\
= &amp; \frac{\bar{T}_A+1-\bar{T}_A-\bar{D}_B+\bar{T}_A\bar{D}_B+ \bar{D}_A\bar{T}_A}{\bar{T}_A(1-\bar{D}_A)}\\
= &amp; \frac{1-\bar{D}_B+\bar{T}_A(\bar{D}_B-\bar{D}_A)}{\bar{T}_A(1-\bar{D}_A)}.
\end{align*}\]</span></p>
<p>As a consequence, we have:</p>
<p><span class="math display">\[\begin{align*}
  \text{plim}N(X&#39;X)^{-1} &amp; = \frac{1}{(1-p_A)(1-p)}
                  \left(\begin{array}{cccc}
                    1 &amp; -1 &amp; -1 &amp; 1\\
                    -1 &amp; \frac{1}{p} &amp; 1 &amp; -\frac{1}{p} \\
                    -1 &amp; 1 &amp; \frac{1}{p_A} &amp; -\frac{1}{p_A} \\
                    1 &amp; -\frac{1}{p} &amp; -\frac{1}{p_A}  &amp; \frac{1}{pp_A}
            \end{array}\right),
\end{align*}\]</span></p>
<p>using the Law of Large Numbers, Slutskyâs Theorem and the fact that <span class="math inline">\(\text{plim}\bar{T}_A=p_A\)</span>, the proportion of observations stemming from the After period, <span class="math inline">\(\text{plim}\bar{D}_A=\text{plim}\bar{D}_B=p\)</span> and the fact that <span class="math inline">\(\frac{1}{p}\frac{1-p_A}{p_A}+\frac{1}{p}=\frac{1}{pp_A}\)</span>.</p>
<p>Now, we can derive the asymptotic distribution of <span class="math inline">\(\sqrt{N}(\hat{\Theta}_{OLS}-\Theta)=N(X&#39;X)^{-1}\frac{\sqrt{N}}{N}X&#39;\epsilon\)</span>.
Using the Delta Method, we have that <span class="math inline">\(\sqrt{N}(\hat{\Theta}_{OLS}-\Theta)\stackrel{d}{\rightarrow}\mathcal{N}\left((0,0,0,0),\sigma_{XX}^{-1}\mathbf{V_{x\epsilon}}\sigma_{XX}^{-1}\right)\)</span>.
So weâre in for a treat: deriving the lower diagonal term in the quadratic form <span class="math inline">\(\sigma_{XX}^{-1}\mathbf{V_{x\epsilon}}\sigma_{XX}^{-1}\)</span>.</p>
<p>Let us start.
We first need the four terms on the last line of <span class="math inline">\((1-p_A)^2(1-p)^2\sigma_{XX}^{-1}\mathbf{V_{x\epsilon}}=(\mathbf{A},\mathbf{B},\mathbf{C},\mathbf{D})\)</span> (the squared terms in the beginning are accounting for the constant terms in the matrix multiplication):</p>
<p><span class="math display">\[\begin{align*}
  \mathbf{A}&amp; = \esp{\epsilon_j^2} -\frac{1}{p}\esp{\epsilon_j^2D_j} -\frac{1}{p_A}\esp{\epsilon_j^2T_j} + \frac{1}{pp_A}\esp{\epsilon_j^2D_jT_j}\\
            &amp; =  \esp{\epsilon_j^2|D_j=0,T_j=0}\Pr(D_j=0|T_j=0)\Pr(T_j=0)+\esp{\epsilon_j^2|D_j=1,T_j=0}\Pr(D_j=1|T_j=0)\Pr(T_j=0)\\
            &amp; \phantom{=}+\esp{\epsilon_j^2|D_j=0,T_j=1}\Pr(D_j=0|T_j=1)\Pr(T_j=1)+\esp{\epsilon_j^2|D_j=1,T_j=1}\Pr(D_j=1|T_j=1)\Pr(T_j=1) \\
            &amp; \phantom{=}-\frac{1}{p}\esp{\epsilon_j^2|D_j=1}\Pr(D_j=1) -\frac{1}{p_A}\esp{\epsilon_j^2|T_j=1}\Pr(T_j=1) \\
            &amp; \phantom{=}+ \frac{1}{pp_A}\esp{\epsilon_j^2|D_j=1,T_j=1}\Pr(D_j=1|T_j=1)\Pr(T_j=1)\\
            &amp; =  \esp{\epsilon_j^2|D_j=0,T_j=0}(1-p)(1-p_A)+\esp{\epsilon_j^2|D_j=1,T_j=0}p(1-p_A)\\
            &amp; \phantom{=}+\esp{\epsilon_j^2|D_j=0,T_j=1}(1-p)p_A+\esp{\epsilon_j^2|D_j=1,T_j=1}pp_A \\
            &amp; \phantom{=}-\esp{\epsilon_j^2|D_j=1} -\esp{\epsilon_j^2|T_j=1} \\
            &amp; \phantom{=}+ \esp{\epsilon_j^2|D_j=1,T_j=1}\\
            &amp; =  \esp{\epsilon_j^2|D_j=0,T_j=0}(1-p)(1-p_A)+\esp{\epsilon_j^2|D_j=1,T_j=0}p(1-p_A)\\
            &amp; \phantom{=}+\esp{\epsilon_j^2|D_j=0,T_j=1}(1-p)p_A+\esp{\epsilon_j^2|D_j=1,T_j=1}pp_A \\
            &amp; \phantom{=}-\esp{\epsilon_j^2|D_j=1,T_j=0}(1-p_A)-\esp{\epsilon_j^2|D_j=1,T_j=1}p_A\\
            &amp; \phantom{=}-\esp{\epsilon_j^2|D_j=0,T_j=1}(1-p)-\esp{\epsilon_j^2|D_j=1,T_j=1}p \\
            &amp; \phantom{=}+ \esp{\epsilon_j^2|D_j=1,T_j=1}\\
            &amp; =  \esp{\epsilon_j^2|D_j=0,T_j=0}(1-p)(1-p_A)\\
            &amp; \phantom{=}-\esp{\epsilon_j^2|D_j=1,T_j=0}(1-p)(1-p_A)\\
            &amp; \phantom{=}-\esp{\epsilon_j^2|D_j=0,T_j=1}(1-p)(1-p_A)\\
            &amp; \phantom{=}+\esp{\epsilon_j^2|D_j=1,T_j=1}(1-p)(1-p_A)\\
            &amp; = (1-p)(1-p_A)(\sigma_{\epsilon_{0,0}}^2-\sigma_{\epsilon_{1,0}}^2-\sigma_{\epsilon_{0,1}}^2+\sigma_{\epsilon_{1,1}}^2)\\
  \mathbf{B}&amp; = \esp{\epsilon_j^2D_j} -\frac{1}{p}\esp{\epsilon_j^2D_j} -\frac{1}{p_A}\esp{\epsilon_j^2D_jT_j} + \frac{1}{pp_A}\esp{\epsilon_j^2D_jT_j}\\
            &amp; = \esp{\epsilon_j^2D_j}(1-\frac{1}{p})+\esp{\epsilon_j^2D_jT_j}\frac{1}{p_A}(\frac{1}{p}-1) \\
            &amp; = (1-\frac{1}{p})\esp{\epsilon_j^2|D_j=1}\Pr(D_j=1)+\frac{1}{p_A}(\frac{1}{p}-1)\esp{\epsilon_j^2|D_j=1,T_j=1}\Pr(D_j=1|T_j=1)\Pr(T_j=1) \\
            &amp; = p(1-\frac{1}{p})\left(\esp{\epsilon_j^2|D_j=1,T_j=0}\Pr(T_j=0|D_j=1)+\esp{\epsilon_j^2|D_j=1,T_j=1}\Pr(T_j=1|D_j=1)\right)\\
            &amp; \phantom{=}+\frac{1}{p_A}(\frac{1}{p}-1)\sigma_{\epsilon_{1,1}}^2pp_A\\
            &amp; = p(1-\frac{1}{p})\left(\sigma_{\epsilon_{1,0}}^2(1-p_A)+\sigma_{\epsilon_{1,1}}^2p_A\right)+\frac{1}{p_A}(\frac{1}{p}-1)\sigma_{\epsilon_{1,1}}^2pp_A\\
            &amp; = -(1-p)\left(\sigma_{\epsilon_{1,0}}^2(1-p_A)+\sigma_{\epsilon_{1,1}}^2p_A\right)+(1-p)\sigma_{\epsilon_{1,1}}^2\\
            &amp; = (1-p)\left(\sigma_{\epsilon_{1,1}}^2-\sigma_{\epsilon_{1,0}}^2(1-p_A)-\sigma_{\epsilon_{1,1}}^2p_A\right)\\
            &amp; = (1-p)(1-p_A)\left(\sigma_{\epsilon_{1,1}}^2-\sigma_{\epsilon_{1,0}}^2\right) \\
  \mathbf{C}&amp; = \esp{\epsilon_j^2T_j} -\frac{1}{p}\esp{\epsilon_j^2D_jT_j} -\frac{1}{p_A}\esp{\epsilon_j^2T_j} + \frac{1}{pp_A}\esp{\epsilon_j^2D_jT_j}\\
            &amp; = \esp{\epsilon_j^2|T_j=1}\Pr(T_j=1)-\frac{1}{p}\esp{\epsilon_j^2|D_j=1,T_j=1}\Pr(D_j=1|T_j=1)\Pr(T_j=1)\\
            &amp; \phantom{=}-\frac{1}{p_A}\esp{\epsilon_j^2|T_j=1}\Pr(T_j=1)+\frac{1}{pp_A}\esp{\epsilon_j^2|D_j=1,T_j=1}\Pr(D_j=1|T_j=1)\Pr(T_j=1)\\
            &amp; = -(1-p_A)(\esp{\epsilon_j^2|D_j=1,T_j=1}\Pr(D_j=1|T_j=1)+\esp{\epsilon_j^2|D_j=0,T_j=1}\Pr(D_j=0|T_j=1))\\
            &amp; \phantom{=}+\esp{\epsilon_j^2|D_j=1,T_j=1}(1-p_A)\\
            &amp; = (1-p_A)(\sigma_{\epsilon_{1,1}}^2(1-p)-(1-p)\sigma_{\epsilon_{0,1}}^2)\\
            &amp; = (1-p)(1-p_A)(\sigma_{\epsilon_{1,1}}^2-\sigma_{\epsilon_{0,1}}^2)\\
  \mathbf{D}&amp; = \esp{\epsilon_j^2D_jT_j} -\frac{1}{p}\esp{\epsilon_j^2D_jT_j} -\frac{1}{p_A}\esp{\epsilon_j^2D_jT_j} + \frac{1}{pp_A}\esp{\epsilon_j^2D_jT_j}\\
            &amp; = \esp{\epsilon_j^2|D_j=1,T_j=1}\Pr(D_j=1|T_j=1)\Pr(T_j=1)(1-\frac{1}{p}-\frac{1}{p_A}+\frac{1}{pp_A})\\
            &amp; = \sigma_{\epsilon_{1,1}}^2(pp_A-p_A-p+1)\\
            &amp; = (1-p)(1-p_A)\sigma_{\epsilon_{1,1}}^2
\end{align*}\]</span></p>
<p>since <span class="math inline">\(1-p-p_A+pp_A=(1-p)(1-p_A)\)</span>, and where <span class="math inline">\(\sigma_{\epsilon_{d,t}}^2=\esp{\epsilon_j^2|D_j=d,T_j=t}\)</span>.
We also make use of the fact that <span class="math inline">\(\Pr(D_j=d|T_j=t)=\Pr(D_j=d)\)</span> and <span class="math inline">\(\Pr(T_j=t|D_j=d)=\Pr(T_j=t)\)</span>, that is that the participants and non participants are sampled exactly in the same proportion in both periods.</p>
<p>Let us now obtain <span class="math inline">\(\var{\hat{\beta}^{OLS}}\)</span>, the variance of the <span class="math inline">\(\hat{\beta}^{OLS}\)</span> parameter.
It is the last diagonal term of the matrix <span class="math inline">\(\sigma_{XX}^{-1}\mathbf{V_{x\epsilon}}\sigma_{XX}^{-1}\)</span>.
We know that:</p>
<p><span class="math display">\[\begin{align*}
  \var{\hat{\beta}^{OLS}} &amp; = \frac{1}{(1-p)^2(1-p_A)^2}\left(\mathbf{A}-\frac{1}{p}\mathbf{B}-\frac{1}{p_A}\mathbf{C}+\frac{1}{pp_A}\mathbf{D}\right)\\
                          &amp; = \frac{1}{(1-p)(1-p_A)}\left(\sigma_{\epsilon_{0,0}}^2-\sigma_{\epsilon_{1,0}}^2-\sigma_{\epsilon_{0,1}}^2+\sigma_{\epsilon_{1,1}}^2
                                                          -\frac{1}{p}(\sigma_{\epsilon_{1,1}}^2-\sigma_{\epsilon_{1,0}}^2) \right.\\
                          &amp; \phantom{= \frac{1}{(1-p)(1-p_A)}\left(\right.}\left.
                                                          -\frac{1}{p_A}(\sigma_{\epsilon_{1,1}}^2-\sigma_{\epsilon_{0,1}}^2)+ \frac{1}{pp_A}\sigma_{\epsilon_{1,1}}^2\right)\\
                          &amp; = \frac{1}{(1-p)(1-p_A)}\left(\sigma_{\epsilon_{0,0}}^2+\sigma_{\epsilon_{1,0}}^2(-1+\frac{1}{p})+\sigma_{\epsilon_{0,1}}^2(-1+\frac{1}{p_A})+\sigma_{\epsilon_{1,1}}^2(1-\frac{1}{p}-\frac{1}{p_A}+\frac{1}{pp_A})\right)\\
                          &amp; = \frac{1}{(1-p)(1-p_A)}\left(\sigma_{\epsilon_{0,0}}^2+\sigma_{\epsilon_{1,0}}^2\frac{1-p}{p}+\sigma_{\epsilon_{0,1}}^2(\frac{1-p_A}{p_A}+\sigma_{\epsilon_{1,1}}^2\frac{pp_A-p_A-p+1}{pp_A}\right)\\
                          &amp; = \frac{\sigma_{\epsilon_{0,0}}^2}{(1-p)(1-p_A)}+\frac{\sigma_{\epsilon_{1,0}}^2}{p(1-p_A)}+\frac{\sigma_{\epsilon_{0,1}}^2}{(1-p)p_A}+\frac{\sigma_{\epsilon_{1,1}}^2}{pp_A},
\end{align*}\]</span></p>
<p>using again the fact that <span class="math inline">\(1-p-p_A+pp_A=(1-p)(1-p_A)\)</span>.</p>
<p>Finally, using the formula for <span class="math inline">\(\epsilon_j\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
 \sigma_{\epsilon_{0,0}}^2 &amp; = \esp{\epsilon_j^2|D_j=0,T_j=0} \\
                          &amp; = \esp{(Y_j-\esp{Y^0_j|D_j=0,T_j=0})^2|D_j=0,T_j=0}\\
                          &amp; = \esp{(Y^0_{i,B}-\esp{Y^0_{i,B}|D_i=0)^2}|D_i=0}\\
                          &amp; = \var{Y^0_{i,B}|D_i=0}\\
 \sigma_{\epsilon_{1,0}}^2 &amp; = \esp{\epsilon_j^2|D_j=1,T_j=0} \\
                          &amp; = \esp{(Y_j-\esp{Y^0_j|D_j=1,T_j=0})^2|D_j=1,T_j=0}\\
                          &amp; = \esp{(Y^0_{i,B}-\esp{Y^0_{i,B}|D_i=1)^2}|D_i=1}\\
                          &amp; = \var{Y^0_{i,B}|D_i=1}\\
 \sigma_{\epsilon_{0,1}}^2 &amp; = \esp{\epsilon_j^2|D_j=0,T_j=1} \\
                          &amp; = \esp{(Y_j-\esp{Y^0_j|D_j=0,T_j=1})^2|D_j=0,T_j=1}\\
                          &amp; = \esp{(Y^0_{i,A}-\esp{Y^0_{i,A}|D_i=0)^2}|D_i=0}\\
                          &amp; = \var{Y^0_{i,A}|D_i=0}\\
 \sigma_{\epsilon_{1,1}}^2 &amp; = \esp{\epsilon_j^2|D_j=1,T_j=1} \\
                          &amp; = \esp{(Y_j-\esp{Y^1_j|D_j=1,T_j=1})^2|D_j=1,T_j=1}\\
                          &amp; = \esp{(Y^1_{i,A}-\esp{Y^1_{i,A}|D_i=1)^2}|D_i=1}\\
                          &amp; = \var{Y^1_{i,A}|D_i=1}
\end{align*}\]</span></p>
<p>This proves the result.</p>
</div>
<div id="proofEquivDIDSApop" class="section level3 hasAnchor" number="16.3.3">
<h3><span class="header-section-number">A.3.3</span> Proof of Theorem <a href="NE.html#thm:EquivDIDSApop">4.14</a><a href="proofs.html#proofEquivDIDSApop" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The proof uses saturated models as <a href="https://press.princeton.edu/books/paperback/9780691120355/mostly-harmless-econometrics">Angrist and Pischke(2009)</a>.
A saturated model is a model involving only categorical variables where the model has a separate parameter for each various sets of parameter values that
the covariates can take.
We can check that Sun and Abrahamâs model is a saturated model.
Letâs start with the model in repeated cross sections (with group fixed effects).
In the population, excluding the group of individuals that are always treated (adding this group would entail adding a separate dummy for
each date at which they are observed, I leave that as an exercise), with <span class="math inline">\(T=4\)</span> (larger time series do not change the basic result):</p>
<p><span class="math display">\[\begin{align*} 
\esp{Y_{i,1}|D_i=\infty} &amp; = \alpha \\ 
\esp{Y_{i,2}|D_i=\infty} &amp;= \alpha+\delta_2  \\ 
\esp{Y_{i,3}|D_i=\infty} &amp; = \alpha+\delta_3  \\
\esp{Y_{i,4}|D_i=\infty} &amp; = \alpha+\delta_4  \\ 
\esp{Y_{i,1}|D_i=2} &amp; = \alpha +\mu_2  \\ 
\esp{Y_{i,2}|D_i=2} &amp; = \alpha +\mu_2 +\delta_2 + \beta_{2,0}^{SA}\\ 
\esp{Y_{i,3}|D_i=2} &amp; = \alpha +\mu_2 +\delta_3 + \beta_{2,1}^{SA}  \\
\esp{Y_{i,4}|D_i=2} &amp; = \alpha +\mu_2 +\delta_4 + \beta_{2,2}^{SA}  \\
\esp{Y_{i,1}|D_i=3} &amp; = \alpha +\mu_3 +          \beta_{3,-2}^{SA}   \\
\esp{Y_{i,2}|D_i=3} &amp; = \alpha +\mu_3 + \delta_2   \\ 
\esp{Y_{i,3}|D_i=3} &amp; = \alpha +\mu_3 + \delta_3 + \beta_{3,0}^{SA}  \\ 
\esp{Y_{i,4}|D_i=3} &amp; = \alpha+\mu_3 + \delta_4 + \beta_{3,1}^{SA}  \\ 
\esp{Y_{i,1}|D_i=4} &amp; = \alpha +\mu_4 +\beta_{4,-3}^{SA}   \\ 
\esp{Y_{i,2}|D_i=4} &amp; = \alpha +\mu_4 + \delta_2+\beta_{4,-2}^{SA}   \\ 
\esp{Y_{i,3}|D_i=4} &amp; = \alpha +\mu_4 + \delta_3   \\
\esp{Y_{i,4}|D_i=4} &amp; = \alpha +\mu_4 + \delta_4 + \beta_{4,0}^{SA} 
\end{align*}\]</span></p>
<p>The model has 16 parameters to model the 16 different combinations of the regressors.
It is thus a saturated model.
Let us now state the Linear Conditional Expectation Function Theorem:</p>
<div class="theorem">
<p><span id="thm:LinearCEF" class="theorem"><strong>Theorem A.2  (Linear Conditional Expectation Function) </strong></span>Let <span class="math inline">\(\esp{Y_i|X_i}=X_i&#39;\Theta^*\)</span> for a <span class="math inline">\(K\times 1\)</span> vector of coefficients <span class="math inline">\(\Theta^*\)</span>. Then <span class="math inline">\(\Theta^*=\esp{X_i&#39;X_i}^{-1}\esp{X_i&#39;Y_i}=\Theta^{OLS}\)</span>.</p>
</div>
<p>Theorem <a href="proofs.html#thm:LinearCEF">A.2</a> states that the coefficients of a model with a
linear conditional expectation function can be obtained by using OLS. Applying
Theorem <a href="proofs.html#thm:LinearCEF">A.2</a> to Sun and Abrahamâs saturated model, we have that:</p>
<p><span class="math display">\[\begin{align*} 
\alpha &amp; = \esp{Y_{i,1}|D_i=\infty}=\alpha^{OLS}  \\ 
\delta_2 &amp; =\esp{Y_{i,2}|D_i=\infty}-\esp{Y_{i,1}|D_i=\infty}=\delta_2^{OLS}  \\ 
\delta_3 &amp;= \esp{Y_{i,3}|D_i=\infty}-\esp{Y_{i,1}|D_i=\infty}=\delta_3^{OLS}   \\ 
\delta_4 &amp; = \esp{Y_{i,4}|D_i=\infty}-\esp{Y_{i,1}|D_i=\infty}=\delta_4^{OLS}   \\ 
\mu_2 &amp; = \esp{Y_{i,1}|D_i=2}-\esp{Y_{i,1}|D_i=\infty}     =\mu_2^{OLS}   \\ 
\mu_3 &amp; = \esp{Y_{i,2}|D_i=3}-\esp{Y_{i,2}|D_i=\infty}     =\mu_3^{OLS}   \\ 
\mu_4 &amp; = \esp{Y_{i,3}|D_i=4}-\esp{Y_{i,3}|D_i=\infty}     =\mu_4^{OLS}   \\
\beta_{2,0}^{SA} &amp; = \esp{Y_{i,2}|D_i=2}-\esp{Y_{i,1}|D_i=2}-(\esp{Y_{i,2}|D_i=\infty}-\esp{Y_{i,1}|D_i=\infty}) =\beta_{2,0}^{OLS} \\ 
\beta_{2,1}^{SA} &amp; = \esp{Y_{i,3}|D_i=2}-\esp{Y_{i,1}|D_i=2}-(\esp{Y_{i,3}|D_i=\infty}-\esp{Y_{i,1}|D_i=\infty})=\beta_{2,1}^{OLS} \\ 
\beta_{2,2}^{SA} &amp; = \esp{Y_{i,4}|D_i=2}-\esp{Y_{i,1}|D_i=2}-(\esp{Y_{i,4}|D_i=\infty}-\esp{Y_{i,1}|D_i=\infty})=\beta_{2,2}^{OLS} \\ 
\beta_{3,-2}^{SA} &amp; =\esp{Y_{i,1}|D_i=3}-\esp{Y_{i,2}|D_i=3}-(\esp{Y_{i,1}|D_i=\infty}-\esp{Y_{i,2}|D_i=\infty})=\beta_{3,-2}^{OLS}\\ 
\beta_{3,0}^{SA} &amp; =\esp{Y_{i,3}|D_i=3}-\esp{Y_{i,2}|D_i=3}-(\esp{Y_{i,3}|D_i=\infty}-\esp{Y_{i,2}|D_i=\infty})=\beta_{3,0}^{OLS}\\ 
\beta_{3,1}^{SA} &amp; =\esp{Y_{i,4}|D_i=3}-\esp{Y_{i,2}|D_i=3}-(\esp{Y_{i,4}|D_i=\infty}-\esp{Y_{i,2}|D_i=\infty})=\beta_{3,1}^{OLS}\\ 
\beta_{4,-3}^{SA} &amp; =\esp{Y_{i,1}|D_i=4}-\esp{Y_{i,3}|D_i=4}-(\esp{Y_{i,1}|D_i=\infty}-\esp{Y_{i,3}|D_i=\infty})=\beta_{4,-3}^{OLS}\\ 
\beta_{4,-2}^{SA} &amp; =\esp{Y_{i,2}|D_i=4}-\esp{Y_{i,3}|D_i=4}-(\esp{Y_{i,2}|D_i=\infty}-\esp{Y_{i,3}|D_i=\infty})=\beta_{4,-2}^{OLS}\\ 
\beta_{4,0}^{SA} &amp; =\esp{Y_{i,4}|D_i=4}-\esp{Y_{i,3}|D_i=4}-(\esp{Y_{i,4}|D_i=\infty}-\esp{Y_{i,3}|D_i=\infty})=\beta_{4,0}^{OLS}
\end{align*}\]</span></p>
<p>This proves that Sun and Abrahamâs estimator is actually equal to the individual DID estimators <span class="math inline">\(\beta_{d,\tau}^{SA}=\Delta^{Y}_{DID}(d,\infty,\tau,d-1)\)</span> in the population, which completes the proof for the model with group fixed effects and
<span class="math inline">\(T=4\)</span>.
I leave generalizing this result to any <span class="math inline">\(T\)</span> and to the panel data model with individual fixed effects as an exercise.</p>
</div>
<div id="proofEquivDIDSAsamp" class="section level3 hasAnchor" number="16.3.4">
<h3><span class="header-section-number">A.3.4</span> Proof of Theorem <a href="NE.html#thm:EquivDIDSAsamp">4.15</a><a href="proofs.html#proofEquivDIDSAsamp" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us start with Sun and Abrahamâs model in repeated cross sections, with group fixed effects.
Here is how we can write this model with four time periods:</p>
<p><span class="math display">\[\begin{align*} 
Y &amp; = X\Theta + \epsilon 
\end{align*}\]</span>
with
<span class="math display">\[\begin{align*} 
Y &amp; =\left(\begin{array}{c} Y_{1,1} \\ \vdots \\ Y_{N_1,1}\\ Y_{1,2} \\ \vdots \\Y_{N_2,2}\\ Y_{1,3} \\ \vdots \\ Y_{N_3,3}\\ Y_{1,4} \\ \vdots \\ Y_{N_4,4}
\end{array}\right)  \\ 
X &amp; = \left(\begin{array}{cccccccccccccccc} 
1 &amp; D^2_1 &amp; D^3_1 &amp; D^4_1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; D^3_1 &amp; 0 &amp; 0 &amp; D^4_1 &amp; 0 &amp; 0\\ 
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp;\vdots &amp; \vdots&amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ 
1 &amp; D^2_{N_1} &amp; D^3_{N_1} &amp; D^4_{N_1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; D^3_{N_1} &amp; 0 &amp; 0 &amp; D^4_{N_1} &amp; 0 &amp; 0\\ 
1 &amp; D^2_1 &amp; D^3_1 &amp; D^4_1 &amp; 1 &amp; 0 &amp; 0 &amp; D^2_1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; D^4_1 &amp;0\\ 
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp;\vdots &amp; \vdots &amp; \vdots &amp; \vdots&amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ 
1 &amp; D^2_{N_2} &amp; D^3_{N_2} &amp; D^4_{N_2} &amp; 1 &amp; 0 &amp; 0 &amp; D^2_{N_2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;0 &amp; D^4_{N_2} &amp; 0\\ 
1 &amp; D^2_1 &amp; D^3_1 &amp; D^4_1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; D^2_1 &amp; 0 &amp; 0 &amp; D^3_1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ 
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots&amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ 
1 &amp; D^2_{N_3} &amp; D^3_{N_3} &amp; D^4_{N_3} &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; D^2_{N_3} &amp; 0 &amp; 0 &amp; D^3_{N_3} &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ 
1 &amp; D^2_1 &amp; D^3_1 &amp; D^4_1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; D^2_1 &amp; 0 &amp; 0 &amp; D^3_1 &amp; 0 &amp; 0 &amp; D^4_1\\ 
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots&amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ 
1 &amp; D^2_{N_4} &amp; D^3_{N_4} &amp; D^4_{N_4} &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; D^2_{N_4} &amp; 0 &amp; 0 &amp; D^3_{N_4} &amp; 0 &amp; 0 &amp; D^4_{N_4} 
\end{array}\right)\\ 
\Theta &amp; =\left(\begin{array}{c}    
\alpha \\ \mu_2\\ \mu_3\\ \mu_4 \\ 
\delta_2\\ \delta_3\\ \delta_4 \\   
\beta^{SA}_{2,0}\\  \beta^{SA}_{2,1}\\  \beta^{SA}_{2,2}\\  
\beta^{SA}_{3,-2}\\ \beta^{SA}_{3,0}\\  \beta^{SA}_{3,1}\\  
\beta^{SA}_{4,-3}\\ \beta^{SA}_{4,-2}\\ \beta^{SA}_{4,0}
\end{array}\right)\\ 
\epsilon &amp; =\left(\begin{array}{c}  
\epsilon_{1,1} \\ \vdots \\\epsilon_{N_1,1}\\ \epsilon_{1,2} \\ \vdots \\ \epsilon_{N_2,2}\\ \epsilon_{1,3} \\ \vdots \\ \epsilon_{N_3,3}\\ \epsilon_{1,4} \\ \vdots \\ \epsilon_{N_4,4}
\end{array}\right), 
\end{align*}\]</span></p>
<p>with <span class="math inline">\(D^d_i=\uns{D_i=d}\)</span> and <span class="math inline">\(N_t\)</span> the number of observations at time <span class="math inline">\(t\)</span>.
If we are in a panel, each <span class="math inline">\(i\)</span> is the same across time periods.
If we are in a repeated cross section, the <span class="math inline">\(i\)</span> index refers to different individuals.
This model is very difficult to solve by brute force, since its <span class="math inline">\(X&#39;X\)</span> matrix is <span class="math inline">\(16 \times 16\)</span> and has no easy simplification on sight.
Here is the <span class="math inline">\(X&#39;X\)</span> for panel data (which is slightly simpler), with <span class="math inline">\(\bar{D^d}=\frac{1}{N}\sum_{i=1}^N\uns{D_i=d}\)</span>, and <span class="math inline">\(N\)</span> the number of individuals in the panel:</p>
<p><span class="math display">\[\begin{align*} 
X&#39;X &amp; = N\left(\begin{array}{cccccccccccccccc} 
T &amp; T\bar{D^2} &amp; T\bar{D^3} &amp; T\bar{D^4} &amp; 1 &amp; 1 &amp; 1 &amp; \bar{D^2} &amp; \bar{D^2} &amp; \bar{D^2} &amp; \bar{D^3} &amp; \bar{D^3} &amp; \bar{D^3} &amp; \bar{D^4} &amp; \bar{D^4} &amp; \bar{D^4}\\
T\bar{D^2} &amp; T\bar{D^2} &amp; 0           &amp; 0          &amp; \bar{D^2} &amp; \bar{D^2} &amp; \bar{D^2} &amp; \bar{D^2} &amp; \bar{D^2} &amp; \bar{D^2} &amp; 0         &amp; 0         &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 
T\bar{D^3} &amp; 0          &amp; T\bar{D^3}  &amp; 0          &amp; \bar{D^3} &amp; \bar{D^3} &amp; \bar{D^3} &amp; 0         &amp; 0         &amp; 0         &amp; \bar{D^3} &amp; \bar{D^3} &amp; \bar{D^3}   &amp; 0 &amp; 0 &amp; 0\\ 
T\bar{D^4} &amp; 0          &amp; 0           &amp; T\bar{D^4} &amp; \bar{D^4} &amp; \bar{D^4} &amp; \bar{D^4} &amp; 0         &amp; 0         &amp; 0 &amp; 0    &amp; 0    &amp; 0   &amp; \bar{D^4} &amp; \bar{D^4} &amp; \bar{D^4}\\ 
1 &amp; \bar{D^2}  &amp; \bar{D^3}   &amp; \bar{D^4}  &amp; 1         &amp; 0         &amp; 0         &amp; \bar{D^2} &amp; 0 &amp; 0         &amp; 0    &amp; 0    &amp; 0   &amp; 0 &amp; \bar{D^4} &amp; 0 \\ 
1 &amp; \bar{D^2}  &amp; \bar{D^3}   &amp; \bar{D^4}  &amp; 0         &amp; 1         &amp; 0         &amp; 0         &amp; \bar{D^2} &amp; 0         &amp; 0   &amp; \bar{D^3} &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; \bar{D^2}  &amp; \bar{D^3}   &amp; \bar{D^4}  &amp; 0         &amp; 0         &amp; 1         &amp; 0         &amp; 0 &amp; \bar{D^2} &amp; 0   &amp; 0          &amp; \bar{D^3} &amp; 0 &amp; 0 &amp; \bar{D^4}\\ 
\bar{D^2} &amp; \bar{D^2} &amp; 0            &amp; 0          &amp; \bar{D^2} &amp; 0         &amp; 0         &amp; \bar{D^2} &amp; 0         &amp; 0         &amp; 0    &amp; 0    &amp; 0   &amp; 0 &amp; 0 &amp; 0 \\ 
\bar{D^2} &amp; \bar{D^2} &amp; 0            &amp; 0          &amp; 0         &amp; \bar{D^2} &amp; 0         &amp; 0 &amp; \bar{D^2} &amp; 0         &amp; 0    &amp; 0    &amp; 0   &amp; 0 &amp; 0 &amp; 0\\ 
\bar{D^2} &amp; \bar{D^2} &amp; 0            &amp; 0          &amp; 0         &amp; 0         &amp; \bar{D^2} &amp; 0         &amp; 0 &amp; \bar{D^2} &amp; 0    &amp; 0    &amp; 0   &amp; 0 &amp; 0 &amp; 0 \\ 
\bar{D^3} &amp; 0         &amp; \bar{D^3}    &amp; 0          &amp; 0         &amp; 0         &amp; 0         &amp; 0         &amp; 0         &amp;  0 &amp; \bar{D^3} &amp; 0    &amp; 0   &amp; 0 &amp; 0 &amp; 0  \\ 
\bar{D^3} &amp; 0         &amp; \bar{D^3}    &amp; 0          &amp; 0         &amp; \bar{D^3} &amp; 0         &amp; 0         &amp; 0         &amp;  0 &amp; 0         &amp; \bar{D^3} &amp; 0   &amp; 0 &amp; 0 &amp; 0   \\ 
\bar{D^3} &amp; 0         &amp; \bar{D^3}    &amp; 0          &amp; 0         &amp; 0         &amp; \bar{D^3} &amp; 0         &amp; 0         &amp;  0 &amp; 0         &amp; 0         &amp; \bar{D^3} &amp; 0 &amp; 0 &amp; 0 \\ 
\bar{D^4} &amp; 0         &amp; 0            &amp; \bar{D^4}  &amp; 0         &amp; 0         &amp; 0         &amp; 0         &amp; 0         &amp;  0 &amp; 0         &amp; 0         &amp;  0        &amp; \bar{D^4} &amp; 0 &amp; 0 \\ 
\bar{D^4} &amp; 0         &amp; 0            &amp; \bar{D^4}  &amp; \bar{D^4} &amp; 0         &amp; 0         &amp; 0         &amp; 0         &amp; 0        &amp; 0         &amp; 0         &amp;  0        &amp; 0 &amp; \bar{D^4} &amp; 0 \\ 
\bar{D^4} &amp; 0         &amp; 0            &amp; \bar{D^4}  &amp; 0         &amp; 0         &amp; \bar{D^4} &amp; 0         &amp; 0         &amp;  0        &amp; 0         &amp; 0         &amp;  0        &amp; 0 &amp; 0 &amp; \bar{D^4} 
\end{array}\right) 
\end{align*}\]</span></p>
<p>The epiphany comes when you are able to write this model with a separate constant, time and group dummies for each separate treated group for which we want to estimate the DID model for.
We have 9 separate interaction parameters <span class="math inline">\(\beta^{SA}_{d,\tau}\)</span> to estimate. We are thus going to estimate <span class="math inline">\(9 \times 4\)</span> parameters total (<em>i.e.</em> run 9 separate regressions with four parameters, but all at once).
We thus have to write a <span class="math inline">\(36 \times 36\)</span> <span class="math inline">\(X&#39;X\)</span> matrix.
The key is that this matrix is going to be block diagonal, with <span class="math inline">\(4 \times 4\)</span> blocks that are identical to the blocks of the <span class="math inline">\(X&#39;X\)</span> matrix in the case of a simple OLS DID estimator with two time periods.
Also, the parameters that are redundant in this model (<em>i.e.</em> that appear several times at different places) will be estimated in exactly the same way, which shows that the two formulations of the model (<span class="math inline">\(16 \times 16\)</span> and <span class="math inline">\(36 \times 36\)</span>) are equivalent and estimate the exact same set of
16 parameters.
In order to see how this works (and to prove the result), letâs write the model for <span class="math inline">\(\beta^{SA}_{2,0}\)</span>.
To be able to do that, we are going to order all the observations in each time period by the opposite of the treatment group to which they belong.
We also denote <span class="math inline">\(N_t^{d}\)</span> the number of observations of group <span class="math inline">\(D_i=d\)</span> in period <span class="math inline">\(t\)</span>, <span class="math inline">\(D^d_{i,t}=\uns{D_i=d}\)</span> and <span class="math inline">\(T^{d+\tau}_{i,t}=\uns{d+\tau=t}\)</span>.
With these notations, we have:</p>
<p><span class="math display">\[\begin{align*} 
\underbrace{\left(\begin{array}{c} 
Y_{1,1} \\  \vdots \\   Y_{N_1^{\infty},1} \\Y_{N_1^{\infty}+1,1} \\    \vdots \\   Y_{N_1^{\infty}+N_1^{2},1} \\ Y_{1,2} \\    \vdots \\   Y_{N_2^{\infty},2} \\Y_{N_2^{\infty}+1,2} \\    \vdots \\   Y_{N_2^{\infty}+N_2^{2},2} 
\end{array}\right)}_{Y_{2,0}} &amp; = 
\underbrace{\left(\begin{array}{cccc} 
1 &amp; D^2_{1,1} &amp; T^2_{1,1} &amp; D^2_{1,1}T^2_{1,1}\\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\
1 &amp; D^2_{N_1^{\infty},1} &amp; T^2_{N_1^{\infty},1} &amp; D^2_{N_1^{\infty},1}T^2_{N_1^{\infty},1} \\ 
1 &amp; D^2_{N_1^{\infty}+1,1} &amp; T^2_{N_1^{\infty}+1,1} &amp; D^2_{N_1^{\infty}+1,1}T^2_{N_1^{\infty}+1,1} \\ 
\vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ 
1 &amp; D^2_{N_1^{\infty}+N_1^{2},1} &amp; T^2_{N_1^{\infty}+N_1^{2},1} &amp; D^2_{N_1^{\infty}+N_1^{2},1}T^2_{N_1^{\infty}+N_1^{2},1} \\ 
1 &amp; D^2_{1,2} &amp; T^2_{1,2} &amp; D^2_{1,2}T^2_{1,2}\\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ 
1 &amp; D^2_{N_1^{\infty},2} &amp; T^2_{N_1^{\infty},2} &amp; D^2_{N_1^{\infty},2}T^2_{N_1^{\infty},2} \\ 
1 &amp; D^2_{N_1^{\infty}+1,2} &amp; T^2_{N_1^{\infty}+1,2} &amp; D^2_{N_1^{\infty}+1,2}T^2_{N_1^{\infty}+1,2} \\ 
\vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ 1 &amp; D^2_{N_1^{\infty}+N_1^{2},2} &amp; T^2_{N_1^{\infty}+N_1^{2},2} &amp; D^2_{N_1^{\infty}+N_1^{2},2}T^2_{N_1^{\infty}+N_1^{2},2} 
\end{array}\right)}_{X_{2,0}}
\underbrace{\left(\begin{array}{c}  
\tilde\alpha_{2,0} \\ \tilde\mu_{2,0} \\ \tilde\delta_{2,0} \\  \beta^{SA}_{2,0} \end{array}\right)}_{\Theta_{2,0}} + 
\underbrace{\left(\begin{array}{c} 
\epsilon_{1,1} \\   \vdots \\   \epsilon_{N_1^{\infty},1} \\\epsilon_{N_1^{\infty}+1,1} \\  \vdots \\   \epsilon_{N_1^{\infty}+N_1^{2},1} \\ \epsilon_{1,2} \\  \vdots \\   \epsilon_{N_2^{\infty},2} \\\epsilon_{N_2^{\infty}+1,2} \\  \vdots \\   \epsilon_{N_2^{\infty}+N_2^{2},2} 
\end{array}\right)}_{\epsilon_{2,0}}
\end{align*}\]</span></p>
<p>Now, we can write 9 such models, one for each <span class="math inline">\(\beta^{SA}_{d,\tau}\)</span>.
If we stack the <span class="math inline">\(Y_{d,\tau}\)</span> on top of each other, starting with <span class="math inline">\(d=2\)</span> and <span class="math inline">\(\tau=0\)</span>, and we stack in the same way the <span class="math inline">\(\Theta_{d,\tau}\)</span> vectors, and, finally, we regroup all the <span class="math inline">\(X_{d,\tau}\)</span> matrices in a block diagonal matrix, we have a new model
<span class="math inline">\(\tilde{Y} = \tilde{X}\tilde{\Theta} + \tilde{\epsilon}\)</span>.
The stacked model has <span class="math inline">\(4 \times 9=36\)</span> parameters while the original model has 16 parameters.
For the two models to be identical, it has to be that there exists <span class="math inline">\(36-16=20\)</span> direct restrictions on the parameters of the stacked model.
Using the fact that some parts of the data set are duplicated in teh stack model, we can determine the link between the parameters in the stacked model and the ones in the original model.
For example, we know that <span class="math inline">\(Y_{1,1}=\tilde{\alpha}_{2,0}+\epsilon_{1,1}=\tilde{\alpha}_{2,1}+\epsilon_{1,1}=\tilde{\alpha}_{2,2}+\epsilon_{1,1}=\alpha+\epsilon_{1,1}\)</span>.
As a consequence, we have <span class="math inline">\(\tilde{\alpha}_{2,0}=\tilde{\alpha}_{2,1}=\tilde{\alpha}_{2,2}=\alpha\)</span>.
Using similar sets of restrictions, we can also show that: <span class="math inline">\(\tilde\delta_{2,0}=\delta_2\)</span>, <span class="math inline">\(\tilde\delta_{2,1}=\delta_3\)</span> and <span class="math inline">\(\tilde\delta_{2,2}=\delta_4\)</span>; <span class="math inline">\(\tilde{\mu}_{d,\tau}=\mu_d\)</span>, <span class="math inline">\(\forall d,\tau\)</span>; <span class="math inline">\(\tilde{\alpha}_{3,-2}=\tilde{\alpha}_{3,0}=\tilde{\alpha}_{3,1}=\alpha+\delta_2\)</span>;
<span class="math inline">\(\tilde{\alpha}_{4,-3}=\tilde{\alpha}_{4,-2}=\tilde{\alpha}_{4,0}=\alpha+\delta_3\)</span>; <span class="math inline">\(\tilde\delta_{3,-2}=-\delta_2\)</span>; <span class="math inline">\(\tilde\delta_{3,0}=\delta_3-\delta_2\)</span>; <span class="math inline">\(\tilde\delta_{3,1}=\delta_4-\delta_2\)</span>; <span class="math inline">\(\tilde\delta_{4,-3}=-\delta_3\)</span>; <span class="math inline">\(\tilde\delta_{4,-2}=\delta_2-\delta_3\)</span>; <span class="math inline">\(\tilde\delta_{4,0}=\delta_4-\delta_3\)</span>.
We have thus shown that every single parameter in the stacked model can be derived from the parameters in the original model. What is left to check now is that the estimation of the stacked model by OLS abides by the constraints
implied by these equalities.
In order to complete the proof, we make use of the fact that <a href="https://en.wikipedia.org/wiki/Block_matrix">the inverse of a block diagonal matrix is the blog diagonal matrix of the inverses of each block</a>.
Using the proof of Theorem <a href="NE.html#thm:asympnoiseDIDCross">4.7</a> (especially the beginning of the proof, which derives the OLS DID estimator in repeated cross sections of different sizes), we can now show that:</p>
<p><span class="math display">\[\begin{align*} 
\hat{\tilde{\alpha}}^{OLS}_{d,\tau} &amp; = \bar{Y}^{\infty}_{d-1}\\
\hat{\tilde{\mu}}^{OLS}_{d,\tau} &amp; = \bar{Y}^{d}_{d-1}-\bar{Y}^{\infty}_{d-1}\\
\hat{\tilde{\delta}}^{OLS}_{d,\tau} &amp; =
\bar{Y}^{\infty}_{d+\tau}-\bar{Y}^{\infty}_{d-1}\\ \hat{\beta}^{SA}_{d,\tau} &amp; =
\bar{Y}^{d}_{d+\tau}-\bar{Y}^{d}_{d-1}-(\bar{Y}^{\infty}_{d+\tau}-\bar{Y}^{\infty}_{d-1}).
\end{align*}\]</span></p>
<p>These results show that all the constraints on the parameters of the stacked model are verified (I leave this as an exercise).
The last equality proves the result for the OLS DID model in repeated cross sections.
The proof for panel data follows exactly the same lines.</p>
<p>Let us now turn to the First Difference estimator in panel data.
The First Difference transformation of Sun and Abraham model which uses <span class="math inline">\(d-1\)</span> as the benchmark period can be written as follows (for <span class="math inline">\(\tau\neq-1\)</span>):</p>
<p><span class="math display">\[\begin{align*}
  Y_{i,d+\tau} - Y_{i,d-1} &amp; = \alpha_{d,\tau}^{FD} + \beta_{d,\tau}^{FD}\uns{D_i=d} + \epsilon^{FD}_{i,d+\tau},
\end{align*}\]</span></p>
<p>with:</p>
<p><span class="math display">\[\begin{align*}
  \alpha_{d,\tau}^{FD} &amp; = \delta_{d+\tau} - \delta_{d-1}\\
  \beta_{d,\tau}^{FD} &amp; = \beta_{d,\tau}^{SA}\\
  \epsilon^{FD}_{i,d+\tau} &amp; = \epsilon^{SA}_{i,d+\tau}-\epsilon^{FD}_{i,d-1}.
\end{align*}\]</span></p>
<p>Using the same trick as for the model in repeated cross sections, we can rewrite this model as stacked model with a block diagonal matrix of covariates.
Here is the block corresponding to the estimation of <span class="math inline">\(\beta^{SA}_{2,0}\)</span>:</p>
<p><span class="math display">\[\begin{align*} 
\underbrace{\left(\begin{array}{c} 
Y_{1,2}-Y_{1,1}  \\ \vdots \\   Y_{N^{\infty},2}-   Y_{N^{\infty},1} \\Y_{N^{\infty}+1,2}-Y_{N^{\infty}+1,1} \\ \vdots \\   Y_{N^{\infty}+N^{2},2} -Y_{N^{\infty}+N^{2},1}
\end{array}\right)}_{\Delta Y_{2,0}} &amp; = 
\underbrace{\left(\begin{array}{cccc} 
1 &amp; 0 \\ 
\vdots &amp; \vdots\\ 
1 &amp; 0 \\ 
1 &amp; 1 \\ 
\vdots &amp; \vdots\\ 
1 &amp; 1 
\end{array}\right)}_{\Delta X_{2,0}}
\underbrace{\left(\begin{array}{c}  
\alpha^{FD}_{2,0} \\    \beta^{FD}_{2,0} \end{array}\right)}_{\Theta^{FD}_{2,0}} + 
\underbrace{\left(\begin{array}{c} 
\epsilon^{FD}_{1,2} \\  \vdots \\   \epsilon^{FD}_{N^{\infty},2} \\\epsilon^{FD}_{N^{\infty}+1,2} \\    \vdots \\   \epsilon^{FD}_{N^{\infty}+N^{2},2} 
\end{array}\right)}_{\epsilon^{FD}_{2,0}}
\end{align*}\]</span></p>
<p>Stacking all the vectors of outcomes, the vector of coefficients and the vector of residuals on top of each other, and organizing the matrices of covariates in a block diagonal matrix, we obtain the stacked Sun and Abraham model in first differences: <span class="math inline">\(\Delta Y = \Delta X\Theta^{FD} + \epsilon^{FD}\)</span>.
Using the fact that <a href="https://en.wikipedia.org/wiki/Block_matrix">the inverse of a block diagonal matrix is the blog diagonal matrix of the inverses of each block</a> and the proof of Lemma <a href="proofs.html#lem:WWOLS">A.3</a>, we can show that <span class="math inline">\(\hat\beta^{FD}_{d,\tau}\)</span> is the with/without estimator applied to <span class="math inline">\(Y_{i,d+\tau} - Y_{i,d-1}\)</span>.
The result follows.</p>
<p>Let us now study the Within estimator of Sun and Abraham model in panel data.
The within mean of Sun and Abraham model depends on the group the observation belongs to.
For <span class="math inline">\(i\)</span> such that <span class="math inline">\(D_i=d\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
  \underbrace{\frac{1}{T}\sum_{t=1}^TY_{i,t}}_{\bar{Y}_{i,.}} = \underbrace{\frac{1}{T}\sum_{t=1}^T\delta_{t}}_{\bar{\delta}}+ \underbrace{\frac{1}{T}\sum_{\tau\neq-1}^T\beta_{d,\tau}^{SA}}_{\bar{\beta}_d} \uns{D_i=d}+\underbrace{\frac{1}{T}\sum_{t=1}^T\epsilon_{i,t}}_{\bar{\epsilon}_{i,.}}.
\end{align*}\]</span></p>
<p>As a consequence, for <span class="math inline">\(i\)</span> such that <span class="math inline">\(D_i=d\)</span> or <span class="math inline">\(D_i=\infty\)</span>, we can write the within transformation of Sun and Abraham model as follows:</p>
<p><span class="math display">\[\begin{align*}
  Y_{i,d+\tau}-\bar{Y}_{i,.} &amp; = \underbrace{\delta_{d-1}-\bar{\delta}}_{\alpha_d^{FE}}+\underbrace{(\delta_{d+\tau}-\delta_{d-1})}_{\delta_{d+\tau}^{FE}}\uns{T_i=d+\tau}\underbrace{-\bar{\beta}_d}_{\mu_d^{FE}}\uns{D_i=d} +\beta_{d,\tau}^{SA}\uns{D_i=d}\uns{T_i=d+\tau}+\epsilon_{i,t}-\bar{\epsilon}_{i,.}.
\end{align*}\]</span></p>
<p>The within transformation of Sun and Abraham model is thus equivalent to the OLS DID model applied to the within-transformed outcomes <span class="math inline">\(Y_{i,d+\tau}-\bar{Y}_{i,.}\)</span>.
Building a stacked model of the within transformed Sun and Abraham model and using the fact that <a href="https://en.wikipedia.org/wiki/Block_matrix">the inverse of a block diagonal matrix is the blog diagonal matrix of the inverses of each block</a> along with Theorem <a href="NE.html#thm:EstimDID">4.5</a> proves that:</p>
<p><span class="math display">\[\begin{align*}
\hat\beta_{d,\tau}^{SA} &amp; = \frac{\sum_{i=1}^{N}(Y_{i,d+\tau}-\bar{Y}_{i,.}-(Y_{i,d-1}-\bar{Y}_{i,.}))\uns{D_i=d}}{\sum_{i=1}^{N}\uns{D_i=d}} \\
                            &amp; \phantom{=} - \frac{\sum_{i=1}^{N}(Y_{i,d+\tau}-\bar{Y}_{i,.}-(Y_{i,d-1}-\bar{Y}_{i,.}))\uns{D_i=\infty}}{\sum_{i=1}^{N}\uns{D_i=\infty}}\\
                            &amp; = \frac{\sum_{i=1}^{N}(Y_{i,d+\tau}-Y_{i,d-1})\uns{D_i=d}}{\sum_{i=1}^{N}\uns{D_i=d}} \\
                            &amp; \phantom{=} - \frac{\sum_{i=1}^{N}(Y_{i,d+\tau}-Y_{i,d-1})\uns{D_i=\infty}}{\sum_{i=1}^{N}\uns{D_i=\infty}},
\end{align*}\]</span></p>
<p>which proves the result.</p>
<p>Let us finally look at the Least Squares Dummy Variables estimator.
Letâs denote <span class="math inline">\(X_{\mu}\)</span> the matrix of individual dummies in the LSDV estimator.
We are going to apply Theorem <a href="proofs.html#thm:FWL">A.1</a>, <em>i.e.</em> Frish-Waugh-Lovell Theorem, partialling out these individual dummies from the list of regressors.
First, we have <span class="math inline">\((X&#39;_{\mu}X_{\mu})^{-1}=\frac{1}{T}I_N\)</span> where <span class="math inline">\(I_N\)</span> is the identity matrix of dimension <span class="math inline">\(N\)</span> and <span class="math inline">\(T\)</span> is the total number of time periods in the panel.
Second, we have that <span class="math inline">\(M_{\mu}Y=X_{\mu}(X&#39;_{\mu}X_{\mu})^{-1}X&#39;_{\mu}Y=\left(\dots,Y_{i,t}-\bar{Y}_{i,.},\dots\right)\)</span>.
For the time fixed effects, we have <span class="math inline">\(M_{\mu}X_{-\mu,T}\)</span> a matrix with <span class="math inline">\(1-\frac{1}{T}\)</span> where <span class="math inline">\(T_{i,t}=1\)</span> and <span class="math inline">\(-\frac{1}{T}\)</span> otherwise.
For the interactive treatment dummies, we have <span class="math inline">\(M_{\mu}X_{-\mu,DT}\)</span> a matrix with <span class="math inline">\(D^d_i(1-\frac{1}{T})\)</span> where <span class="math inline">\(D^d_i\)</span> appeared in the original <span class="math inline">\(X_{-\mu,DT}\)</span> matrix (the last 9 columns of the <span class="math inline">\(X\)</span> matrix) and <span class="math inline">\(-\frac{D^d_i}{T}\)</span> otherwise.
As a consequence of Theorem <a href="proofs.html#thm:FWL">A.1</a>, we can rewrite the LSDV model as follows:</p>
<p><span class="math display">\[\begin{align*}
  Y_{i,t} - \bar{Y}_{i,.} &amp; = \delta_t-\bar\delta-\sum_d\bar\beta^{SA}_d\uns{D_i=d}+\sum_d\sum_{\tau\neq-1}\beta^{SA}_{d,\tau}\uns{D_i=d}\uns{t=d+\tau}+\epsilon^{LSDV}_{i,t} - \bar{\epsilon}^{LSDV}_{i,.}.
\end{align*}\]</span></p>
<p>This is the same formula as the one we have uncovered in the within transformation we have studied just above.
Using the same approach proves the result.</p>
</div>
<div id="proofasympnoiseSACross" class="section level3 hasAnchor" number="16.3.5">
<h3><span class="header-section-number">A.3.5</span> Proof of Theorem <a href="NE.html#thm:asympnoiseSACross">4.19</a><a href="proofs.html#proofasympnoiseSACross" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using the beginning of the proof of Lemma <a href="proofs.html#lem:asympOLS">A.4</a>, we know that: <span class="math inline">\(\sqrt{N}(\hat{\tilde\Theta}_{OLS}-\tilde\Theta)=N(\tilde{X}&#39;\tilde{X})^{-1}\frac{\sqrt{N}}{N}\tilde{X}&#39;\tilde{\epsilon}\)</span>.
Using Slutskyâs Theorem, we know that we can study both terms separately (see the same proof of Lemma <a href="proofs.html#lem:asympOLS">A.4</a>).
<span class="math inline">\(N(\tilde{X}&#39;\tilde{X})^{-1}\)</span> can be derived rather directly from the fact that Sun and Abraham model can be written as a block diagonal matrix, as shown in the proof of Theorem <a href="NE.html#thm:EquivDIDSAsamp">4.15</a>.
The most difficult part is going to be to derive the distribution of <span class="math inline">\(\frac{\sqrt{N}}{N}\tilde{X}&#39;\tilde{\epsilon}\)</span>.</p>
<p>Let us start with <span class="math inline">\(N(\tilde{X}&#39;\tilde{X})^{-1}\)</span>.
Let us define <span class="math inline">\(N_{t,d}\)</span> the number of observations observed in group <span class="math inline">\(d\)</span> at period <span class="math inline">\(t\)</span>.
We also define <span class="math inline">\(N^B_{d,\tau}=N_{d-1,\infty}+N_{d-1,d}\)</span> the number of observations used to estimate <span class="math inline">\(\hat{\beta}^{SA}_{d,\tau}\)</span> that are observed in the reference (or before) period and <span class="math inline">\(N^A_{d,\tau}=N_{d+\tau,\infty}+N_{d+\tau,d}\)</span> the number of observations used to estimate <span class="math inline">\(\hat{\beta}^{SA}_{d,\tau}\)</span> that are observed in the after period.
We also define <span class="math inline">\(N^{SA}_{d,\tau}=N^A_{d,\tau}+N^B_{d,\tau}\)</span>, the number of observations used to estimate <span class="math inline">\(\hat{\beta}^{SA}_{d,\tau}\)</span>.
We also define <span class="math inline">\(\bar{T}_A^{d,\tau}=\frac{N^A_{d,\tau}}{N^{SA}_{d,\tau}}\)</span>, <span class="math inline">\(k^{d,\tau}=\frac{N^B_{d,\tau}}{N^{A}_{d,\tau}}=\frac{1-\bar{T}_A^{d,\tau}}{\bar{T}_A^{d,\tau}}\)</span>.
We let <span class="math inline">\(\bar{D}_A^{d,\tau}\)</span> and <span class="math inline">\(\bar{D}_B^{d,\tau}\)</span> denote the proportion of treated observations in the after and before periods used to estimate <span class="math inline">\(\hat{\beta}^{SA}_{d,\tau}\)</span>.
We also define <span class="math inline">\(\bar{P}^{d,\tau}=\frac{N^{SA}_{d,\tau}}{N}\)</span> the proportion of observations used to estimate <span class="math inline">\(\hat{\beta}^{SA}_{d,\tau}\)</span>.
We also have: <span class="math inline">\(\text{plim}\bar{P}^{d,\tau}=p^{d,\tau}\)</span>, <span class="math inline">\(\text{plim}\bar{D}_A^{d,\tau}=\text{plim}\bar{D}_B^{d,\tau}=p^{d,\tau}_D\)</span> and <span class="math inline">\(\text{plim}\bar{T}_A^{d,\tau}=p^{d,\tau}_A\)</span>. <span class="math inline">\(p^{d,\tau}=\Pr(D^{d,\tau}_i=1)\)</span>, with <span class="math inline">\(D^{d,\tau}_i=\uns{(D_i=d\lor D_i=\infty)\land(T_i=d-1\lor T_i=d+\tau)}\)</span> a dummy indicating that a unit in the population belongs to the set of units used to identify <span class="math inline">\(\beta^{SA}_{d,\tau}\)</span>. <span class="math inline">\(p^{d,\tau}_D=\Pr(D_i=d|D^{d,\tau}_i=1)\)</span> is the proportion of treated units among the set of units used to identify <span class="math inline">\(\beta^{SA}_{d,\tau}\)</span>.
<span class="math inline">\(p^{d,\tau}_A=\Pr(T_i=d+\tau|D^{d,\tau}_i=1)\)</span> is the proportion of units belonging to the after period among the set of units used to identify <span class="math inline">\(\beta^{SA}_{d,\tau}\)</span>.
Finally, let <span class="math inline">\((X&#39;X)^{-1}_{d,\tau}\)</span> denote the block of the matrix <span class="math inline">\((\tilde{X}&#39;\tilde{X})^{-1}\)</span> which is used to estimate <span class="math inline">\(\hat{\beta}^{SA}_{d,\tau}\)</span>.
With all these definitions, we can now follow the proof of Theorem <a href="NE.html#thm:asympnoiseDIDCross">4.7</a> in order to derive the following result:</p>
<p><span class="math display">\[\begin{align*}
\sigma_{\tilde{X}\tilde{X}^{-1}}^{d,\tau} &amp;= \text{plim}N(X&#39;X)^{-1}_{d,\tau} = \frac{1}{p^{d,\tau}(1-p^{d,\tau}_A)(1-p^{d,\tau}_D)}
\left(\begin{array}{cccc}
1 &amp; -1 &amp; -1 &amp; 1\\
-1 &amp; \frac{1}{p^{d,\tau}_D} &amp; 1 &amp; -\frac{1}{p^{d,\tau}_D} \\
-1 &amp; 1 &amp; \frac{1}{p^{d,\tau}_A} &amp; -\frac{1}{p^{d,\tau}_A} \\
1 &amp; -\frac{1}{p^{d,\tau}_D} &amp; -\frac{1}{p^{d,\tau}_A}  &amp; \frac{1}{p^{d,\tau}_Dp^{d,\tau}_A}
\end{array}\right)
\end{align*}\]</span></p>
<p>Using the fact that <a href="https://en.wikipedia.org/wiki/Block_matrix">the inverse of a block diagonal matrix is the blog diagonal matrix of the inverses of each block</a>, we now know
<span class="math inline">\(\text{plim}N(\tilde{X}&#39;\tilde{X})^{-1}=\sigma_{\tilde{X}\tilde{X}}^{-1}\)</span> is block diagonal matrix with blocks equal to <span class="math inline">\(\sigma_{\tilde{X}\tilde{X}^{-1}}^{d,\tau}\)</span>.</p>
<p>Let us now turn to <span class="math inline">\(\frac{\sqrt{N}}{N}\tilde{X}&#39;\tilde{\epsilon}\)</span>.
In order to derive its distribution, we have to write Sun and Abraham model in a repeated cross section with the observations grouped by blocks corresponding to the parameters they help to estimate.
This model can be written as:</p>
<p><span class="math display">\[\begin{align*}
  Y_{j}D^{d,\tau}_j &amp;  = \tilde\alpha^{d,\tau}D^{d,\tau}_j + \tilde\mu_{d,\tau}\uns{D_{j}=d}D^{d,\tau}_j + \tilde\delta_{d,\tau}\uns{T_j=d+\tau}D^{d,\tau}_j \\
                    &amp; \phantom{=}  +  \beta_{d,\tau}^{SA}\uns{D_{i}=d}\uns{T_j=d+\tau}D^{d,\tau}_j  + \tilde\epsilon_{j}D^{d,\tau}_j,
\end{align*}\]</span></p>
<p>with:</p>
<p><span class="math display">\[\begin{align*}
\tilde\epsilon_{j} &amp; = Y_{j}-\left(\esp{Y^0_{j}|D_j=\infty,T_j=d-1}\right.\\
                          &amp; \phantom{=Y_{j}-\left(\right.}+\uns{D_j=d}(\esp{Y^0_{j}|D_j=d,T_j=d-1}-\esp{Y^0_{j}|D_j=\infty,T_j=d-1})
                          &amp; \phantom{=Y_{j}-\left(\right.}+\uns{T_j=d+\tau}(\esp{Y^0_{j}|D_j=\infty,T_j=d+\tau}-\esp{Y^0_{j}|D_j=\infty,T_j=d-1})\\
                          &amp; \phantom{=Y_{j}-\left(\right.}+\uns{D_{j}=d}\uns{T_j=d+\tau}(\esp{Y^1_{j}|D_j=d,T_j=d+\tau}-\esp{Y^0_{j}|D_j=d,T_j=d-1}\\
                          &amp; \phantom{=Y_{j}-\left(\right.+\uns{D_{j}=d}\uns{T_j=d+\tau}}\left.-(\esp{Y^0_{j}|D_j=\infty,T_j=d+\tau}-\esp{Y^0_{j}|D_j=\infty,T_j=d-1}))\right).
\end{align*}\]</span></p>
<p>It is pretty straightforward to prove that <span class="math inline">\(\esp{\tilde\epsilon_{j}D^{d,\tau}_j}=0\)</span>.
For that, note that <span class="math inline">\(D^{d,\tau}_j=f(D_j,T_j)\)</span> so that conditioning on <span class="math inline">\(D^{d,\tau}_j\)</span> is irrelevant when also conditioning on <span class="math inline">\((D_j,T_j)\)</span>.
Then, check that <span class="math inline">\(\esp{\tilde\epsilon_{j}D^{d,\tau}_j\uns{D_{j}=d}\uns{T_j=d+\tau}}=0\)</span>.
The same thing holds for <span class="math inline">\(\esp{\tilde\epsilon_{j}D^{d,\tau}_j\uns{T_j=d+\tau}}=0\)</span> and for <span class="math inline">\(\esp{\tilde\epsilon_{j}D^{d,\tau}_j\uns{D_{j}=d}}=0\)</span>, which proves the result.</p>
<p>It can also be shown that <span class="math inline">\(\esp{\tilde\epsilon_{j}D^{d,\tau}_jD^{d&#39;,\tau&#39;}_j}=0\)</span>, with either <span class="math inline">\(j\neq j&#39;\)</span> or <span class="math inline">\(\tau\neq \tau&#39;\)</span> or both.
If it is the case that <span class="math inline">\(D^{d,\tau}_j\Ind D^{d&#39;,\tau&#39;}_j\)</span>, then the term is zero.
If <span class="math inline">\(D^{d,\tau}_j\)</span> and <span class="math inline">\(D^{d&#39;,\tau&#39;}_j\)</span> are not independent, by definition of <span class="math inline">\(D^{d,\tau}_j\)</span>, <span class="math inline">\(\esp{\tilde\epsilon_{j}D^{d,\tau}_jD^{d&#39;,\tau&#39;}_j}\)</span> can only involve the following terms: <span class="math inline">\(\esp{\tilde\epsilon_{j}|D_j=d,T_j=d+\tau}\)</span>, <span class="math inline">\(\esp{\tilde\epsilon_{j}|D_j=d,T_j=d-1}\)</span>, <span class="math inline">\(\esp{\tilde\epsilon_{j}|D_j=\infty,T_j=d+\tau}\)</span> and <span class="math inline">\(\esp{\tilde\epsilon_{j}|D_j=\infty,T_j=d-1}\)</span>, and all of these terms are equal to zero.</p>
<p>Using the vector version of the Central Limit Theorem that we have already used in the proof of Theorem <a href="NE.html#thm:asympnoiseDIDCross">4.7</a>, we thus have that <span class="math inline">\(\frac{\sqrt{N}}{N}\tilde{X}&#39;\tilde{\epsilon}\sim\mathcal{N}(\mathbf{0},\mathbf{V_{\tilde{x}\tilde{\epsilon}}})\)</span>.
Using the Delta Method, we have that <span class="math inline">\(\sqrt{N}(\hat{\Theta}_{OLS}-\Theta)\stackrel{d}{\rightarrow}\mathcal{N}\left(\mathbf{0},\sigma_{\tilde{X}\tilde{X}}^{-1}\mathbf{V_{\tilde{x}\tilde{\epsilon}}}\sigma_{\tilde{X}\tilde{X}}^{-1}\right)\)</span>.
In order to prove the result, we simply need to derive the fourth term on the diagonal of each <span class="math inline">\(4\times 4\)</span> block of <span class="math inline">\(\sigma_{\tilde{X}\tilde{X}}^{-1}\mathbf{V_{\tilde{x}\tilde{\epsilon}}}\sigma_{\tilde{X}\tilde{X}}^{-1}\)</span>.
Since <span class="math inline">\(\sigma_{\tilde{X}\tilde{X}}^{-1}\)</span> is block diagonal, the <span class="math inline">\(4\times 4\)</span> blocks of <span class="math inline">\(\sigma_{\tilde{X}\tilde{X}}^{-1}\mathbf{V_{\tilde{x}\tilde{\epsilon}}}\sigma_{\tilde{X}\tilde{X}}^{-1}\)</span> are equal to <span class="math inline">\(\sigma_{\tilde{X}\tilde{X}^{-1}}^{d,\tau}\mathbf{V}^{d,\tau}_{\mathbf{\tilde{x}\tilde{\epsilon}}}\sigma_{\tilde{X}\tilde{X}^{-1}}^{d,\tau}\)</span>, with (following the proof of Theorem <a href="NE.html#thm:asympnoiseDIDCross">4.7</a>):</p>
<p><span class="math display">\[\begin{align*}
  \mathbf{V}^{d,\tau}_{\mathbf{\tilde{x}\tilde{\epsilon}}} &amp; =
                                        \esp{\epsilon_j^2D^{d,\tau}_j\left(\begin{array}{cccc}
                                              1 &amp; D^d_j &amp;   T^{d,\tau}_j &amp; D^d_jT^{d,\tau}_j \\
                                              D^d_j &amp; D^d_j &amp;   D^d_jT^{d,\tau}_j &amp; D^d_jT^{d,\tau}_j \\
                                              T^{d,\tau}_j &amp; D^d_jT^{d,\tau}_j &amp;    T^{d,\tau}_j &amp; D^d_jT^{d,\tau}_j \\
                                              D^d_jT^{d,\tau}_j &amp; D^d_jT^{d,\tau}_j &amp;   D^d_jT^{d,\tau}_j &amp; D^d_jT^{d,\tau}_j
                                            \end{array}\right)},
\end{align*}\]</span></p>
<p>with <span class="math inline">\(D^d_j=\uns{D_{j}=d}\)</span> and <span class="math inline">\(T_j^{d,\tau}=\uns{T_j=d+\tau}D^{d,\tau}_j\)</span>.
Following the proof of Theorem <a href="NE.html#thm:asympnoiseDIDCross">4.7</a> proves the result.</p>
</div>
<div id="proofasympnoiseSATTCross" class="section level3 hasAnchor" number="16.3.6">
<h3><span class="header-section-number">A.3.6</span> Proof of Theorem <a href="NE.html#thm:asympnoiseSATTCross">4.21</a><a href="proofs.html#proofasympnoiseSATTCross" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The key to the proof relies on the covariance terms.
The covariance terms come from the off-<span class="math inline">\(4\times 4\)</span>-block-diagonal elements of the <span class="math inline">\(\sigma_{\tilde{X}\tilde{X}}^{-1}\mathbf{V_{\tilde{x}\tilde{\epsilon}}}\sigma_{\tilde{X}\tilde{X}}^{-1}\)</span> matrix.
They are due to the fact that the same data are used repeatedly to estimate the <span class="math inline">\(\beta^{SA}_{d,\tau}\)</span> parameters.
For example, the observations from the never treated group are used as benchmarks for the estimation of <span class="math inline">\(\beta^{SA}_{2,0}\)</span>, <span class="math inline">\(\beta^{SA}_{2,1}\)</span> and <span class="math inline">\(\beta^{SA}_{2,2}\)</span>.
The same observations are used as post-treatment observations for the estimation of <span class="math inline">\(\beta^{SA}_{2,0}\)</span>, <span class="math inline">\(\beta^{SA}_{3,-2}\)</span> and <span class="math inline">\(\beta^{SA}_{4,-3}\)</span>.</p>
<p>In order to rigorously derive these covariance terms, we need to derive the off-<span class="math inline">\(4\times 4\)</span>-block-diagonal elements of the <span class="math inline">\(\sigma_{\tilde{X}\tilde{X}}^{-1}\mathbf{V_{\tilde{x}\tilde{\epsilon}}}\sigma_{\tilde{X}\tilde{X}}^{-1}\)</span> matrix in the proof of Theorem <a href="NE.html#thm:asympnoiseSACross">4.19</a>.
For two sets of observations used to estimate <span class="math inline">\(\beta^{SA}_{d,\tau}\)</span> and <span class="math inline">\(\beta^{SA}_{d&#39;,\tau&#39;}\)</span>, <span class="math inline">\(d\neq d&#39;\)</span> or <span class="math inline">\(\tau\neq\tau&#39;\)</span>, we only need the last line of the off-<span class="math inline">\(4\times 4\)</span>-block-diagonal covariance matrix of <span class="math inline">\(\sigma_{\tilde{X}\tilde{X}}^{-1}\mathbf{V_{\tilde{x}\tilde{\epsilon}}}\sigma_{\tilde{X}\tilde{X}}^{-1}\)</span>.
Indeed, using results in the proof of Theorem <a href="NE.html#thm:asympnoiseSACross">4.19</a>, we can show that:</p>
<p><span class="math display">\[\begin{align*}
  \text{Cov}(\hat{\beta}^{SA}_{d,\tau},\hat{\beta}^{SA}_{d&#39;,\tau&#39;}) &amp; = \frac{1}{p^{d,\tau}p^{d&#39;,\tau&#39;}(1-p_A^{d,\tau})(1-p_A^{d&#39;,\tau&#39;})(1-p_D^{d,\tau})(1-p_D^{d&#39;,\tau&#39;})} \\
                &amp; \phantom{=}  \left(\mathbf{A}_4-\frac{1}{p_D^{d,\tau}}\mathbf{B}_4-\frac{1}{p_A^{d,\tau}}\mathbf{C}_4+\frac{1}{p_A^{d,\tau}p_D^{d,\tau}}\mathbf{D}_4\right),
\end{align*}\]</span></p>
<p>with:</p>
<p><span class="math display">\[\begin{align*}
  \mathbf{A}_4 &amp; = \esp{\epsilon_j^2D_j^{d&#39;,\tau&#39;}D_j^{d,\tau}} -\frac{1}{p_D^{d&#39;,\tau&#39;}}\esp{\epsilon_j^2D^{d&#39;}_jD_j^{d&#39;,\tau&#39;}D_j^{d,\tau}} -\frac{1}{p^{d&#39;,\tau&#39;}_A}\esp{\epsilon_j^2T^{d&#39;,\tau&#39;}_jD_j^{d&#39;,\tau&#39;}D_j^{d,\tau}} \\
                &amp; \phantom{=} + \frac{1}{p_D^{d&#39;,\tau&#39;}p_A^{d&#39;,\tau&#39;}}\esp{\epsilon_j^2D^{d&#39;}_jT^{d&#39;,\tau&#39;}_jD_j^{d&#39;,\tau&#39;}D_j^{d,\tau}}\\
  \mathbf{B}_4 &amp; = \esp{\epsilon_j^2D_j^{d&#39;,\tau&#39;}D^{d}_jD_j^{d,\tau}} -\frac{1}{p_D^{d&#39;,\tau&#39;}}\esp{\epsilon_j^2D^{d&#39;}_jD_j^{d&#39;,\tau&#39;}D^{d}_jD_j^{d,\tau}} -\frac{1}{p^{d&#39;,\tau&#39;}_A}\esp{\epsilon_j^2T^{d&#39;,\tau&#39;}_jD_j^{d&#39;,\tau&#39;}D^{d}_jD_j^{d,\tau}} \\
                &amp; \phantom{=} + \frac{1}{p_D^{d&#39;,\tau&#39;}p_A^{d&#39;,\tau&#39;}}\esp{\epsilon_j^2D^{d&#39;}_jT^{d&#39;,\tau&#39;}_jD_j^{d&#39;,\tau&#39;}D^{d}_jD_j^{d,\tau}}\\
  \mathbf{C}_4 &amp; = \esp{\epsilon_j^2D_j^{d&#39;,\tau&#39;}T^{d,\tau}_jD_j^{d,\tau}} -\frac{1}{p_D^{d&#39;,\tau&#39;}}\esp{\epsilon_j^2D^{d&#39;}_jD_j^{d&#39;,\tau&#39;}T^{d,\tau}_jD_j^{d,\tau}} -\frac{1}{p^{d&#39;,\tau&#39;}_A}\esp{\epsilon_j^2T^{d&#39;,\tau&#39;}_jD_j^{d&#39;,\tau&#39;}T^{d,\tau}_jD_j^{d,\tau}} \\
                &amp; \phantom{=} + \frac{1}{p_D^{d&#39;,\tau&#39;}p_A^{d&#39;,\tau&#39;}}\esp{\epsilon_j^2D^{d&#39;}_jT^{d&#39;,\tau&#39;}_jD_j^{d&#39;,\tau&#39;}T^{d,\tau}_jD_j^{d,\tau}}\\
\mathbf{D}_4 &amp; = \esp{\epsilon_j^2D_j^{d&#39;,\tau&#39;}D^{d}_jT^{d,\tau}_jD_j^{d,\tau}} -\frac{1}{p_D^{d&#39;,\tau&#39;}}\esp{\epsilon_j^2D^{d&#39;}_jD_j^{d&#39;,\tau&#39;}D^{d}_jT^{d,\tau}_jD_j^{d,\tau}} -\frac{1}{p^{d&#39;,\tau&#39;}_A}\esp{\epsilon_j^2T^{d&#39;,\tau&#39;}_jD_j^{d&#39;,\tau&#39;}D^{d}_jT^{d,\tau}_jD_j^{d,\tau}} \\
                &amp; \phantom{=} + \frac{1}{p_D^{d&#39;,\tau&#39;}p_A^{d&#39;,\tau&#39;}}\esp{\epsilon_j^2D^{d&#39;}_jT^{d&#39;,\tau&#39;}_jD_j^{d&#39;,\tau&#39;}D^{d}_jT^{d,\tau}_jD_j^{d,\tau}}
\end{align*}\]</span></p>
<p>Let us start with <span class="math inline">\(\mathbf{A}_4\)</span>âs first term, <span class="math inline">\(\esp{\epsilon_j^2D_j^{d&#39;,\tau&#39;}D_j^{d,\tau}}\)</span>.
Note first that if <span class="math inline">\(d=d&#39;\)</span>, we have to have <span class="math inline">\(\tau\neq\tau&#39;\)</span>, otherwise the term would be on the diagonal.
As a consequence, with <span class="math inline">\(d=d&#39;\)</span>, we can have only two configurations for which <span class="math inline">\(D_j^{d&#39;,\tau&#39;}D_j^{d,\tau}=1\)</span>, and they both correspond to a baseline observation (<span class="math inline">\(T_j=d-1\)</span>) either for the control group (<span class="math inline">\(D_j=\infty\)</span>) or for the treated group (<span class="math inline">\(D_j=d\)</span>).
In that case, we thus have:</p>
<p><span class="math display">\[\begin{align*}
\esp{\epsilon_j^2D_j^{d&#39;,\tau&#39;}D_j^{d,\tau}} &amp; = \left(\var{Y^0_{i,d-1}|D_i=\infty}(1-p_D^{d,\tau,d&#39;,\tau&#39;})\right.\\
                                            &amp; \phantom{=}\left.+\var{Y^0_{i,d-1}|D_i=d}p_D^{d,\tau,d&#39;,\tau&#39;}\right)p_{d-1}^{d,\tau,d&#39;,\tau&#39;}p^{d,\tau,d&#39;,\tau&#39;},
\end{align*}\]</span></p>
<p>with <span class="math inline">\(p_D^{d,\tau,d&#39;,\tau&#39;}\)</span> the proportion of treated individuals in the group such as <span class="math inline">\(D_j^{d&#39;,\tau&#39;}D_j^{d,\tau}=1\)</span> and <span class="math inline">\(p_{d-1}^{d,\tau,d&#39;,\tau&#39;}\)</span> is the proportion of observations observed in period <span class="math inline">\(d-1\)</span> among the observations such as <span class="math inline">\(D_j^{d&#39;,\tau&#39;}D_j^{d,\tau}=1\)</span> and <span class="math inline">\(p^{d,\tau,d&#39;,\tau&#39;}\)</span> the proportion of observations such as <span class="math inline">\(D_j^{d&#39;,\tau&#39;}D_j^{d,\tau}=1\)</span>.</p>
<p>When <span class="math inline">\(d\neq d&#39;\)</span>, we have three possible cases: <span class="math inline">\(d-1=d&#39;+\tau&#39;\)</span>, <span class="math inline">\(d&#39;-1=d+\tau\)</span> or <span class="math inline">\(d&#39;+\tau&#39;=d+\tau\)</span>.
Because the treated groups are different in that case, the only possible correspondence in these cases is due to the control group, with the After period for one treated group being the Before period for another treated group or the After periods being the same for both groups.
If <span class="math inline">\(d-1=d&#39;+\tau&#39;\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
  \esp{\epsilon_j^2D_j^{d&#39;,\tau&#39;}D_j^{d,\tau}} &amp; = \var{Y^0_{i,d-1}|D_i=\infty}(1-p_D^{d,\tau,d&#39;,\tau&#39;})p_{d-1}^{d,\tau,d&#39;,\tau&#39;}p^{d,\tau,d&#39;,\tau&#39;},
\end{align*}\]</span></p>
<p>where <span class="math inline">\(p_{d-1}^{d,\tau,d&#39;,\tau&#39;}\)</span> is the proportion of observations observed in period <span class="math inline">\(d-1\)</span> among the observations such as <span class="math inline">\(D_j^{d&#39;,\tau&#39;}D_j^{d,\tau}=1\)</span>.</p>
<p>If <span class="math inline">\(d&#39;-1=d+\tau\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
  \esp{\epsilon_j^2D_j^{d&#39;,\tau&#39;}D_j^{d,\tau}} &amp; = \var{Y^0_{i,d&#39;-1}|D_i=\infty}(1-p_D^{d,\tau,d&#39;,\tau&#39;})p_{d&#39;-1}^{d,\tau,d&#39;,\tau&#39;}p^{d,\tau,d&#39;,\tau&#39;}.
\end{align*}\]</span></p>
<p>Finally, if <span class="math inline">\(d&#39;+\tau&#39;=d+\tau\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
  \esp{\epsilon_j^2D_j^{d&#39;,\tau&#39;}D_j^{d,\tau}} &amp; = \var{Y^0_{i,d+\tau}|D_i=\infty}(1-p_D^{d,\tau,d&#39;,\tau&#39;})p_{d+\tau}^{d,\tau,d&#39;,\tau&#39;}p^{d,\tau,d&#39;,\tau&#39;}.
\end{align*}\]</span></p>
<p>Let us now look at the next term: <span class="math inline">\(\esp{\epsilon_j^2D^{d&#39;}_jD_j^{d&#39;,\tau&#39;}D_j^{d,\tau}}\)</span>.
Here, there is only one case that yields a non zero term, when <span class="math inline">\(d=d&#39;\)</span> (all the other configurations involve only the untreated group and thus have <span class="math inline">\(D^{d&#39;}_j=0\)</span>).
In that case, we have:</p>
<p><span class="math display">\[\begin{align*}
  \esp{\epsilon_j^2D^{d&#39;}_jD_j^{d&#39;,\tau&#39;}D_j^{d,\tau}} &amp; = \var{Y^0_{i,d&#39;-1}|D_i=d&#39;}p_D^{d,\tau,d&#39;,\tau&#39;}p_{d&#39;-1}^{d,\tau,d&#39;,\tau&#39;}p^{d,\tau,d&#39;,\tau&#39;}.
\end{align*}\]</span></p>
<p>Let us now look at the next term: <span class="math inline">\(\esp{\epsilon_j^2T^{d&#39;,\tau&#39;}_jD_j^{d&#39;,\tau&#39;}D_j^{d,\tau}}\)</span>.
Here, there are two cases that yield a non zero term, when <span class="math inline">\(d-1=d&#39;+\tau&#39;\)</span> and when <span class="math inline">\(d&#39;+\tau&#39;=d+\tau\)</span> (all the other configurations involve only the Before period and thus have <span class="math inline">\(T^{d&#39;,\tau&#39;}_j=0\)</span>).
In both case, we have:</p>
<p><span class="math display">\[\begin{align*}
  \esp{\epsilon_j^2T^{d&#39;,\tau&#39;}_jD_j^{d&#39;,\tau&#39;}D_j^{d,\tau}} &amp; = \var{Y^0_{i,d&#39;+\tau&#39;}|D_i=\infty}(1-p_D^{d,\tau,d&#39;,\tau&#39;})p_{d&#39;+\tau&#39;}^{d,\tau,d&#39;,\tau&#39;}p^{d,\tau,d&#39;,\tau&#39;}.
\end{align*}\]</span></p>
<p>Let us now look at the next term: <span class="math inline">\(\esp{\epsilon_j^2D^{d&#39;}_jT^{d&#39;,\tau&#39;}_jD_j^{d&#39;,\tau&#39;}D_j^{d,\tau}}\)</span>.
This term is equal to zero since there is no treated observation observed in a post-treatment period such that <span class="math inline">\(D_j^{d&#39;,\tau&#39;}D_j^{d,\tau}=1\)</span>.</p>
<p>Let us now move on to <span class="math inline">\(\mathbf{B}_4\)</span>.
For <span class="math inline">\(\esp{\epsilon_j^2D_j^{d&#39;,\tau&#39;}D^{d}_jD_j^{d,\tau}}\)</span> to be non zero, we have to have that <span class="math inline">\(d=d&#39;\)</span> (otherwise, <span class="math inline">\(D_j^d=0\)</span>).
The only corresponding nonzero case corresponds to a baseline observation for the treated group:</p>
<p><span class="math display">\[\begin{align*}
  \esp{\epsilon_j^2D_j^{d&#39;,\tau&#39;}D^{d}_jD_j^{d,\tau}} &amp; = \var{Y^0_{i,d-1}|D_i=d}p_D^{d,\tau,d&#39;,\tau&#39;}p_{d-1}^{d,\tau,d&#39;,\tau&#39;}p^{d,\tau,d&#39;,\tau&#39;}.
\end{align*}\]</span></p>
<p>The next term (<span class="math inline">\(\esp{\epsilon_j^2D^{d&#39;}_jD_j^{d&#39;,\tau&#39;}D^{d}_jD_j^{d,\tau}}\)</span>) is the same since, with <span class="math inline">\(d=d&#39;\)</span>, <span class="math inline">\(D_j^d=D_j^{d&#39;}\)</span>.
The last two terms of <span class="math inline">\(\mathbf{B}_4\)</span> are null everywhere.
This is because it can only be that <span class="math inline">\(d=d&#39;\)</span> (since <span class="math inline">\(D_j^d=1\)</span>) and it cannot be a baseline observation (since <span class="math inline">\(T^{d,\tau&#39;}_j=1\)</span>).
Since treated observations appear only once for each <span class="math inline">\(d\)</span>, <span class="math inline">\(\tau\neq\tau&#39;\Rightarrow\esp{\epsilon_j^2T^{d&#39;,\tau&#39;}_jD_j^{d&#39;,\tau&#39;}D^{d}_jD_j^{d,\tau}}=0\)</span>.</p>
<p>Let us now move on to <span class="math inline">\(\mathbf{C}_4\)</span>.
For the first term <span class="math inline">\(\esp{\epsilon_j^2D_j^{d&#39;,\tau&#39;}T^{d,\tau}_jD_j^{d,\tau}}\)</span>, we cannot have nonzero terms when <span class="math inline">\(d=d&#39;\)</span>, since this case involves only baseline period variances and this contradicts <span class="math inline">\(T^{d,\tau}_j=1\)</span>, and thus the term is null in that case.
The only possible nonzero cases involve <span class="math inline">\(d&#39;-1=d+\tau\)</span> or <span class="math inline">\(d&#39;+\tau&#39;=d+\tau\)</span>.
In that case, we have:</p>
<p><span class="math display">\[\begin{align*}
  \esp{\epsilon_j^2D_j^{d&#39;,\tau&#39;}T^{d,\tau}_jD_j^{d,\tau}} &amp; = \var{Y^0_{i,d+\tau}|D_i=\infty}(1-p_D^{d,\tau,d&#39;,\tau&#39;})p_{d+\tau}^{d,\tau,d&#39;,\tau&#39;}p^{d,\tau,d&#39;,\tau&#39;}.
\end{align*}\]</span></p>
<p>The next term in <span class="math inline">\(\mathbf{C}_4\)</span> is zero everywhere since it involves the same term as above plus the additional requirement that <span class="math inline">\(D^{d&#39;}_j=1\)</span>.
Since this entails that observations have to belong to a treatment group, and the previous term only includes terms from the control group, this term has to be zero everywhere.</p>
<p>The term <span class="math inline">\(\esp{\epsilon_j^2T^{d&#39;,\tau&#39;}_jD_j^{d&#39;,\tau&#39;}T^{d,\tau}_jD_j^{d,\tau}}\)</span> is non zero only when <span class="math inline">\(d&#39;+\tau&#39;=d+\tau\)</span> (it is the same as the first term in <span class="math inline">\(\mathbf{C}_4\)</span> with the added constraint that <span class="math inline">\(T^{d&#39;,\tau&#39;}_j=1\)</span>).
We thus have:
<span class="math display">\[\begin{align*}
  \esp{\epsilon_j^2T^{d&#39;,\tau&#39;}_jD_j^{d&#39;,\tau&#39;}T^{d,\tau}_jD_j^{d,\tau}} &amp; = \var{Y^0_{i,d+\tau}|D_i=\infty}(1-p_D^{d,\tau,d&#39;,\tau&#39;})p_{d+\tau}^{d,\tau,d&#39;,\tau&#39;}p^{d,\tau,d&#39;,\tau&#39;}.
\end{align*}\]</span></p>
<p>The last term in <span class="math inline">\(\mathbf{C}_4\)</span> everywhere since it is a subset of the second term, which is already zero.</p>
<p>Finally, all the terms in <span class="math inline">\(\mathbf{D}_4\)</span> are equal to zero.
This is because <span class="math inline">\(T^{d,\tau}_jD^{d}_j=1\)</span> implies that we cannot have <span class="math inline">\(d=d&#39;\)</span> (because the only nonzero terms would then be the ones in the baseline period, which contradicts the fact that <span class="math inline">\(T^{d,\tau}_j=1\)</span>).
The remaining potential nonzero configurations only concern the control group, which runs counter <span class="math inline">\(D^{d}_j=1\)</span>.
Hence the result.</p>
<p>Collecting terms, we now have, when <span class="math inline">\(d=d&#39;\)</span>:</p>
<p><span class="math display">\[\begin{align*}
  \mathbf{A}_4 &amp; = \var{Y^0_{i,d-1}|D_i=\infty}(1-p_D^{d,\tau,d&#39;,\tau&#39;})p_{d-1}^{d,\tau,d&#39;,\tau&#39;}p^{d,\tau,d&#39;,\tau&#39;}\\
             &amp; \phantom{=}+\var{Y^0_{i,d-1}|D_i=d}p_D^{d,\tau,d&#39;,\tau&#39;}p_{d-1}^{d,\tau,d&#39;,\tau&#39;}p^{d,\tau,d&#39;,\tau&#39;}(1-\frac{1}{p_D^{d&#39;,\tau&#39;}})\\
  \mathbf{B}_4 &amp; = \var{Y^0_{i,d-1}|D_i=d}p_D^{d,\tau,d&#39;,\tau&#39;}p_{d-1}^{d,\tau,d&#39;,\tau&#39;}p^{d,\tau,d&#39;,\tau&#39;}(1-\frac{1}{p_D^{d&#39;,\tau&#39;}})\\
  \mathbf{C}_4 &amp; = 0 \\
\mathbf{D}_4 &amp; = 0,
\end{align*}\]</span></p>
<p>and thus:</p>
<p><span class="math display">\[\begin{align*}
  \text{Cov}(\hat{\beta}^{SA}_{d,\tau},\hat{\beta}^{SA}_{d&#39;,\tau&#39;}) &amp; = \frac{p^{d,\tau,d&#39;,\tau&#39;}p_{d-1}^{d,\tau,d&#39;,\tau&#39;}}{p^{d,\tau}p^{d&#39;,\tau&#39;}} \left(\frac{\var{Y^0_{i,d-1}|D_i=\infty}(1-p_D^{d,\tau,d&#39;,\tau&#39;})}{(1-p_A^{d,\tau})(1-p_A^{d&#39;,\tau&#39;})(1-p_D^{d,\tau})(1-p_D^{d&#39;,\tau&#39;})} \right.\\
                &amp; \phantom{=}\left.+\frac{\var{Y^0_{i,d-1}|D_i=d}p_D^{d,\tau,d&#39;,\tau&#39;}}{(1-p_A^{d,\tau})(1-p_A^{d&#39;,\tau&#39;})p_D^{d,\tau}p_D^{d&#39;,\tau&#39;}}\right).
\end{align*}\]</span></p>
<p>Alternatively, when <span class="math inline">\(d+\tau=d&#39;+\tau&#39;\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
  \mathbf{A}_4 &amp; = \var{Y^0_{i,d+\tau}|D_i=\infty}(1-p_D^{d,\tau,d&#39;,\tau&#39;})p_{d+\tau}^{d,\tau,d&#39;,\tau&#39;}p^{d,\tau,d&#39;,\tau&#39;}(1-\frac{1}{p^{d&#39;,\tau&#39;}_A}) \\
  \mathbf{B}_4 &amp; = 0\\
  \mathbf{C}_4 &amp; = \var{Y^0_{i,d+\tau}|D_i=\infty}(1-p_D^{d,\tau,d&#39;,\tau&#39;})p_{d+\tau}^{d,\tau,d&#39;,\tau&#39;}p^{d,\tau,d&#39;,\tau&#39;}(1-\frac{1}{p^{d&#39;,\tau&#39;}_A})\\
\mathbf{D}_4 &amp; = 0
\end{align*}\]</span></p>
<p>and thus:</p>
<p><span class="math display">\[\begin{align*}
  \text{Cov}(\hat{\beta}^{SA}_{d,\tau},\hat{\beta}^{SA}_{d&#39;,\tau&#39;}) &amp; = \frac{p^{d,\tau,d&#39;,\tau&#39;}p_{d+\tau}^{d,\tau,d&#39;,\tau&#39;}\var{Y^0_{i,d+\tau}|D_i=\infty}(1-p_D^{d,\tau,d&#39;,\tau&#39;})}{p^{d,\tau}p^{d&#39;,\tau&#39;}p_A^{d,\tau}p_A^{d&#39;,\tau&#39;}(1-p_D^{d,\tau})(1-p_D^{d&#39;,\tau&#39;})}
\end{align*}\]</span></p>
<p>Finally, when <span class="math inline">\(d-1=d&#39;+\tau&#39;\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
  \mathbf{A}_4 &amp; = \var{Y^0_{i,d-1}|D_i=\infty}(1-p_D^{d,\tau,d&#39;,\tau&#39;})p_{d-1}^{d,\tau,d&#39;,\tau&#39;}p^{d,\tau,d&#39;,\tau&#39;}(1-\frac{1}{p^{d&#39;,\tau&#39;}_A})\\
  \mathbf{B}_4 &amp; = 0\\
  \mathbf{C}_4 &amp; = 0\\
\mathbf{D}_4 &amp; = 0
\end{align*}\]</span></p>
<p>and thus:</p>
<p><span class="math display">\[\begin{align*}
  \text{Cov}(\hat{\beta}^{SA}_{d,\tau},\hat{\beta}^{SA}_{d&#39;,\tau&#39;}) &amp; = -\frac{p^{d,\tau,d&#39;,\tau&#39;}p_{d-1}^{d,\tau,d&#39;,\tau&#39;}\var{Y^0_{i,d-1}|D_i=\infty}(1-p_D^{d,\tau,d&#39;,\tau&#39;})}{p^{d,\tau}p^{d&#39;,\tau&#39;}(1-p_A^{d,\tau})p_A^{d&#39;,\tau&#39;}(1-p_D^{d,\tau})(1-p_D^{d&#39;,\tau&#39;})}.
\end{align*}\]</span></p>
<p>And when <span class="math inline">\(d&#39;-1=d+\tau\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
  \mathbf{A}_4 &amp; = \var{Y^0_{i,d&#39;-1}|D_i=\infty}(1-p_D^{d,\tau,d&#39;,\tau&#39;})p_{d&#39;-1}^{d,\tau,d&#39;,\tau&#39;}p^{d,\tau,d&#39;,\tau&#39;} \\
  \mathbf{B}_4 &amp; = 0\\
  \mathbf{C}_4 &amp; = \var{Y^0_{i,d&#39;-1}|D_i=\infty}(1-p_D^{d,\tau,d&#39;,\tau&#39;})p_{d&#39;-1}^{d,\tau,d&#39;,\tau&#39;}p^{d,\tau,d&#39;,\tau&#39;} \\
\mathbf{D}_4 &amp; = 0
\end{align*}\]</span></p>
<p>and thus:</p>
<p><span class="math display">\[\begin{align*}
  \text{Cov}(\hat{\beta}^{SA}_{d,\tau},\hat{\beta}^{SA}_{d&#39;,\tau&#39;}) &amp; = -\frac{p^{d,\tau,d&#39;,\tau&#39;}p_{d&#39;-1}^{d,\tau,d&#39;,\tau&#39;}\var{Y^0_{i,d&#39;-1}|D_i=\infty}(1-p_D^{d,\tau,d&#39;,\tau&#39;})}{p^{d,\tau}p^{d&#39;,\tau&#39;}p_A^{d,\tau}(1-p_A^{d&#39;,\tau&#39;})(1-p_D^{d,\tau})(1-p_D^{d&#39;,\tau&#39;})}
\end{align*}\]</span></p>
<p>This proves the result.</p>
</div>
<div id="proofasympnoiseSAPanel" class="section level3 hasAnchor" number="16.3.7">
<h3><span class="header-section-number">A.3.7</span> Proof of Theorem <a href="NE.html#thm:asympnoiseSAPanel">4.20</a><a href="proofs.html#proofasympnoiseSAPanel" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The proof follows closely that of Theorem <a href="NE.html#thm:asympnoiseSACross">4.19</a>, except that our stacked model is <span class="math inline">\(\Delta Y = \Delta X\Theta^{FD} + \epsilon^{FD}\)</span>, as introduced in the proof of <a href="NE.html#thm:EquivDIDSAsamp">4.15</a>.
Using the beginning of the proof of Lemma <a href="proofs.html#lem:asympOLS">A.4</a>, we know that: <span class="math inline">\(\sqrt{N}(\hat{\Theta}^{FD}-\Theta^{FD})=N(\Delta X&#39;\Delta X)^{-1}\frac{\sqrt{N}}{N}\Delta X&#39;\epsilon^{FD}\)</span>.
Using Slutskyâs Theorem, we know that we can study both terms separately (see the same proof of Lemma <a href="proofs.html#lem:asympOLS">A.4</a>).</p>
<p>Let us start with <span class="math inline">\(N(\Delta X&#39;\Delta X)^{-1}\)</span>.
Letâs denote <span class="math inline">\(N_{d,\infty}\)</span> the number of observations that are such that <span class="math inline">\(D_i=d\)</span> or <span class="math inline">\(D_i=\infty\)</span> and <span class="math inline">\(p^{d,\infty}=\text{plim}\frac{N_{d,\infty}}{N}\)</span>.
Letâs also denote <span class="math inline">\(p^{d,\infty}_D=\Pr(D_i=d|D_i=d\cup D_i=\infty)\)</span>.
Using the same reasoning as in the proof of Theorem <a href="NE.html#thm:asympnoiseSACross">4.19</a>, and using the proof of Theorem <a href="FPSI.html#thm:asympnoiseWW">2.5</a>, we can show that:</p>
<p><span class="math display">\[\begin{align*}
\sigma_{\Delta X \Delta X^{-1}}^{d,\tau} &amp;= \text{plim}N(\Delta X&#39;\Delta X)^{-1}_{d,\tau} = \frac{1}{p^{d,\infty}p^{d,\infty}_D(1-p^{d,\infty}_D)}
\left(\begin{array}{cc}
p^{d,\infty}_D &amp; -p^{d,\infty}_D\\
-p^{d,\infty}_D &amp;  1
\end{array}\right)
\end{align*}\]</span></p>
<p>with <span class="math inline">\(N(\Delta X&#39;\Delta X)^{-1}_{d,\tau}\)</span> the block of the <span class="math inline">\(N(\Delta X&#39;\Delta X)^{-1}\)</span> matrix that is related to the estimation of <span class="math inline">\(\hat{\beta}^{SA}_{d,\tau}\)</span> and using the fact that <a href="https://en.wikipedia.org/wiki/Block_matrix">the inverse of a block diagonal matrix is the blog diagonal matrix of the inverses of each block</a>.
The proof then follows the line of the proof of Theorem <a href="FPSI.html#thm:asympnoiseWW">2.5</a>, replacing <span class="math inline">\(Y_{i}\)</span> by <span class="math inline">\(Y_{i,d+\tau}-Y_{i,d-1}\)</span>.
This proves the result.</p>
</div>
<div id="proofasympnoiseSATTPanel" class="section level3 hasAnchor" number="16.3.8">
<h3><span class="header-section-number">A.3.8</span> Proof of Theorem <a href="NE.html#thm:asympnoiseSATTPanel">4.22</a><a href="proofs.html#proofasympnoiseSATTPanel" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The key to the proof lies in the covariance terms.
They in turn depend on the product of the following matrix: <span class="math inline">\(\sigma_{\Delta{X}\Delta{X}^{-1}}^{d,\tau,d&#39;,\tau&#39;}\mathbf{V}^{d,\tau,d&#39;,\tau&#39;}_{\mathbf{\Delta{x}\Delta{\epsilon}}}\sigma_{\Delta{X}\Delta{X}^{-1}}^{d,\tau,d&#39;,\tau&#39;}\)</span>, where:</p>
<p><span class="math display">\[\begin{align*}
  \mathbf{V}^{d,\tau,d&#39;,\tau&#39;}_{\mathbf{\Delta{x}\Delta{\epsilon}}} &amp; =
                                        \esp{\Delta\epsilon_j^2\left(\begin{array}{cccc}
                                              D^{d,\tau}_j &amp; D^d_jD^{d,\tau}_j &amp;    D^{d,\tau}_jD^{d&#39;,\tau&#39;}_j &amp; D^{d&#39;}_jD^{d,\tau}_jD^{d&#39;,\tau&#39;}_j \\
                                              D^d_jD^{d,\tau}_j &amp; D^d_jD^{d,\tau}_j &amp;   D^d_jD^{d,\tau}_jD^{d&#39;,\tau&#39;}_j &amp; D^{d&#39;}_jD^d_jD^{d,\tau}_jD^{d&#39;,\tau&#39;}_j \\
                                              D^{d,\tau}_jD^{d&#39;,\tau&#39;}_j  &amp; D^d_jD^{d,\tau}_jD^{d&#39;,\tau&#39;}_j  &amp;  D^{d&#39;,\tau&#39;}_j &amp; D^{d&#39;}_jD^{d&#39;,\tau&#39;}_j \\
                                              D^{d&#39;}_jD^{d,\tau}_jD^{d&#39;,\tau&#39;}_j &amp; D^{d&#39;}_jD^d_jD^{d,\tau}_jD^{d&#39;,\tau&#39;}_j &amp;    D^{d&#39;}_jD^{d&#39;,\tau&#39;}_j &amp; D^{d&#39;}_jD^{d&#39;,\tau&#39;}_j
                                            \end{array}\right)},
\end{align*}\]</span></p>
<p>and <span class="math inline">\(\mathbf{V}^{d,\tau,d&#39;,\tau&#39;}_{\mathbf{\Delta{x}\Delta{\epsilon}}}\)</span> is the part of the <span class="math inline">\(\mathbf{V}_{\mathbf{\Delta{x}\Delta{\epsilon}}}\)</span> matrix that relates to the estimators of <span class="math inline">\(\beta^{SA}_{d,\tau}\)</span> and <span class="math inline">\(\beta^{SA}_{d&#39;,\tau&#39;}\)</span>.
We also have that <span class="math inline">\(\sigma_{\Delta{X}\Delta{X}^{-1}}^{d,\tau,d&#39;,\tau&#39;}\)</span> is the blocks of the matrix <span class="math inline">\(\sigma_{\Delta{X}\Delta{X}^{-1}}\)</span> corresponding to the same estimation, with blocks formed by <span class="math inline">\(\sigma_{\Delta X \Delta X^{-1}}^{d,\tau}\)</span> and <span class="math inline">\(\sigma_{\Delta X \Delta X^{-1}}^{d&#39;,\tau&#39;}\)</span>.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mediation-analysis.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/chabefer/STCI/blob/master/20_Appendix.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["STCI.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"toc_depth": 1
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
