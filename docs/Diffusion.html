<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Diffusion effects | Statistical Tools for Causal Inference</title>
  <meta name="description" content="This is an open source collaborative book." />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Diffusion effects | Statistical Tools for Causal Inference" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is an open source collaborative book." />
  <meta name="github-repo" content="chabefer/STCI" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Diffusion effects | Statistical Tools for Causal Inference" />
  
  <meta name="twitter:description" content="This is an open source collaborative book." />
  

<meta name="author" content="Sylvain Chabé-Ferret" />


<meta name="date" content="2025-03-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="LaLonde.html"/>
<link rel="next" href="Distribution.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
$$
\newcommand{\uns}[1]{\mathbf{1}[#1]}
\newcommand{\unsi}[2]{\mathbf{1}_{#1}(#2)}
\newcommand{\N}{\mathbf{N}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\esp}[1]{\mathbf{E}[#1]}
\newcommand{\espsub}[2]{\mathbf{E}_{#2}[#1]}
\newcommand{\hatesp}[1]{\hat{\mathbf{E}}[ #1 ]}
\newcommand{\espE}{\mathbf{E}}
\newcommand{\Ind}{\perp\kern-5pt\perp}
\newcommand{\var}[1]{\mathbf{V}[ #1 ]}
\newcommand{\hatvar}[1]{\hat{\mathbf{V}}[ #1 ]}
\newcommand{\cov}[1]{\mathbf{C}[ #1 ]}
\newcommand{\plim}[1]{\text{plim}_{ #1 \rightarrow \infty}}
\newcommand{\plims}{\text{plim}}
\newcommand{\distr}{\stackrel{d}{\rightarrow}}
\newcommand{\probconv}{\stackrel{p}{\rightarrow}}
\newcommand{\partder}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\partdersq}[2]{\frac{\partial^2 #1}{\partial #2^2}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\asym}{Asym}
$$


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Tools for Causal Inference</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>I Fundamental Problems of Inference</b></span></li>
<li class="chapter" data-level="" data-path="introduction-the-two-fundamental-problems-of-inference.html"><a href="introduction-the-two-fundamental-problems-of-inference.html"><i class="fa fa-check"></i>Introduction: the Two Fundamental Problems of Inference</a></li>
<li class="chapter" data-level="1" data-path="FPCI.html"><a href="FPCI.html"><i class="fa fa-check"></i><b>1</b> Fundamental Problem of Causal Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="FPCI.html"><a href="FPCI.html#rubin-causal-model"><i class="fa fa-check"></i><b>1.1</b> Rubin Causal Model</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="FPCI.html"><a href="FPCI.html#treatment-allocation-rule"><i class="fa fa-check"></i><b>1.1.1</b> Treatment allocation rule</a></li>
<li class="chapter" data-level="1.1.2" data-path="FPCI.html"><a href="FPCI.html#potential-outcomes"><i class="fa fa-check"></i><b>1.1.2</b> Potential outcomes</a></li>
<li class="chapter" data-level="1.1.3" data-path="FPCI.html"><a href="FPCI.html#switching-equation"><i class="fa fa-check"></i><b>1.1.3</b> Switching equation</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="FPCI.html"><a href="FPCI.html#treatment-effects"><i class="fa fa-check"></i><b>1.2</b> Treatment effects</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="FPCI.html"><a href="FPCI.html#individual-level-treatment-effects"><i class="fa fa-check"></i><b>1.2.1</b> Individual level treatment effects</a></li>
<li class="chapter" data-level="1.2.2" data-path="FPCI.html"><a href="FPCI.html#average-treatment-effect-on-the-treated"><i class="fa fa-check"></i><b>1.2.2</b> Average treatment effect on the treated</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="FPCI.html"><a href="FPCI.html#fundamental-problem-of-causal-inference"><i class="fa fa-check"></i><b>1.3</b> Fundamental problem of causal inference</a></li>
<li class="chapter" data-level="1.4" data-path="FPCI.html"><a href="FPCI.html#intuitive-estimators-confounding-factors-and-selection-bias"><i class="fa fa-check"></i><b>1.4</b> Intuitive estimators, confounding factors and selection bias</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="FPCI.html"><a href="FPCI.html#withwithout-comparison-selection-bias-and-cross-sectional-confounders"><i class="fa fa-check"></i><b>1.4.1</b> With/Without comparison, selection bias and cross-sectional confounders</a></li>
<li class="chapter" data-level="1.4.2" data-path="FPCI.html"><a href="FPCI.html#the-beforeafter-comparison-temporal-confounders-and-time-trend-bias"><i class="fa fa-check"></i><b>1.4.2</b> The before/after comparison, temporal confounders and time trend bias</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="FPSI.html"><a href="FPSI.html"><i class="fa fa-check"></i><b>2</b> Fundamental Problem of Statistical Inference</a>
<ul>
<li class="chapter" data-level="2.1" data-path="FPSI.html"><a href="FPSI.html#sec:sampnoise"><i class="fa fa-check"></i><b>2.1</b> What is sampling noise? Definition and illustration</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="FPSI.html"><a href="FPSI.html#sec:definitionnoise"><i class="fa fa-check"></i><b>2.1.1</b> Sampling noise, a definition</a></li>
<li class="chapter" data-level="2.1.2" data-path="FPSI.html"><a href="FPSI.html#sec:illusnoisepop"><i class="fa fa-check"></i><b>2.1.2</b> Sampling noise for the population treatment effect</a></li>
<li class="chapter" data-level="2.1.3" data-path="FPSI.html"><a href="FPSI.html#sec:illusnoisesamp"><i class="fa fa-check"></i><b>2.1.3</b> Sampling noise for the sample treatment effect</a></li>
<li class="chapter" data-level="2.1.4" data-path="FPSI.html"><a href="FPSI.html#sec:confinterv"><i class="fa fa-check"></i><b>2.1.4</b> Building confidence intervals from estimates of sampling noise</a></li>
<li class="chapter" data-level="2.1.5" data-path="FPSI.html"><a href="FPSI.html#reporting-sampling-noise-a-proposal"><i class="fa fa-check"></i><b>2.1.5</b> Reporting sampling noise: a proposal</a></li>
<li class="chapter" data-level="2.1.6" data-path="FPSI.html"><a href="FPSI.html#sec:effectsize"><i class="fa fa-check"></i><b>2.1.6</b> Using effect sizes to normalize the reporting of treatment effects and their precision</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="FPSI.html"><a href="FPSI.html#sec:estimsampnoise"><i class="fa fa-check"></i><b>2.2</b> Estimating sampling noise</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="FPSI.html"><a href="FPSI.html#sec:assumptions"><i class="fa fa-check"></i><b>2.2.1</b> Assumptions</a></li>
<li class="chapter" data-level="2.2.2" data-path="FPSI.html"><a href="FPSI.html#sec:cheb"><i class="fa fa-check"></i><b>2.2.2</b> Using Chebyshev’s inequality</a></li>
<li class="chapter" data-level="2.2.3" data-path="FPSI.html"><a href="FPSI.html#sec:CLT"><i class="fa fa-check"></i><b>2.2.3</b> Using the Central Limit Theorem</a></li>
<li class="chapter" data-level="2.2.4" data-path="FPSI.html"><a href="FPSI.html#sec:resamp"><i class="fa fa-check"></i><b>2.2.4</b> Using resampling methods</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Methods of Causal Inference</b></span></li>
<li class="chapter" data-level="3" data-path="RCT.html"><a href="RCT.html"><i class="fa fa-check"></i><b>3</b> Randomized Controlled Trials</a>
<ul>
<li class="chapter" data-level="3.1" data-path="RCT.html"><a href="RCT.html#sec:design1"><i class="fa fa-check"></i><b>3.1</b> Brute Force Design</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="RCT.html"><a href="RCT.html#identification"><i class="fa fa-check"></i><b>3.1.1</b> Identification</a></li>
<li class="chapter" data-level="3.1.2" data-path="RCT.html"><a href="RCT.html#estimating-ate"><i class="fa fa-check"></i><b>3.1.2</b> Estimating ATE</a></li>
<li class="chapter" data-level="3.1.3" data-path="RCT.html"><a href="RCT.html#estimating-sampling-noise"><i class="fa fa-check"></i><b>3.1.3</b> Estimating Sampling Noise</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="RCT.html"><a href="RCT.html#sec:design2"><i class="fa fa-check"></i><b>3.2</b> Self-Selection design</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="RCT.html"><a href="RCT.html#identification-1"><i class="fa fa-check"></i><b>3.2.1</b> Identification</a></li>
<li class="chapter" data-level="3.2.2" data-path="RCT.html"><a href="RCT.html#estimating-tt"><i class="fa fa-check"></i><b>3.2.2</b> Estimating TT</a></li>
<li class="chapter" data-level="3.2.3" data-path="RCT.html"><a href="RCT.html#estimating-sampling-noise-1"><i class="fa fa-check"></i><b>3.2.3</b> Estimating Sampling Noise</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="RCT.html"><a href="RCT.html#sec:design3"><i class="fa fa-check"></i><b>3.3</b> Eligibility design</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="RCT.html"><a href="RCT.html#identification-2"><i class="fa fa-check"></i><b>3.3.1</b> Identification</a></li>
<li class="chapter" data-level="3.3.2" data-path="RCT.html"><a href="RCT.html#estimating-the-ite-and-the-tt"><i class="fa fa-check"></i><b>3.3.2</b> Estimating the ITE and the TT</a></li>
<li class="chapter" data-level="3.3.3" data-path="RCT.html"><a href="RCT.html#estimating-sampling-noise-2"><i class="fa fa-check"></i><b>3.3.3</b> Estimating sampling noise</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="RCT.html"><a href="RCT.html#sec:design4"><i class="fa fa-check"></i><b>3.4</b> Encouragement Design</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="RCT.html"><a href="RCT.html#identification-3"><i class="fa fa-check"></i><b>3.4.1</b> Identification</a></li>
<li class="chapter" data-level="3.4.2" data-path="RCT.html"><a href="RCT.html#IVRCT"><i class="fa fa-check"></i><b>3.4.2</b> Estimating the Local Average Treatment Effect and the Intention to Treat Effect</a></li>
<li class="chapter" data-level="3.4.3" data-path="RCT.html"><a href="RCT.html#estimating-sampling-noise-3"><i class="fa fa-check"></i><b>3.4.3</b> Estimating sampling noise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="NE.html"><a href="NE.html"><i class="fa fa-check"></i><b>4</b> Natural Experiments</a>
<ul>
<li class="chapter" data-level="4.1" data-path="NE.html"><a href="NE.html#instrumental-variables"><i class="fa fa-check"></i><b>4.1</b> Instrumental Variables</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="NE.html"><a href="NE.html#an-example-where-monotonicity-does-not-hold"><i class="fa fa-check"></i><b>4.1.1</b> An example where Monotonicity does not hold</a></li>
<li class="chapter" data-level="4.1.2" data-path="NE.html"><a href="NE.html#identification-4"><i class="fa fa-check"></i><b>4.1.2</b> Identification</a></li>
<li class="chapter" data-level="4.1.3" data-path="NE.html"><a href="NE.html#estimation"><i class="fa fa-check"></i><b>4.1.3</b> Estimation</a></li>
<li class="chapter" data-level="4.1.4" data-path="NE.html"><a href="NE.html#estimation-of-sampling-noise"><i class="fa fa-check"></i><b>4.1.4</b> Estimation of sampling noise</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="NE.html"><a href="NE.html#regression-discontinuity-designs"><i class="fa fa-check"></i><b>4.2</b> Regression Discontinuity Designs</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="NE.html"><a href="NE.html#sharp-regression-discontinuity-designs"><i class="fa fa-check"></i><b>4.2.1</b> Sharp Regression Discontinuity Designs</a></li>
<li class="chapter" data-level="4.2.2" data-path="NE.html"><a href="NE.html#fuzzy-regression-discontinuity-designs"><i class="fa fa-check"></i><b>4.2.2</b> Fuzzy Regression Discontinuity Designs</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="NE.html"><a href="NE.html#DID"><i class="fa fa-check"></i><b>4.3</b> Difference In Differences</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="NE.html"><a href="NE.html#sec:DIDbasic"><i class="fa fa-check"></i><b>4.3.1</b> Difference In Differences with two time periods</a></li>
<li class="chapter" data-level="4.3.2" data-path="NE.html"><a href="NE.html#sec:DIDr"><i class="fa fa-check"></i><b>4.3.2</b> Reverse Difference In Differences designs with two time periods</a></li>
<li class="chapter" data-level="4.3.3" data-path="NE.html"><a href="NE.html#DIDStaggered"><i class="fa fa-check"></i><b>4.3.3</b> Difference In Differences with multiple time periods</a></li>
<li class="chapter" data-level="4.3.4" data-path="NE.html"><a href="NE.html#difference-in-differences-with-instrumental-variables"><i class="fa fa-check"></i><b>4.3.4</b> Difference In Differences with Instrumental Variables</a></li>
<li class="chapter" data-level="4.3.5" data-path="NE.html"><a href="NE.html#difference-in-differences-with-continuous-treatment-variables-and-staggered-adoption"><i class="fa fa-check"></i><b>4.3.5</b> Difference In Differences with Continuous Treatment Variables and Staggered Adoption</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="OM.html"><a href="OM.html"><i class="fa fa-check"></i><b>5</b> Observational Methods</a>
<ul>
<li class="chapter" data-level="5.1" data-path="OM.html"><a href="OM.html#parametric-methods"><i class="fa fa-check"></i><b>5.1</b> Parametric methods</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="OM.html"><a href="OM.html#assuming-espy_i0x_i-is-known"><i class="fa fa-check"></i><b>5.1.1</b> Assuming <span class="math inline">\(\esp{Y_i^0|X_i}\)</span> is known</a></li>
<li class="chapter" data-level="5.1.2" data-path="OM.html"><a href="OM.html#assuming-espy_i1x_i-is-known"><i class="fa fa-check"></i><b>5.1.2</b> Assuming <span class="math inline">\(\esp{Y_i^1|X_i}\)</span> is known</a></li>
<li class="chapter" data-level="5.1.3" data-path="OM.html"><a href="OM.html#BiasOLS"><i class="fa fa-check"></i><b>5.1.3</b> Properties of the OLS estimator under Conditional Independence</a></li>
<li class="chapter" data-level="5.1.4" data-path="OM.html"><a href="OM.html#problems-with-parametric-methods"><i class="fa fa-check"></i><b>5.1.4</b> Problems with parametric methods</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="OM.html"><a href="OM.html#nonparametric-methods"><i class="fa fa-check"></i><b>5.2</b> Nonparametric methods</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="OM.html"><a href="OM.html#identification-13"><i class="fa fa-check"></i><b>5.2.1</b> Identification</a></li>
<li class="chapter" data-level="5.2.2" data-path="OM.html"><a href="OM.html#estimation-9"><i class="fa fa-check"></i><b>5.2.2</b> Estimation</a></li>
<li class="chapter" data-level="5.2.3" data-path="OM.html"><a href="OM.html#estimating-precision-2"><i class="fa fa-check"></i><b>5.2.3</b> Estimating precision</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="OM.html"><a href="OM.html#imputation-methods"><i class="fa fa-check"></i><b>5.3</b> Imputation methods</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="threats.html"><a href="threats.html"><i class="fa fa-check"></i><b>6</b> Threats to the validity of Causal Inference</a>
<ul>
<li class="chapter" data-level="6.1" data-path="threats.html"><a href="threats.html#threats-to-internal-validity"><i class="fa fa-check"></i><b>6.1</b> Threats to internal validity</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="threats.html"><a href="threats.html#survey-bias"><i class="fa fa-check"></i><b>6.1.1</b> Survey bias</a></li>
<li class="chapter" data-level="6.1.2" data-path="threats.html"><a href="threats.html#experimenter-bias"><i class="fa fa-check"></i><b>6.1.2</b> Experimenter bias</a></li>
<li class="chapter" data-level="6.1.3" data-path="threats.html"><a href="threats.html#substitution-bias"><i class="fa fa-check"></i><b>6.1.3</b> Substitution bias</a></li>
<li class="chapter" data-level="6.1.4" data-path="threats.html"><a href="threats.html#diffusion-bias"><i class="fa fa-check"></i><b>6.1.4</b> Diffusion bias</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="threats.html"><a href="threats.html#threats-to-the-measurement-of-precision"><i class="fa fa-check"></i><b>6.2</b> Threats to the measurement of precision</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="threats.html"><a href="threats.html#insufficient-precision"><i class="fa fa-check"></i><b>6.2.1</b> Insufficient precision</a></li>
<li class="chapter" data-level="6.2.2" data-path="threats.html"><a href="threats.html#clustering"><i class="fa fa-check"></i><b>6.2.2</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="threats.html"><a href="threats.html#threats-to-external-validity"><i class="fa fa-check"></i><b>6.3</b> Threats to external validity</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="threats.html"><a href="threats.html#randomization-bias"><i class="fa fa-check"></i><b>6.3.1</b> Randomization bias</a></li>
<li class="chapter" data-level="6.3.2" data-path="threats.html"><a href="threats.html#equilibrium-effects"><i class="fa fa-check"></i><b>6.3.2</b> Equilibrium effects</a></li>
<li class="chapter" data-level="6.3.3" data-path="threats.html"><a href="threats.html#context-effects"><i class="fa fa-check"></i><b>6.3.3</b> Context effects</a></li>
<li class="chapter" data-level="6.3.4" data-path="threats.html"><a href="threats.html#site-selection-bias"><i class="fa fa-check"></i><b>6.3.4</b> Site selection bias</a></li>
<li class="chapter" data-level="6.3.5" data-path="threats.html"><a href="threats.html#publication-bias"><i class="fa fa-check"></i><b>6.3.5</b> Publication bias</a></li>
<li class="chapter" data-level="6.3.6" data-path="threats.html"><a href="threats.html#ethical-and-political-issues"><i class="fa fa-check"></i><b>6.3.6</b> Ethical and political issues</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Additional Topics</b></span></li>
<li class="chapter" data-level="7" data-path="Power.html"><a href="Power.html"><i class="fa fa-check"></i><b>7</b> Power Analysis</a>
<ul>
<li class="chapter" data-level="7.1" data-path="Power.html"><a href="Power.html#basics-of-traditional-power-analysis-using-test-statistics"><i class="fa fa-check"></i><b>7.1</b> Basics of traditional power analysis using test statistics</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="Power.html"><a href="Power.html#power"><i class="fa fa-check"></i><b>7.1.1</b> Power</a></li>
<li class="chapter" data-level="7.1.2" data-path="Power.html"><a href="Power.html#minimum-detectable-effect"><i class="fa fa-check"></i><b>7.1.2</b> Minimum Detectable Effect</a></li>
<li class="chapter" data-level="7.1.3" data-path="Power.html"><a href="Power.html#minimum-required-sample-size"><i class="fa fa-check"></i><b>7.1.3</b> Minimum Required Sample Size</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="Power.html"><a href="Power.html#traditional-power-analysis-in-practice"><i class="fa fa-check"></i><b>7.2</b> Traditional power analysis in practice</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="Power.html"><a href="Power.html#power-analysis-for-randomized-controlled-trials"><i class="fa fa-check"></i><b>7.2.1</b> Power Analysis for Randomized Controlled Trials</a></li>
<li class="chapter" data-level="7.2.2" data-path="Power.html"><a href="Power.html#power-analysis-for-natural-experiments"><i class="fa fa-check"></i><b>7.2.2</b> Power Analysis for Natural Experiments</a></li>
<li class="chapter" data-level="7.2.3" data-path="Power.html"><a href="Power.html#power-analysis-for-observational-methods"><i class="fa fa-check"></i><b>7.2.3</b> Power Analysis for Observational Methods</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="Power.html"><a href="Power.html#limitations-of-and-alternatives-to-traditional-power-analysis"><i class="fa fa-check"></i><b>7.3</b> Limitations of and alternatives to traditional power analysis</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="Power.html"><a href="Power.html#limitations-of-traditional-power-analysis"><i class="fa fa-check"></i><b>7.3.1</b> Limitations of traditional power analysis</a></li>
<li class="chapter" data-level="7.3.2" data-path="Power.html"><a href="Power.html#an-alternative-to-traditional-power-analysis"><i class="fa fa-check"></i><b>7.3.2</b> An alternative to traditional power analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="placebo.html"><a href="placebo.html"><i class="fa fa-check"></i><b>8</b> Placebo Tests</a>
<ul>
<li class="chapter" data-level="8.1" data-path="placebo.html"><a href="placebo.html#placebo-tests-for-randomized-controlled-trials"><i class="fa fa-check"></i><b>8.1</b> Placebo tests for randomized controlled trials</a></li>
<li class="chapter" data-level="8.2" data-path="placebo.html"><a href="placebo.html#placebo-tests-for-natural-experiments"><i class="fa fa-check"></i><b>8.2</b> Placebo tests for natural experiments</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="placebo.html"><a href="placebo.html#placebo-tests-for-instrumental-variables"><i class="fa fa-check"></i><b>8.2.1</b> Placebo tests for Instrumental Variables</a></li>
<li class="chapter" data-level="8.2.2" data-path="placebo.html"><a href="placebo.html#placebo-tests-for-regression-discontinuity-designs"><i class="fa fa-check"></i><b>8.2.2</b> Placebo tests for Regression Discontinuity Designs</a></li>
<li class="chapter" data-level="8.2.3" data-path="placebo.html"><a href="placebo.html#placebo-tests-for-difference-in-differences"><i class="fa fa-check"></i><b>8.2.3</b> Placebo tests for Difference in Differences</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="placebo.html"><a href="placebo.html#placebo-tests-for-observational-methods"><i class="fa fa-check"></i><b>8.3</b> Placebo tests for observational methods</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="cluster.html"><a href="cluster.html"><i class="fa fa-check"></i><b>9</b> Clustering</a>
<ul>
<li class="chapter" data-level="9.1" data-path="cluster.html"><a href="cluster.html#ClusterRCT"><i class="fa fa-check"></i><b>9.1</b> Clustering in Randomized Controlled Trials</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="cluster.html"><a href="cluster.html#an-example"><i class="fa fa-check"></i><b>9.1.1</b> An example</a></li>
<li class="chapter" data-level="9.1.2" data-path="cluster.html"><a href="cluster.html#design-effect"><i class="fa fa-check"></i><b>9.1.2</b> Design effect</a></li>
<li class="chapter" data-level="9.1.3" data-path="cluster.html"><a href="cluster.html#estimating-sampling-noise-accounting-for-clustering"><i class="fa fa-check"></i><b>9.1.3</b> Estimating sampling noise accounting for clustering</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="cluster.html"><a href="cluster.html#clustering-in-panel-data"><i class="fa fa-check"></i><b>9.2</b> Clustering in panel data</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="cluster.html"><a href="cluster.html#an-example-1"><i class="fa fa-check"></i><b>9.2.1</b> An example</a></li>
<li class="chapter" data-level="9.2.2" data-path="cluster.html"><a href="cluster.html#design-effect-in-panel-data"><i class="fa fa-check"></i><b>9.2.2</b> Design effect in panel data</a></li>
<li class="chapter" data-level="9.2.3" data-path="cluster.html"><a href="cluster.html#estimating-sampling-noise-in-panel-data-with-autocorrelated-error-terms"><i class="fa fa-check"></i><b>9.2.3</b> Estimating sampling noise in panel data with autocorrelated error terms</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="cluster.html"><a href="cluster.html#spatial-correlation"><i class="fa fa-check"></i><b>9.3</b> Spatial correlation</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="cluster.html"><a href="cluster.html#an-example-2"><i class="fa fa-check"></i><b>9.3.1</b> An example</a></li>
<li class="chapter" data-level="9.3.2" data-path="cluster.html"><a href="cluster.html#design-effect-in-spatially-autocorrelated-data"><i class="fa fa-check"></i><b>9.3.2</b> Design effect in spatially autocorrelated data</a></li>
<li class="chapter" data-level="9.3.3" data-path="cluster.html"><a href="cluster.html#estimating-sampling-noise-with-spatially-autocorrelated-data"><i class="fa fa-check"></i><b>9.3.3</b> Estimating sampling noise with spatially autocorrelated data</a></li>
<li class="chapter" data-level="9.3.4" data-path="cluster.html"><a href="cluster.html#testing-for-spatial-autocorrelation"><i class="fa fa-check"></i><b>9.3.4</b> Testing for spatial autocorrelation</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="cluster.html"><a href="cluster.html#clustering-on-a-network"><i class="fa fa-check"></i><b>9.4</b> Clustering on a network</a></li>
<li class="chapter" data-level="9.5" data-path="cluster.html"><a href="cluster.html#multi-way-clustering"><i class="fa fa-check"></i><b>9.5</b> Multi-way clustering</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="cluster.html"><a href="cluster.html#at-which-level-should-we-cluster"><i class="fa fa-check"></i><b>9.5.1</b> At which level should we cluster?</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="cluster.html"><a href="cluster.html#what-to-do-when-there-are-few-clusters"><i class="fa fa-check"></i><b>9.6</b> What to do when there are few clusters?</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="cluster.html"><a href="cluster.html#aggregation"><i class="fa fa-check"></i><b>9.6.1</b> Aggregation</a></li>
<li class="chapter" data-level="9.6.2" data-path="cluster.html"><a href="cluster.html#permutation-tests"><i class="fa fa-check"></i><b>9.6.2</b> Permutation tests</a></li>
<li class="chapter" data-level="9.6.3" data-path="cluster.html"><a href="cluster.html#wild-bootstrap"><i class="fa fa-check"></i><b>9.6.3</b> Wild bootstrap</a></li>
<li class="chapter" data-level="9.6.4" data-path="cluster.html"><a href="cluster.html#ibragimov-and-muller-2010-group-based-inference"><i class="fa fa-check"></i><b>9.6.4</b> Ibragimov and Muller (2010) group-based inference</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="cluster.html"><a href="cluster.html#CLTDD"><i class="fa fa-check"></i><b>9.7</b> Central Limit Theorems for Dependent Data</a></li>
<li class="chapter" data-level="9.8" data-path="cluster.html"><a href="cluster.html#DesignBasedClusters"><i class="fa fa-check"></i><b>9.8</b> Sampling-based and design-based approaches to clustering</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="LaLonde.html"><a href="LaLonde.html"><i class="fa fa-check"></i><b>10</b> LaLonde Tests</a></li>
<li class="chapter" data-level="11" data-path="Diffusion.html"><a href="Diffusion.html"><i class="fa fa-check"></i><b>11</b> Diffusion effects</a>
<ul>
<li class="chapter" data-level="11.1" data-path="Diffusion.html"><a href="Diffusion.html#allowing-for-diffusion-effects-in-rubin-causal-model"><i class="fa fa-check"></i><b>11.1</b> Allowing for diffusion effects in Rubin Causal Model</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="Diffusion.html"><a href="Diffusion.html#potential-outcomes-and-treatment-effects-with-diffusion-effects"><i class="fa fa-check"></i><b>11.1.1</b> Potential outcomes and treatment effects with diffusion effects</a></li>
<li class="chapter" data-level="11.1.2" data-path="Diffusion.html"><a href="Diffusion.html#encoding-the-absence-of-diffusion-effects"><i class="fa fa-check"></i><b>11.1.2</b> Encoding the absence of diffusion effects</a></li>
<li class="chapter" data-level="11.1.3" data-path="Diffusion.html"><a href="Diffusion.html#TreatmentExposure"><i class="fa fa-check"></i><b>11.1.3</b> Treatment exposure</a></li>
<li class="chapter" data-level="11.1.4" data-path="Diffusion.html"><a href="Diffusion.html#fundamental-problem-of-causal-inference-for-diffusion-effects"><i class="fa fa-check"></i><b>11.1.4</b> Fundamental problem of causal inference for diffusion effects</a></li>
<li class="chapter" data-level="11.1.5" data-path="Diffusion.html"><a href="Diffusion.html#bias-of-one-step-randomized-controlled-trials"><i class="fa fa-check"></i><b>11.1.5</b> Bias of one-step randomized controlled trials</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="Diffusion.html"><a href="Diffusion.html#diffusion-effects-with-coarse-networks"><i class="fa fa-check"></i><b>11.2</b> Diffusion effects with coarse networks</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="Diffusion.html"><a href="Diffusion.html#optimal-treatment-allocation-under-monotone-response"><i class="fa fa-check"></i><b>11.2.1</b> Optimal treatment allocation under monotone response</a></li>
<li class="chapter" data-level="11.2.2" data-path="Diffusion.html"><a href="Diffusion.html#identifying-optimal-treatment-levels"><i class="fa fa-check"></i><b>11.2.2</b> Identifying optimal treatment levels</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="Diffusion.html"><a href="Diffusion.html#diffusion-effects-with-detailed-networks"><i class="fa fa-check"></i><b>11.3</b> Diffusion effects with detailed networks</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="Diffusion.html"><a href="Diffusion.html#setting"><i class="fa fa-check"></i><b>11.3.1</b> Setting</a></li>
<li class="chapter" data-level="11.3.2" data-path="Diffusion.html"><a href="Diffusion.html#identification-of-causal-effects"><i class="fa fa-check"></i><b>11.3.2</b> Identification of causal effects</a></li>
<li class="chapter" data-level="11.3.3" data-path="Diffusion.html"><a href="Diffusion.html#estimation-of-causal-effects"><i class="fa fa-check"></i><b>11.3.3</b> Estimation of causal effects</a></li>
<li class="chapter" data-level="11.3.4" data-path="Diffusion.html"><a href="Diffusion.html#estimation-of-sampling-noise-6"><i class="fa fa-check"></i><b>11.3.4</b> Estimation of sampling noise</a></li>
<li class="chapter" data-level="11.3.5" data-path="Diffusion.html"><a href="Diffusion.html#nonparametric-tests-for-the-existence-of-diffusion-effects-based-on-randomization-inference"><i class="fa fa-check"></i><b>11.3.5</b> Nonparametric tests for the existence of diffusion effects based on randomization inference</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="Diffusion.html"><a href="Diffusion.html#diffusion-effects-with-did"><i class="fa fa-check"></i><b>11.4</b> Diffusion effects with DID</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Distribution.html"><a href="Distribution.html"><i class="fa fa-check"></i><b>12</b> Distributional effects</a></li>
<li class="chapter" data-level="13" data-path="meta.html"><a href="meta.html"><i class="fa fa-check"></i><b>13</b> Meta-analysis and Publication Bias</a>
<ul>
<li class="chapter" data-level="13.1" data-path="meta.html"><a href="meta.html#meta-analysis"><i class="fa fa-check"></i><b>13.1</b> Meta-analysis</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="meta.html"><a href="meta.html#basic-setting"><i class="fa fa-check"></i><b>13.1.1</b> Basic setting</a></li>
<li class="chapter" data-level="13.1.2" data-path="meta.html"><a href="meta.html#why-vote-counting-does-not-work"><i class="fa fa-check"></i><b>13.1.2</b> Why vote-counting does not work</a></li>
<li class="chapter" data-level="13.1.3" data-path="meta.html"><a href="meta.html#MetaWA"><i class="fa fa-check"></i><b>13.1.3</b> Meta-analysis when treatment effects are homogeneous: the fixed effects approach</a></li>
<li class="chapter" data-level="13.1.4" data-path="meta.html"><a href="meta.html#meta-analysis-when-treatment-effects-are-heterogeneous-the-random-effects-approach"><i class="fa fa-check"></i><b>13.1.4</b> Meta-analysis when treatment effects are heterogeneous: the random effects approach</a></li>
<li class="chapter" data-level="13.1.5" data-path="meta.html"><a href="meta.html#meta-regression"><i class="fa fa-check"></i><b>13.1.5</b> Meta-regression</a></li>
<li class="chapter" data-level="13.1.6" data-path="meta.html"><a href="meta.html#constantly-updated-meta-analysis"><i class="fa fa-check"></i><b>13.1.6</b> Constantly updated meta-analysis</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="meta.html"><a href="meta.html#publication-bias-and-site-selection-bias"><i class="fa fa-check"></i><b>13.2</b> Publication bias and site selection bias</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="meta.html"><a href="meta.html#sources-of-publication-bias-and-of-site-selection-bias-and-questionable-research-practices"><i class="fa fa-check"></i><b>13.2.1</b> Sources of publication bias and of site selection bias and Questionable Research Practices</a></li>
<li class="chapter" data-level="13.2.2" data-path="meta.html"><a href="meta.html#detection-of-and-correction-for-publication-bias"><i class="fa fa-check"></i><b>13.2.2</b> Detection of and correction for publication bias</a></li>
<li class="chapter" data-level="13.2.3" data-path="meta.html"><a href="meta.html#getting-rid-of-publication-bias-registered-reports-and-pre-analysis-plans"><i class="fa fa-check"></i><b>13.2.3</b> Getting rid of publication bias: registered reports and pre-analysis plans</a></li>
<li class="chapter" data-level="13.2.4" data-path="meta.html"><a href="meta.html#detection-of-and-correction-for-site-selection-bias"><i class="fa fa-check"></i><b>13.2.4</b> Detection of and correction for site selection bias</a></li>
<li class="chapter" data-level="13.2.5" data-path="meta.html"><a href="meta.html#vote-counting-and-publication-bias"><i class="fa fa-check"></i><b>13.2.5</b> Vote counting and publication bias</a></li>
<li class="chapter" data-level="13.2.6" data-path="meta.html"><a href="meta.html#the-value-of-a-statistically-significant-result"><i class="fa fa-check"></i><b>13.2.6</b> The value of a statistically significant result</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Bounds.html"><a href="Bounds.html"><i class="fa fa-check"></i><b>14</b> Bounds</a></li>
<li class="chapter" data-level="15" data-path="mediation-analysis.html"><a href="mediation-analysis.html"><i class="fa fa-check"></i><b>15</b> Mediation Analysis</a>
<ul>
<li class="chapter" data-level="15.1" data-path="mediation-analysis.html"><a href="mediation-analysis.html#mediation-analysis-a-framework"><i class="fa fa-check"></i><b>15.1</b> Mediation analysis: a framework</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="mediation-analysis.html"><a href="mediation-analysis.html#defining-mediated-and-unmediated-treatment-effects"><i class="fa fa-check"></i><b>15.1.1</b> Defining mediated and unmediated treatment effects</a></li>
<li class="chapter" data-level="15.1.2" data-path="mediation-analysis.html"><a href="mediation-analysis.html#decomposing-mediated-and-unmediated-effects"><i class="fa fa-check"></i><b>15.1.2</b> Decomposing mediated and unmediated effects</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="mediation-analysis.html"><a href="mediation-analysis.html#the-fundamental-problem-of-mediation-analysis"><i class="fa fa-check"></i><b>15.2</b> The Fundamental Problem of Mediation Analysis</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="mediation-analysis.html"><a href="mediation-analysis.html#the-fundamental-problem-of-mediation-analysis-1"><i class="fa fa-check"></i><b>15.2.1</b> The Fundamental Problem of Mediation Analysis</a></li>
<li class="chapter" data-level="15.2.2" data-path="mediation-analysis.html"><a href="mediation-analysis.html#biases-of-intuitive-comparisons"><i class="fa fa-check"></i><b>15.2.2</b> Biases of Intuitive Comparisons</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="mediation-analysis.html"><a href="mediation-analysis.html#mediation-analysis-with-experimental-data"><i class="fa fa-check"></i><b>15.3</b> Mediation analysis with experimental data</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="mediation-analysis.html"><a href="mediation-analysis.html#mediation-analysis-in-the-parallel-design"><i class="fa fa-check"></i><b>15.3.1</b> Mediation analysis in the Parallel design</a></li>
<li class="chapter" data-level="15.3.2" data-path="mediation-analysis.html"><a href="mediation-analysis.html#mediation-analysis-in-the-sequential-self-selection-design"><i class="fa fa-check"></i><b>15.3.2</b> Mediation analysis in the Sequential Self-Selection design</a></li>
<li class="chapter" data-level="15.3.3" data-path="mediation-analysis.html"><a href="mediation-analysis.html#mediation-analysis-in-the-crossover-design"><i class="fa fa-check"></i><b>15.3.3</b> Mediation analysis in the Crossover design</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="mediation-analysis.html"><a href="mediation-analysis.html#mediation-analysis-under-unconfoundedness"><i class="fa fa-check"></i><b>15.4</b> Mediation analysis under unconfoundedness</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="mediation-analysis.html"><a href="mediation-analysis.html#non-parametric-identification-under-sequential-ignorability"><i class="fa fa-check"></i><b>15.4.1</b> Non-parametric identification under sequential ignorability</a></li>
<li class="chapter" data-level="15.4.2" data-path="mediation-analysis.html"><a href="mediation-analysis.html#mediation-analysis-under-sequential-ignorability-in-linear-models"><i class="fa fa-check"></i><b>15.4.2</b> Mediation analysis under sequential ignorability in linear models</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="mediation-analysis.html"><a href="mediation-analysis.html#mediation-analysis-with-panel-data"><i class="fa fa-check"></i><b>15.5</b> Mediation analysis with panel data</a></li>
<li class="chapter" data-level="15.6" data-path="mediation-analysis.html"><a href="mediation-analysis.html#mediation-analysis-with-instruments"><i class="fa fa-check"></i><b>15.6</b> Mediation analysis with instruments</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="stratification.html"><a href="stratification.html"><i class="fa fa-check"></i><b>16</b> Stratification</a>
<ul>
<li class="chapter" data-level="16.1" data-path="stratification.html"><a href="stratification.html#ClassicalStratification"><i class="fa fa-check"></i><b>16.1</b> Analysis of classical stratified experiments</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="stratification.html"><a href="stratification.html#an-example-3"><i class="fa fa-check"></i><b>16.1.1</b> An example</a></li>
<li class="chapter" data-level="16.1.2" data-path="stratification.html"><a href="stratification.html#estimating-treatment-effects-with-stratified-randomized-controlled-trials"><i class="fa fa-check"></i><b>16.1.2</b> Estimating treatment effects with stratified randomized controlled trials</a></li>
<li class="chapter" data-level="16.1.3" data-path="stratification.html"><a href="stratification.html#estimating-sampling-noise-in-stratified-randomized-controlled-trials"><i class="fa fa-check"></i><b>16.1.3</b> Estimating sampling noise in stratified randomized controlled trials</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="stratification.html"><a href="stratification.html#PairRCT"><i class="fa fa-check"></i><b>16.2</b> Analysis of pairwise randomized controlled trials</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="stratification.html"><a href="stratification.html#building-a-sample-of-pairs"><i class="fa fa-check"></i><b>16.2.1</b> Building a sample of pairs</a></li>
<li class="chapter" data-level="16.2.2" data-path="stratification.html"><a href="stratification.html#estimating-treatment-effects-in-pairwise-randomized-controlled-trials"><i class="fa fa-check"></i><b>16.2.2</b> Estimating treatment effects in pairwise randomized controlled trials</a></li>
<li class="chapter" data-level="16.2.3" data-path="stratification.html"><a href="stratification.html#estimating-sampling-noise-in-pairwise-randomized-controlled-trials"><i class="fa fa-check"></i><b>16.2.3</b> Estimating sampling noise in pairwise randomized controlled trials</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="proofs.html"><a href="proofs.html"><i class="fa fa-check"></i><b>A</b> Proofs</a>
<ul>
<li class="chapter" data-level="A.1" data-path="proofs.html"><a href="proofs.html#proofs-of-results-in-chapter-reffpsi"><i class="fa fa-check"></i><b>A.1</b> Proofs of results in Chapter @ref(FPSI)</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="proofs.html"><a href="proofs.html#proofcheb"><i class="fa fa-check"></i><b>A.1.1</b> Proof of Theorem @ref(thm:uppsampnoise)</a></li>
<li class="chapter" data-level="A.1.2" data-path="proofs.html"><a href="proofs.html#proofCLT"><i class="fa fa-check"></i><b>A.1.2</b> Proof of Theorem @ref(thm:asympnoiseWW)</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="proofs.html"><a href="proofs.html#proofs-of-results-in-chapter-refrct"><i class="fa fa-check"></i><b>A.2</b> Proofs of results in Chapter @ref(RCT)</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="proofs.html"><a href="proofs.html#proofIdentLATE"><i class="fa fa-check"></i><b>A.2.1</b> Proof of Theorem @ref(thm:IdentLATE)</a></li>
<li class="chapter" data-level="A.2.2" data-path="proofs.html"><a href="proofs.html#proofWaldIV"><i class="fa fa-check"></i><b>A.2.2</b> Proof of Theorem @ref(thm:WaldIV)</a></li>
<li class="chapter" data-level="A.2.3" data-path="proofs.html"><a href="proofs.html#ProofAsymWald"><i class="fa fa-check"></i><b>A.2.3</b> Proof of Theorem @ref(thm:asymWald)</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="proofs.html"><a href="proofs.html#proofs-of-results-in-chapter-refne"><i class="fa fa-check"></i><b>A.3</b> Proofs of results in Chapter @ref(NE)</a>
<ul>
<li class="chapter" data-level="A.3.1" data-path="proofs.html"><a href="proofs.html#proofEstimDID"><i class="fa fa-check"></i><b>A.3.1</b> Proof of Theorem @ref(thm:EstimDID)</a></li>
<li class="chapter" data-level="A.3.2" data-path="proofs.html"><a href="proofs.html#proofasympnoiseDIDCross"><i class="fa fa-check"></i><b>A.3.2</b> Proof of Theorem @ref(thm:asympnoiseDIDCross)</a></li>
<li class="chapter" data-level="A.3.3" data-path="proofs.html"><a href="proofs.html#proofEquivDIDSApop"><i class="fa fa-check"></i><b>A.3.3</b> Proof of Theorem @ref(thm:EquivDIDSApop)</a></li>
<li class="chapter" data-level="A.3.4" data-path="proofs.html"><a href="proofs.html#proofEquivDIDSAsamp"><i class="fa fa-check"></i><b>A.3.4</b> Proof of Theorem @ref(thm:EquivDIDSAsamp)</a></li>
<li class="chapter" data-level="A.3.5" data-path="proofs.html"><a href="proofs.html#proofasympnoiseSACross"><i class="fa fa-check"></i><b>A.3.5</b> Proof of Theorem @ref(thm:asympnoiseSACross)</a></li>
<li class="chapter" data-level="A.3.6" data-path="proofs.html"><a href="proofs.html#proofasympnoiseSATTCross"><i class="fa fa-check"></i><b>A.3.6</b> Proof of Theorem @ref(thm:asympnoiseSATTCross)</a></li>
<li class="chapter" data-level="A.3.7" data-path="proofs.html"><a href="proofs.html#proofasympnoiseSAPanel"><i class="fa fa-check"></i><b>A.3.7</b> Proof of Theorem @ref(thm:asympnoiseSAPanel)</a></li>
<li class="chapter" data-level="A.3.8" data-path="proofs.html"><a href="proofs.html#proofasympnoiseSATTPanel"><i class="fa fa-check"></i><b>A.3.8</b> Proof of Theorem @ref(thm:asympnoiseSATTPanel)</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="proofs.html"><a href="proofs.html#proofs-of-results-in-chapter-refom"><i class="fa fa-check"></i><b>A.4</b> Proofs of results in Chapter @ref(OM)</a>
<ul>
<li class="chapter" data-level="A.4.1" data-path="proofs.html"><a href="proofs.html#proofAsympWWOLS10"><i class="fa fa-check"></i><b>A.4.1</b> Proof of Theorem @ref(thm:AsympWWOLS10)</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="proofs.html"><a href="proofs.html#proofs-of-results-in-chapter-refcluster"><i class="fa fa-check"></i><b>A.5</b> Proofs of results in Chapter @ref(cluster)</a>
<ul>
<li class="chapter" data-level="A.5.1" data-path="proofs.html"><a href="proofs.html#proofVarWWClus"><i class="fa fa-check"></i><b>A.5.1</b> Proof of Theorem @ref(thm:VarWWClus)</a></li>
<li class="chapter" data-level="A.5.2" data-path="proofs.html"><a href="proofs.html#proofasympnoiseSATTPanelAR1"><i class="fa fa-check"></i><b>A.5.2</b> Proof of Theorem @ref(thm:asympnoiseSATTPanelAR1)</a></li>
<li class="chapter" data-level="A.5.3" data-path="proofs.html"><a href="proofs.html#proofVarWWSpatial"><i class="fa fa-check"></i><b>A.5.3</b> Proof of Theorem @ref(thm:VarWWSpatial)</a></li>
</ul></li>
<li class="chapter" data-level="A.6" data-path="proofs.html"><a href="proofs.html#proofs-of-results-in-chapter-refdiffusion"><i class="fa fa-check"></i><b>A.6</b> Proofs of results in Chapter @ref(Diffusion)</a>
<ul>
<li class="chapter" data-level="A.6.1" data-path="proofs.html"><a href="proofs.html#proofSmoothSymAlloc"><i class="fa fa-check"></i><b>A.6.1</b> Proof of Theorem @ref(thm:SmoothSymAlloc)</a></li>
<li class="chapter" data-level="A.6.2" data-path="proofs.html"><a href="proofs.html#proofIdentSmoothSymAlloc"><i class="fa fa-check"></i><b>A.6.2</b> Proof of Theorem @ref(thm:IdentSmoothSymAlloc)</a></li>
<li class="chapter" data-level="A.6.3" data-path="proofs.html"><a href="proofs.html#proofContagionDiffusionSmoothSymAlloc"><i class="fa fa-check"></i><b>A.6.3</b> Proof of Theorem @ref(thm:ContagionDiffusionSmoothSymAlloc)</a></li>
</ul></li>
<li class="chapter" data-level="A.7" data-path="proofs.html"><a href="proofs.html#proofs-of-results-in-chapter-refstratification"><i class="fa fa-check"></i><b>A.7</b> Proofs of results in Chapter @ref(stratification)</a>
<ul>
<li class="chapter" data-level="A.7.1" data-path="proofs.html"><a href="proofs.html#proofSFEdecomp"><i class="fa fa-check"></i><b>A.7.1</b> Proof of Theorem @ref(thm:SFEdecomp)</a></li>
<li class="chapter" data-level="A.7.2" data-path="proofs.html"><a href="proofs.html#proofSFEconsistent"><i class="fa fa-check"></i><b>A.7.2</b> Proof of Theorem @ref(thm:SFEconsistent)</a></li>
<li class="chapter" data-level="A.7.3" data-path="proofs.html"><a href="proofs.html#proofSATUnbiasedConsistent"><i class="fa fa-check"></i><b>A.7.3</b> Proof of Theorem @ref(thm:SATUnbiasedConsistent)</a></li>
<li class="chapter" data-level="A.7.4" data-path="proofs.html"><a href="proofs.html#proofasympnoiseSATStrata"><i class="fa fa-check"></i><b>A.7.4</b> Proof of Theorem @ref(thm:asympnoiseSATStrata)</a></li>
<li class="chapter" data-level="A.7.5" data-path="proofs.html"><a href="proofs.html#proofasympnoiseSFEStrata"><i class="fa fa-check"></i><b>A.7.5</b> Proof of Theorem @ref(thm:asympnoiseSFEStrata)</a></li>
<li class="chapter" data-level="A.7.6" data-path="proofs.html"><a href="proofs.html#proofPairUnbiasedConsistent"><i class="fa fa-check"></i><b>A.7.6</b> Proof of Theorem @ref(thm:PairUnbiasedConsistent)</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Tools for Causal Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="Diffusion" class="section level1 hasAnchor" number="11">
<h1><span class="header-section-number">Chapter 11</span> Diffusion effects<a href="Diffusion.html#Diffusion" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Up until now, we have assumed that the treatment received by one unit in the population did not have any impact on any other unit.
We have not encoded this assumption formally, but we have implicitly made it all along, starting with our encoding of Rubin Causal Model in Chapter <a href="FPCI.html#FPCI">1</a>.
In this chapter, we are going to relax that assumption, and learn how to deal with the more general cases that then appear.
We are going to cover a host of very important applications, that go from identifying contagion effects to identifying the optimal proportion of individuals to treat at independent locations.
We are first going to start by introducing an extended Rubin Causal Model allowing for diffusion effects and introducing ways to discipline this model so that it becomes estimable.
We are then going to look at various ways to estimate this model and the precision of the resulting estimates, using RCTs, DID, and both parametric and non parametric approaches.
Most of these developments are fairly recent and will enable us to get rapidly in touch with the research frontier.</p>
<div id="allowing-for-diffusion-effects-in-rubin-causal-model" class="section level2 hasAnchor" number="11.1">
<h2><span class="header-section-number">11.1</span> Allowing for diffusion effects in Rubin Causal Model<a href="Diffusion.html#allowing-for-diffusion-effects-in-rubin-causal-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we are going to detail how to encode causality in the presence of diffusion effects.
We are going to start with potential outcomes and a general framework, before considering two very important special cases: the case where diffusion effects are absent and the case where they take a specific form.</p>
<div id="potential-outcomes-and-treatment-effects-with-diffusion-effects" class="section level3 hasAnchor" number="11.1.1">
<h3><span class="header-section-number">11.1.1</span> Potential outcomes and treatment effects with diffusion effects<a href="Diffusion.html#potential-outcomes-and-treatment-effects-with-diffusion-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The main starting point for an extended Rubin Causal Model is to acknowledge that the treatment status of the <span class="math inline">\(N^*\)</span> observations in the population (with <span class="math inline">\(N^*\)</span> possibly infinite) might influence the observed outcome for individual <span class="math inline">\(i\)</span>.
Let <span class="math inline">\(\mathbf{d}=\left\{d_1,\dots,d_{N^*}\right\}\)</span>, with <span class="math inline">\(d_j\in\left\{0,1\right\}\)</span>, <span class="math inline">\(\forall j\in\left\{1,\dots,N^*\right\}\)</span>.
We can therefore write the generalized potential outcome for individual <span class="math inline">\(i\)</span> as <span class="math inline">\(Y_i^{\mathbf{d}}\)</span>.
If we write <span class="math inline">\(\mathbf{D}=\left\{D_1,\dots,D_{N^*}\right\}\)</span>, we can then write the observed outcome for individual <span class="math inline">\(i\)</span> as <span class="math inline">\(Y_i^{\mathbf{D}}\)</span>.
The average effect of the treatment becomes:</p>
<p><span class="math display">\[\begin{align*}
  \Delta^Y_{ATE}(\mathbf{D}) &amp; = \esp{Y_i^{\mathbf{D}}-Y_i^{\mathbf{0}}}\\
                &amp; = \esp{Y_i^{\mathbf{D}}-Y_i^{\mathbf{0}}|D_i=1}\Pr(D_i=1)+\esp{Y_i^{\mathbf{D}}-Y_i^{\mathbf{0}}|D_i=0}\Pr(D_i=0)\\
                &amp; = \Delta^Y_{TT}(\mathbf{D})\Pr(D_i=1)+\Delta^Y_{TUT}(\mathbf{D})\Pr(D_i=0)\\
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\mathbf{0}\)</span> is the null vector of length <span class="math inline">\(N^*\)</span>.
Note that the average effect of the treatment is equal to a weighted average of the effect on the treated and the effect of the untreated.
Note also that these effects differ from the ones we defined in Chapters <a href="FPCI.html#FPCI">1</a> and <a href="FPSI.html#FPSI">2</a>: they depend on the whole vector of treatment assignments.
Indeed, the effect on the untreated is not the one we defined in Section <a href="OM.html#BiasOLS">5.1.3</a>: it is not the difference between taking the treatment and not taking the treatment for those who do not take it.
The TUT we have defined here is the difference in outcomes for the ones who do not take the treatment between a case where the treated individuals in the population receive the treatment and a case where no one receives the treatment.
The only effect of the treatment on the untreated is indirect: it is the effect that transits through diffusion of the treatment effects from the treated to the untreated.
It can be when farmers adopt technologies after seeing their treated neighbors adopt them, or when people contract less diseases because their neighbors are vaccinated.
These effects can also be negative, for example when untreated job seekers are crowded out of a job by the job counselling received by the treated.
In general, I like to call these effects <strong>contagion</strong> effects, to insist on the fact that they are indirect.</p>
<p>Note that the effect on the treated also is different and depends on the whole treatment vector.
In that case, we allow for the effect on the treated to depend on whether or not some or all of their neighbors are treated.
The effect of a vaccine might for example be higher when more people around us are vaccinated.
Or a technology is more likely to be adopted if more neighbors are informed that it exists and encourage to adopt it.
I call these types of effects <strong>amplification</strong> effects, to denote the fact that whether the treated react a lot or not to the treatment might depend on whether their neighbors are also treated.
These effects might also be negative, for example when more job seekers receive counselling, the effectiveness of counselling on the treated might very well decrease.</p>
</div>
<div id="encoding-the-absence-of-diffusion-effects" class="section level3 hasAnchor" number="11.1.2">
<h3><span class="header-section-number">11.1.2</span> Encoding the absence of diffusion effects<a href="Diffusion.html#encoding-the-absence-of-diffusion-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we are going to state the assumption of absence of diffusion effects, that is required for all our previous estimators to work.
This assumption, called the Stable Unit Treatment Value Assumption, is stated as follows:</p>
<div class="hypothesis">
<p><span id="hyp:SUTVA" class="hypothesis"><strong>Hypothesis 11.1  (Stable Unit Treatment Value Assumption) </strong></span>We assume that the effect of the treatment on individual <span class="math inline">\(i\)</span> only depends on whether <span class="math inline">\(i\)</span> receives the treatment or not, and not on whether other individuals in the population receive the treatment as well: <span class="math inline">\(\forall i\)</span>, <span class="math inline">\(D_i=D&#39;_i\Rightarrow Y_i^{\mathbf{D}}=Y_i^{\mathbf{D&#39;}}\)</span>, <span class="math inline">\(\forall\mathbf{D}\neq\mathbf{D&#39;}\)</span>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-252" class="remark"><em>Remark</em>. </span>SUTVA has been coined by Don Rubin in a series of papers: Rubin (<a href="https://doi.org/10.1214/aos/1176344064">1978</a>, <a href="https://doi.org/10.2307/2287653">1980</a>, <a href="https://doi.org/10.1214/ss/1177012032">1990</a>).</p>
</div>
<p>SUTVA implies the version of Rubin Causal Model that we have introduced in Chapter <a href="FPCI.html#FPCI">1</a>.
Indeed, SUTVA implies that the only treatment status that matters for the potential outcomes of individual <span class="math inline">\(i\)</span> is the treatment status of individual <span class="math inline">\(i\)</span>.
As a consequence, we have the following results:</p>
<div class="theorem">
<p><span id="thm:RCMSUTVA" class="theorem"><strong>Theorem 11.1  (Rubin Causal Model and Treatment Effects Under SUTVA) </strong></span>Under Assumption <a href="Diffusion.html#hyp:SUTVA">11.1</a>, the potential outcome of individual <span class="math inline">\(i\)</span> only depends on its treatment status: <span class="math inline">\(\forall i\)</span>, <span class="math inline">\(Y_i^{\mathbf{D}}=Y_i^{D_i}\)</span>.
As a consequence:</p>
<p><span class="math display">\[\begin{align*}
  \Delta^Y_{TT}(\mathbf{D}) &amp; = \Delta^Y_{TT}\\
  \Delta^Y_{TUT}(\mathbf{D}) &amp; = 0.
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-253" class="proof"><em>Proof</em>. </span>The proof of the first result that <span class="math inline">\(Y_i^{\mathbf{D}}=Y_i^{D_i}\)</span> is straightforward from Assumption <a href="Diffusion.html#hyp:SUTVA">11.1</a>.
We therefore have</p>
<p><span class="math display">\[\begin{align*}
  \Delta^Y_{TT}(\mathbf{D}) &amp; = \esp{Y_i^{\mathbf{D}}-Y_i^{\mathbf{0}}|D_i=1}\\
                            &amp; = \esp{Y_i^{D_i}-Y_i^{0}|D_i=1}\\
                            &amp; = \esp{Y_i^{1}-Y_i^{0}|D_i=1}\\
                            &amp; =   \Delta^Y_{TT}\\
  \Delta^Y_{TUT}(\mathbf{D})&amp; = \esp{Y_i^{\mathbf{D}}-Y_i^{\mathbf{0}}|D_i=0}\\
                            &amp; = \esp{Y_i^{D_i}-Y_i^{0}|D_i=0}\\
                            &amp; = \esp{Y_i^{0}-Y_i^{0}|D_i=0}\\
                            &amp; = 0.
\end{align*}\]</span></p>
</div>
</div>
<div id="TreatmentExposure" class="section level3 hasAnchor" number="11.1.3">
<h3><span class="header-section-number">11.1.3</span> Treatment exposure<a href="Diffusion.html#TreatmentExposure" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In general, it is going to prove extremely difficult to do econometric analysis using the very general setting we have defined so far, with potential outcomes depending on the whole treatment vector in the population.
A useful simplifying assumption that we often have to resort to is to specify an exposure mapping, that relates the whole treatment vector to the specifications relevant for the outcomes of interest.
In order to specify the exposure mapping, we are going to assume that all units in the population are part of a network.
This network is summarized by an <span class="math inline">\(N^*\times N^*\)</span> contiguity matrix <span class="math inline">\(A\)</span> where each element <span class="math inline">\(a_{j,i}\)</span> (with <span class="math inline">\(j\)</span> denoting the line and <span class="math inline">\(i\)</span> the column) measures the strength of the relationship between <span class="math inline">\(j\)</span> and <span class="math inline">\(i\)</span>.
For example, if <span class="math inline">\(j\)</span> mentions <span class="math inline">\(i\)</span> as a friend, <span class="math inline">\(a_{j,i}=1\)</span>, whereas <span class="math inline">\(a_{i,j}=1\)</span> whenever <span class="math inline">\(i\)</span> mentions <span class="math inline">\(j\)</span> as a friend.
We can enforce the graph to be symmetric, that is <span class="math inline">\(a_{j,i}=a_{i,j}\)</span>, <span class="math inline">\(\forall (i,j)\)</span>, but it does not have to be the case.
For example, water quality at some point <span class="math inline">\(i\)</span> along a river stream depends on whether water is treated at a point <span class="math inline">\(j\)</span> upstream, but water quality in <span class="math inline">\(j\)</span> does not depend on treatments in a downstream point <span class="math inline">\(i\)</span>.
Because water flows in one direction, the network is not symmetric.</p>
<p>Equipped with a network of links, and denoting <span class="math inline">\(\mathbf{\Omega}=2^{N^*}\)</span> the set of possible treatment allocations, and <span class="math inline">\(\mathbf{\Theta}\)</span> the set of parameters <span class="math inline">\(\theta_i\)</span> relevant for the value of treatment exposure of unit <span class="math inline">\(i\)</span> (possibly containing features of the <span class="math inline">\(A\)</span> matrix), we can define treatment exposure as a mapping <span class="math inline">\(f\)</span> from <span class="math inline">\(\mathbf{\Omega}\times\mathbf{\Theta}\)</span> to <span class="math inline">\(\mathbf{\Delta}\)</span>, the set of possible treatment exposure: <span class="math inline">\(\Delta_i=f(\mathbf{D},\theta_i)\)</span>.
A key assumption we are going to make is that the exposure mapping is propermy specified, that is that it captures perfectly the intricacies of the effects of various treatment vectors:</p>
<div class="hypothesis">
<p><span id="hyp:PropSpecifyExpMap" class="hypothesis"><strong>Hypothesis 11.2  (Properly specified exposure mapping) </strong></span><span class="math inline">\(\forall i\)</span>, <span class="math inline">\(\forall\mathbf{D}\neq\mathbf{D&#39;}\in\mathbf{\Omega}\)</span>, <span class="math inline">\(\forall \theta_i\in\mathbf{\Theta}\)</span>, <span class="math inline">\(f(\mathbf{D},\theta_i)=f(\mathbf{D&#39;},\theta_i)\Rightarrow Y_i^{\mathbf{D}}=Y_i^{\mathbf{D&#39;}}\)</span>.</p>
</div>
<p>Under Assumption <a href="Diffusion.html#hyp:PropSpecifyExpMap">11.2</a>, the potential outcomes can be written as functions of treatment exposure only: <span class="math inline">\(Y_i^{\Delta_i}\)</span>.<br />
As a consequence, we can now define the average treatment effect of the treatment on the treated as follows:</p>
<p><span class="math display">\[\begin{align*}
  \Delta^Y_{TT}(\mathbf{d}) &amp; = \esp{Y_i^{\mathbf{d}}-Y_i^{\mathbf{0}}|\Delta_i=\mathbf{d}},
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\Delta^Y_{TT}(\mathbf{d})\)</span> measures the impact of treatment exposure <span class="math inline">\(\mathbf{d}\)</span> on those who have received it.</p>
<div class="remark">
<p><span id="unlabeled-div-254" class="remark"><em>Remark</em>. </span>The framework based on the use of an exposure mapping has been developped by <a href="https://doi.org/10.1111/j.1368-423X.2012.00368.x">Manski (2013)</a> and <a href="https://doi.org/10.1214/16-AOAS1005">Aronow and Samii (2017)</a>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-255" class="remark"><em>Remark</em>. </span>I use the term ``average treatment effect on the treated’’ because <span class="math inline">\(\Delta^Y_{ATE}(\mathbf{d})\)</span> measures the effect of receiving a treatment vector <span class="math inline">\(d\)</span> on those who receive it.</p>
</div>
<p>We are now equipped with tools that enable us to define treatment effects in the presence of diffusion effects, and to identify various types of diffusion effects.
The key concept that we are going to have to specify is treatment exposure: how does it change with various applications and how do we go around identifying it in various precise cases?
What can we do as well to test for features of treatment exposure without completely specifying it?
This is what we are going to see in what follows, first in the case of Randomized Controlled Trials, and then in the case of Difference in Differences.
We are going to go step by step, and first we ware going to start with simpler networks, that I call coarse networks, before looking at what we can do with more complex networks.</p>
</div>
<div id="fundamental-problem-of-causal-inference-for-diffusion-effects" class="section level3 hasAnchor" number="11.1.4">
<h3><span class="header-section-number">11.1.4</span> Fundamental problem of causal inference for diffusion effects<a href="Diffusion.html#fundamental-problem-of-causal-inference-for-diffusion-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>With diffusion effects and treatment exposure, the Fundamental Problem of Causal Inference strikes again.
Let state the problem using our more general framework of treatment exposure:</p>
<div class="theorem">
<p><span id="thm:FPCIDiff" class="theorem"><strong>Theorem 11.2  (Fundamental problem of causal inference with diffusion effects) </strong></span>It is impossible to observe <span class="math inline">\(\Delta^Y_{TT}(\mathbf{d})\)</span>, <span class="math inline">\(\forall d\in\mathbf{\Delta}\)</span>, either in the population or in the sample.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-256" class="proof"><em>Proof</em>. </span>For the population TT:</p>
<p><span class="math display">\[\begin{align*}
  \Delta^Y_{TT}(\mathbf{d}) &amp; = \esp{Y_i^{\mathbf{d}}-Y_i^{\mathbf{0}}|\Delta_i=\mathbf{d}} \\
                &amp; = \esp{Y_i^{\mathbf{d}}|\Delta_i=\mathbf{d}}-\esp{Y_i^{\mathbf{0}}|\Delta_i=\mathbf{d}}\\
                &amp; = \esp{Y_i|\Delta_i=\mathbf{d}}-\esp{Y_i^{\mathbf{0}}|\Delta_i=\mathbf{d}}.
\end{align*}\]</span></p>
<p><span class="math inline">\(\esp{Y_i^{\mathbf{0}}|\Delta_i=\mathbf{d}}\)</span> is unobserved, and so is <span class="math inline">\(\Delta^Y_{TT}\)</span>.
A similar reasoning holds for the sample average treatment effect.</p>
</div>
<p>We also have a novel formulation of the bias of intuitive methods.
For example, selection bias now depends on <span class="math inline">\(\mathbf{d}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\Delta^Y_{SB}(\mathbf{d}) &amp; = \Delta^Y_{WW}(\mathbf{d})-\Delta^Y_{TT}(\mathbf{d}) \\
              &amp; = \esp{Y_i|\Delta_i=\mathbf{d}}-\esp{Y_i|\Delta_i=\mathbf{0}}-\esp{Y_i^{\mathbf{d}}-Y_i^{\mathbf{0}}|\Delta_i=\mathbf{d}}\\
              &amp; = \esp{Y_i^{\mathbf{0}}|\Delta_i=\mathbf{d}}-\esp{Y_i^{\mathbf{0}}|\Delta_i=\mathbf{0}}.
\end{align*}\]</span></p>
<div class="remark">
<p><span id="unlabeled-div-257" class="remark"><em>Remark</em>. </span>Why is the with/without comparison of individuals with treatment exposure <span class="math inline">\(\Delta_i=\mathbf{d}\)</span> and those with treatment exposure <span class="math inline">\(\Delta_i=\mathbf{0}\)</span> biased for average treatment effect on the treated <span class="math inline">\(\Delta^Y_{TT}(\mathbf{d})\)</span>?
This is because treatment exposure might be correlated with unobserved confounders: individuals with higher treatment exposure might be systematically different from those with the reference level of treatment exposure (here <span class="math inline">\(\mathbf{0}\)</span>).</p>
</div>
</div>
<div id="bias-of-one-step-randomized-controlled-trials" class="section level3 hasAnchor" number="11.1.5">
<h3><span class="header-section-number">11.1.5</span> Bias of one-step randomized controlled trials<a href="Diffusion.html#bias-of-one-step-randomized-controlled-trials" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Explain direction of bias</strong></p>
</div>
</div>
<div id="diffusion-effects-with-coarse-networks" class="section level2 hasAnchor" number="11.2">
<h2><span class="header-section-number">11.2</span> Diffusion effects with coarse networks<a href="Diffusion.html#diffusion-effects-with-coarse-networks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Coarse networks are networks where we do not have a lot of information on the connections between individuals: we only know whether they belong to the same influence group or not.
This type of network characterizes for example of a group of villages, or municipalities, or classes, for which we do not know which links individuals have between each other other than they belong to the same group.
More formally, coarse networks can be characterized by the following property:</p>
<div class="hypothesis">
<p><span id="hyp:CoarseNetwork" class="hypothesis"><strong>Hypothesis 11.3  (Coarse network) </strong></span>We say that our population is characterized by a coarse network if the observed matrix of connections <span class="math inline">\(A\)</span> is block diagonal and we do not know which nodes are activated within each block.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-258" class="remark"><em>Remark</em>. </span>A blog diagonal influence matrix is composed of a set of groups or clusters within which observations influence each other and across which we assume all influences are muted.
This is of course a simplification: some units within a cluster might not really be connected, while some units might be connected to units in an other group.
Also, not all units might be equivalent within a group, with some being more central (<em>e.g.</em> connected) than others.
In a coarse network, we are assuming these differences away.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-259" class="remark"><em>Remark</em>. </span>Another way of framing coarse networks is to say that there is unknown interference within clusters (and no interference across).
This is <a href="http://arxiv.org/abs/2011.08174">Viviano (2023)</a>’s definition.
With Viviano’s approach to coarse networks, we do not know which units interfere within each network and how they do.</p>
</div>
<p>With a coarse network approach, under Assumption <a href="Diffusion.html#hyp:CoarseNetwork">11.3</a>, we might specialize the exposure mapping to things we might know, that is whether the unit itself is treated or not and the proportion of units that are treated in a given cluster <span class="math inline">\(c\)</span>, <span class="math inline">\(p_c\)</span>, or, more generally, the proportion of units with characteristics <span class="math inline">\(X_i=x\)</span> that are treated within clusters with characteristics <span class="math inline">\(Z_c=z\)</span>: <span class="math inline">\(p(x,z)\)</span>.
As a consequence, we might write potential outcomes as <span class="math inline">\(Y_i^{D_i,p_c}\)</span> or, more generally, <span class="math inline">\(Y_i^{D_i,\left\{p(x,Z_c)\right\}_{x\in\mathcal{X}}}\)</span>, with <span class="math inline">\(\mathcal{X}\)</span> the support of <span class="math inline">\(X_i\)</span>.</p>
<div class="remark">
<p><span id="unlabeled-div-260" class="remark"><em>Remark</em>. </span>Lemma 2.1 in <a href="http://arxiv.org/abs/2011.08174">Viviano (2023)</a> shows an example of assumptions under which we can simplify the exposiure mapping and obtain potential outcomes as a function of the proportion of treated units and cluster and unit characteristics.</p>
</div>
<p>Under Assumption <a href="Diffusion.html#hyp:CoarseNetwork">11.3</a>, we can write the average effect of treating a cluster with a proportion of treated <span class="math inline">\(p_c=p\)</span> as follows:</p>
<p><span class="math display">\[\begin{align*}
  \Delta^Y_{TT}(p) &amp; = \esp{Y_i^{D_i,p}-Y_i^{0,0}|p_c=p}\\
                &amp; = \esp{Y_i^{1,p}-Y_i^{0,0}|D_i=1,p_c=p}\Pr(D_i=1|p_c=p)\\
                &amp; \phantom{=} +\esp{Y_i^{0,p}-Y_i^{0,0}|D_i=0,p_c=p}\Pr(D_i=0|p_c=p)\\
                &amp; = \Delta^Y_{TDT}(p)\Pr(D_i=1|p_c=p)+\Delta^Y_{TIT}(p)\Pr(D_i=0|p_c=p),
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\Delta^Y_{TDT}(p)\)</span> is the Average Treatment Effect on the Directly Treated and <span class="math inline">\(\Delta^Y_{TIT}(p)\)</span> is the Average Treatment Effect on the Indirectly Treated.</p>
<p>The main question under Assumption <a href="Diffusion.html#hyp:CoarseNetwork">11.3</a> is to find the allocation of treated units that maximizes some objective function.
We are going to make a distinction between two different cases:</p>
<ol style="list-style-type: decimal">
<li>In the first case, we have a pre-specified budget for treatment effort (in terms of number of treated units) and we have to choose how to spend it optimally.
This often happens in practical policy applications where the budget has been pre-approved but you do not know how to implement it in the most optimal way possible.</li>
<li>In the second case, we already have an existing policy in place, and we would like to know whether it is optimal, and in which direction we should take it if we happen to have some additional budget.</li>
</ol>
<div id="optimal-treatment-allocation-under-monotone-response" class="section level3 hasAnchor" number="11.2.1">
<h3><span class="header-section-number">11.2.1</span> Optimal treatment allocation under monotone response<a href="Diffusion.html#optimal-treatment-allocation-under-monotone-response" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>I develop result on this setting in my own ongoing research.
In order to fix ideas, we are going to start with a simple network with two clusters.
We will then look at what happens with a more general network.
Finally, we will look at how we can use two-steps clustered RCTs to estimate the required parameters and decide on the optimal allocation.</p>
<div id="a-simple-model" class="section level4 hasAnchor" number="11.2.1.1">
<h4><span class="header-section-number">11.2.1.1</span> A simple model<a href="Diffusion.html#a-simple-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let’s start with a very simple example of a network with two clusters <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span>.
Let’s also consider only the case of a discrete outcome (such as participation in a program, getting vaccinated, contracting a disease, adopting a technology, etc.).
For simplicity, we are also going to write that potential outcomes are realizations of a continuous utility variable crossing a threshold:</p>
<p><span class="math display">\[\begin{align*}
  Y_{i}^{0,P_c} &amp; = \uns{\underbrace{\delta_0 + \beta_0 P_{c} -\epsilon_{i,0}}_{Y^*_{i,0}}\geq0}\\
  Y_{i}^{1,P_c} &amp; = \uns{\underbrace{\delta_1 + \beta_1 P_{c} -\epsilon_{i,1}}_{Y^*_{i,1}}\geq0}.
\end{align*}\]</span></p>
<p>What this models tells us is that, when no one else in the cluster is treated (<span class="math inline">\(P_c=0\)</span>), the individual level effect of being treated is equal to <span class="math inline">\(\Delta^Y_i=\uns{\epsilon_{i,1}\leq\delta_1}-\uns{\epsilon_{i,0}\leq\delta_0}\)</span>.
When some units starts receiving the treatment, we have two indirect effects:</p>
<ul>
<li>Increasing the proportion of treated units impacts the outcomes of untreated units, through <span class="math inline">\(\beta_0\)</span>.
This is what I call a <strong>contagion</strong> effect, in which untreated units are somehow contaminated by the treatment received by the treated individuals in the same cluster.
Contagion might refer to receiving information about the existence of a program and eventually deciding the enroll, or being protected by the fact that some neighbors are taking a treatment (in that case, contagion effects might actually prevent some untreated units from being contaminated).
Contagion effects might be negative, if for example treated individuals who receive job training or job search assistance end up finding jobs that would have been allocated to some of the untreated individuals in the absence of the treatment.</li>
<li>Increasing the proportion of treated units impacts the outcomes of treated units, through <span class="math inline">\(\beta_1\)</span>.
This is what I call an <strong>amplification</strong> effect.
There is amplification each time a treated units increases its likelihood of a positive outcome because more units are treated.
This might happen when technological adoption occurs only after most individuals in the cluster have been exposed to it and convinced to make a change.</li>
</ul>
<p>In order to get even more intuition on this problem, we are going to specialize it even further by making the following set of assumptions:</p>
<div class="hypothesis">
<p><span id="hyp:SimpleAllocHyp" class="hypothesis"><strong>Hypothesis 11.4  (Simplified Allocation Problem) </strong></span>We assume that the allocation problem is characterized as follows:</p>
<ul>
<li>There are only two nodes <span class="math inline">\(c=1\)</span> and <span class="math inline">\(c=2\)</span>.</li>
<li>A mass of <span class="math inline">\(1\)</span> units reside at each node.</li>
<li>We can only treat a mass of <span class="math inline">\(1\)</span> units.</li>
<li>We assume the constraint is saturated so that we use all available treatments: <span class="math inline">\(p_1+p_2=1\)</span>.</li>
<li><span class="math inline">\(\epsilon_1\)</span> and <span class="math inline">\(\epsilon_0\)</span> are uniform on <span class="math inline">\(\left[0,1\right]\)</span>.</li>
</ul>
</div>
<p>Under Assumption <a href="Diffusion.html#hyp:SimpleAllocHyp">11.4</a>, we can set <span class="math inline">\(p_1=p\)</span> and <span class="math inline">\(p_2=1-p\)</span>.
Let’s assume our goal is to maximize the total amount of people with <span class="math inline">\(Y_i=1\)</span>.
Under Assumption <a href="Diffusion.html#hyp:SimpleAllocHyp">11.4</a>, this is equivalent to maximizing the sum of the adoption rates at both nodes.
Using the fact that the constraint is saturated, we can write the objective function we aim to maximize as follows:</p>
<p><span class="math display">\[\begin{align*}
  W(p) &amp; = \underbrace{pF_{\epsilon,1}(\alpha_1 + \beta_1p)+(1-p)F_{\epsilon,0}(\alpha_0 + \beta_0p)}_{A(p)}\\
  &amp; \phantom{=}+\underbrace{(1-p)F_{\epsilon,1}(\alpha_1 + \beta_1(1-p))+pF_{\epsilon,0}(\alpha_0 + \beta_0(1-p))}_{A(1-p)}
\end{align*}\]</span></p>
<p>The <span class="math inline">\(A\)</span> function measures how much the probability of observing the favorable outcome <span class="math inline">\(Y_i=1\)</span> in a given cluster increases with the proportion of treated individuals in the cluster, <span class="math inline">\(p\)</span>.
It turns out that the properties of the <span class="math inline">\(A\)</span> function are key to determine the optimal allocation of treatment effort across nodes in the general case with more than two nodes.
For now, in the two-node case and under substantial simplifications, we have the following result:</p>
<div class="theorem">
<p><span id="thm:SimpleAlloc" class="theorem"><strong>Theorem 11.3  (Optimal allocation of treatment effort with two nodes) </strong></span>Under Assumptions <a href="Diffusion.html#hyp:PropSpecifyExpMap">11.2</a>, <a href="Diffusion.html#hyp:CoarseNetwork">11.3</a> and <a href="Diffusion.html#hyp:SimpleAllocHyp">11.4</a>, we have three possible cases for the optimal allocation of treatment effort:</p>
<ul>
<li>When amplification effects dominate (<span class="math inline">\(\beta_1&gt;\beta_0\)</span>): either <span class="math inline">\(p^*=1\)</span> or <span class="math inline">\(p^*=0\)</span></li>
<li>When contagion effects dominate (<span class="math inline">\(\beta_0&gt;\beta_1\)</span>): <span class="math inline">\(p^*=\frac{1}{2}\)</span></li>
<li>When amplification and contagion effects are of the same size (<span class="math inline">\(\beta_0=\beta_1\)</span>): <span class="math inline">\(p^*=\left[0,1\right]\)</span>.</li>
</ul>
</div>
<div class="proof">
<p><span id="unlabeled-div-261" class="proof"><em>Proof</em>. </span>Under Assumption <a href="Diffusion.html#hyp:SimpleAllocHyp">11.4</a>, we have:</p>
<p><span class="math display">\[\begin{align*}
  W(p) &amp; = p(\alpha_1 + \beta_1p)+(1-p)(\alpha_0 + \beta_0p)+(1-p)(\alpha_1 + \beta_1(1-p))+p(\alpha_0 + \beta_0(1-p))\\
      &amp; = \alpha_0+\alpha_1+\beta_1+2(\beta_0-\beta_1)p(1-p),
\end{align*}\]</span></p>
<p>where the second line follows after some algebra.
The problem <span class="math inline">\(\max_{p\in\left[0,1\right]}W(p)\)</span> has the folowing first order condition: <span class="math inline">\(W&#39;(p)=2(\beta_0-\beta_1)(1-2p)=0\)</span> and the following second order condition: <span class="math inline">\(W&#39;&#39;(p)=-4(\beta_0-\beta_1)\)</span>.
When <span class="math inline">\(\beta_0&gt;\beta_1\)</span>, <span class="math inline">\(W&#39;&#39;(p)&lt;0\)</span>, and the interior solution <span class="math inline">\(p^*=\frac{1}{2}\)</span> maximizes <span class="math inline">\(W\)</span>.
When <span class="math inline">\(\beta_0&lt;\beta_1\)</span>, <span class="math inline">\(W&#39;&#39;(p)&gt;0\)</span>, and the interior solution <span class="math inline">\(p^*=\frac{1}{2}\)</span> minimizes <span class="math inline">\(W\)</span>.
In that case, the optimal solution is at a corner, either at <span class="math inline">\(p^*=1\)</span> or at <span class="math inline">\(p^*=0\)</span>.
Since <span class="math inline">\(W(1)=W(0)\)</span>, they are both maxima.
When <span class="math inline">\(\beta_0=\beta_1\)</span>, <span class="math inline">\(W\)</span> is constant and any value in <span class="math inline">\(\left[0,1\right]\)</span> maximizes <span class="math inline">\(W\)</span>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-262" class="remark"><em>Remark</em>. </span>Theorem <a href="Diffusion.html#thm:SimpleAlloc">11.3</a> shows that when amplification effects dominate, it is optimal to focus all treatment effort on one of the two nodes (for example the first, but they are interchangeable).
This is because returns are increasing in this case: the <span class="math inline">\(A\)</span> function is convex, with more people responding to the treatment as more of them receive the treatment.
When contagion effects dominate, it is optimal to treat both nodes, with half of the observations receiving the treatment.
This is because in that case, the <span class="math inline">\(A\)</span> function is concave, and the marginal returns are decreasing when we treat more people.
When both contagion and amplification effects are equal, there is no optimum, or, equivalently, any allocation <span class="math inline">\(p\)</span> will yield the same result.</p>
</div>
</div>
<div id="a-general-model" class="section level4 hasAnchor" number="11.2.1.2">
<h4><span class="header-section-number">11.2.1.2</span> A general model<a href="Diffusion.html#a-general-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>One open question is whether we can generalize the result in Theorem <a href="Diffusion.html#thm:SimpleAlloc">11.3</a> to a much more general setting with several nodes and more general functional forms.
It is actually the case.
Let us now formulate a more general setting:</p>
<div class="hypothesis">
<p><span id="hyp:SymAllocHyp" class="hypothesis"><strong>Hypothesis 11.5  (Symmetric Allocation Problem) </strong></span>We assume that the allocation problem is characterized as follows:</p>
<ul>
<li><span class="math inline">\(K\)</span> nodes indexed from <span class="math inline">\(1\)</span> to <span class="math inline">\(K\)</span>, and each node has size <span class="math inline">\(n_k\)</span>.</li>
<li>At each node, we can choose to treat <span class="math inline">\(r_k\)</span> individuals.</li>
<li>The total number of individuals on the network is <span class="math inline">\(N=\sum_{k=1}^Kn_k\)</span>.</li>
<li>The total number of treated individuals is <span class="math inline">\(R=\sum_{k=1}^Kr_k\)</span>.</li>
<li>We cannot treat more than <span class="math inline">\(\bar{R}\)</span> individuals.</li>
<li>We cannot treat everyone: <span class="math inline">\(\bar{R}&lt;N\)</span>.</li>
<li>The expected outcome at each node (or response function) is only a function of <span class="math inline">\(p\)</span>, that we denote <span class="math inline">\(A(p)\)</span>, with <span class="math inline">\(A&#39;&gt;0\)</span>.</li>
</ul>
</div>
<p><strong><span class="math inline">\(A\)</span> is two things at the same time: connection matrix and response function</strong></p>
<div class="remark">
<p><span id="unlabeled-div-263" class="remark"><em>Remark</em>. </span>Assumption <a href="Diffusion.html#hyp:SymAllocHyp">11.5</a> is mainly restrictive in making the problem symmetric: all nodes are treated in the same way.
The only thing that distinguishes nodes is their respective size.
Apart from that, they all respond in the same (average) way to the treatment.
We do not try to distinguish between nodes based on observed characteristics of the nodes.
We also do not try to vary the identity of treated units based on their observed characteristics.
Another restriction is that <span class="math inline">\(A&#39;&gt;0\)</span>: we only consider treatments for which the response is always strictly increasing in <span class="math inline">\(p\)</span> (and not weakly).</p>
</div>
<p>Under Assumptions <a href="Diffusion.html#hyp:PropSpecifyExpMap">11.2</a>, <a href="Diffusion.html#hyp:CoarseNetwork">11.3</a> and <a href="Diffusion.html#hyp:SymAllocHyp">11.5</a>, we can cast our optimization problem as follows:</p>
<p><span class="math display">\[\begin{align*}
  \max_{\left\{r_k\right\}_{k=1}^K} &amp; \sum_{k=1}^K n_kA(\frac{r_k}{n_k})\label{eqn:MainProbMax}\\
   &amp; \text{under the constraints} \nonumber\\
   R  &amp; =\sum_{k=1}^Kr_k \leq \bar{R} \label{eqn:MainProbR}\\
   r_k &amp; \leq n_k\text{, }\forall k\label{eqn:MainProbn}\\
   r_k &amp; \geq 0\text{, }\forall k.\label{eqn:MainProbr}
\end{align*}\]</span></p>
<p>In my work, I have been able to solve this problem for a smooth response function <span class="math inline">\(A\)</span>, in the following sense:</p>
<div class="hypothesis">
<p><span id="hyp:SmoothResponseHyp" class="hypothesis"><strong>Hypothesis 11.6  (Monotone Response Function) </strong></span>We assume that the reponse function <span class="math inline">\(A\)</span> has constant second derivative on its full support: either <span class="math inline">\(A&#39;&#39;(p)&gt;0\)</span> <span class="math inline">\(\forall p\in\left[0,1\right]\)</span> or <span class="math inline">\(A&#39;&#39;(p)&lt;0\)</span> <span class="math inline">\(\forall p\in\left[0,1\right]\)</span>.</p>
</div>
<p>We can indeed prove the following result:</p>
<div class="theorem">
<p><span id="thm:SmoothSymAlloc" class="theorem"><strong>Theorem 11.4  (Optimal allocation under monotone response with $K$ symmetric nodes) </strong></span>Under Assumptions <a href="Diffusion.html#hyp:PropSpecifyExpMap">11.2</a>, <a href="Diffusion.html#hyp:CoarseNetwork">11.3</a>, <a href="Diffusion.html#hyp:SymAllocHyp">11.5</a> and <a href="Diffusion.html#hyp:SmoothResponseHyp">11.6</a>, the optimal allocation of treatment across nodes is as follows:</p>
<p><span class="math display">\[\begin{align*}
  \frac{r^*_k}{n_k} &amp; =
\begin{cases}
\frac{\bar{R}}{N}\text{, }\forall k &amp; \text{ if }A&#39;&#39;&lt;0\\
\begin{cases}
  0 &amp; \text{ for a set of nodes } \mathcal{J} \text{ such that } \sum_{j\in\mathcal{J}}n_j=N-\bar{R},\\
  1 &amp; \text{ for a set of nodes } \mathcal{L} \text{ such that } \sum_{l\in\mathcal{L}}n_j=\bar{R},
  \end{cases}
&amp;  \text{ if } A&#39;&#39;&gt;0.\\
  \end{cases}
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-264" class="proof"><em>Proof</em>. </span>See Section <a href="proofs.html#proofSmoothSymAlloc">A.6.1</a>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-265" class="remark"><em>Remark</em>. </span>Theorem <a href="Diffusion.html#thm:SmoothSymAlloc">11.4</a> shows that the very simple intuition that we got in the two nodes problem transports well to more complex settings.
The optimal allocation depends on the sign of the second derivative.
When returns are decreasing, we treat each node symmetrically with the same share <span class="math inline">\(p^*=\frac{\bar{R}}{N}\)</span> of the treatment effort.
When returns are increasing, we treat a share <span class="math inline">\(\frac{\bar{R}}{N}\)</span> of the nodes with <span class="math inline">\(p^*=1\)</span> and a share <span class="math inline">\(1-\frac{\bar{R}}{N}\)</span> with <span class="math inline">\(p^*=0\)</span>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-266" class="remark"><em>Remark</em>. </span>There are several open questions on this research front.
To list but a few:</p>
<ul>
<li>Can we relax Assumption <a href="Diffusion.html#hyp:SmoothResponseHyp">11.6</a>?
For example, we know that <span class="math inline">\(A&#39;&#39;\)</span> has not constant sign when the error terms are normal in the model with two nodes, but we still have an optimal solution that has the same shape.</li>
<li>Can we relax Assumption <a href="Diffusion.html#hyp:SymAllocHyp">11.5</a>?<br />
Especially, can we allow for responses that vary as a function of node characteristics and can we allow for treatment allocation based on unit characteristics?</li>
</ul>
</div>
</div>
<div id="using-two-step-clustered-randomized-controlled-trials-to-find-the-optimal-treatment-allocation" class="section level4 hasAnchor" number="11.2.1.3">
<h4><span class="header-section-number">11.2.1.3</span> Using two-step clustered randomized controlled trials to find the optimal treatment allocation<a href="Diffusion.html#using-two-step-clustered-randomized-controlled-trials-to-find-the-optimal-treatment-allocation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In this section, we are going to see that conducting a two-step clustered randomized controlled trial is going to enable us to identify the optimal treatment allocation under Assumptions <a href="Diffusion.html#hyp:PropSpecifyExpMap">11.2</a>, <a href="Diffusion.html#hyp:CoarseNetwork">11.3</a> and <a href="Diffusion.html#hyp:SimpleAllocHyp">11.4</a>.
A two-step clustered randomized controlled trial works as follows:</p>
<ul>
<li>In a first step, we randomly select three sets of nodes, <span class="math inline">\(ST\)</span> and <span class="math inline">\(PT\)</span> and <span class="math inline">\(SC\)</span>, with <span class="math inline">\(K_{ST}+K_{PT}+K_{SC}=\tilde{K}\)</span> and <span class="math inline">\(\tilde{K}\leq K\)</span>.
When <span class="math inline">\(\tilde{K}&lt; K\)</span>, <span class="math inline">\(\tilde{K}\)</span> is a random subset of the <span class="math inline">\(K\)</span> nodes.
<ul>
<li>Nodes that belong to <span class="math inline">\(ST\)</span>, the set of nodes of size <span class="math inline">\(K_{ST}\)</span>, are called <strong>Super Treated</strong> nodes.
The proportion of treated units is <span class="math inline">\(p^R_c=1\)</span>, <span class="math inline">\(\forall c \in ST\)</span>.</li>
<li>Nodes that belong to <span class="math inline">\(PT\)</span>, the set of nodes of size <span class="math inline">\(K_{ST}\)</span>, are called <strong>Partially Treated</strong> nodes.
The proportion of treated units is <span class="math inline">\(p^R_c=\frac{\bar{R}}{N}\equiv p^*\)</span>, <span class="math inline">\(\forall c \in PT\)</span>.</li>
<li>Nodes that belong to <span class="math inline">\(SC\)</span>, the set of nodes of size <span class="math inline">\(K_{SC}\)</span>, are called <strong>Super Control</strong> nodes.
The proportion of treated units is <span class="math inline">\(p^R_c=0\)</span>, <span class="math inline">\(\forall c \in SC\)</span>.</li>
</ul></li>
<li>In a second step, we randomly select <span class="math inline">\(N^1_c=\frac{\bar{R}}{N}N_c\)</span> units to be treated (with <span class="math inline">\(R_i=1\)</span>) and <span class="math inline">\(N^0_c=N_c-N^1_c\)</span> to be in the control group (<span class="math inline">\(R_i=0\)</span>), <span class="math inline">\(\forall c \in PT\)</span>, with <span class="math inline">\(N_c\)</span> the number of units in node <span class="math inline">\(c\)</span>.</li>
</ul>
<p>When implementing the treatment, all units in <span class="math inline">\(ST\)</span> are treated, only <span class="math inline">\(N^1_c\)</span> units are treated in <span class="math inline">\(PT\)</span> and no unit is treated in <span class="math inline">\(SC\)</span>.</p>
<div class="remark">
<p><span id="unlabeled-div-267" class="remark"><em>Remark</em>. </span>Note that rigorously, we should have <span class="math inline">\(N_c^1=\lfloor\frac{\bar{R}}{N}N_c\rfloor\)</span>, but we disregard the complexities brought about by the fact that the number of units has to be an integer.</p>
</div>
<p>The one thing we need to identify now in order to apply Theorem <a href="Diffusion.html#thm:SmoothSymAlloc">11.4</a> is the sign of the second derivative of the <span class="math inline">\(A\)</span> function.
We are going to show that the sign of <span class="math inline">\(A&#39;&#39;\)</span> can be identified in a two-step clustered randomized controlled trial.
Before that, we are going to encode the validity of the two-step clustered randomized controlled trial:</p>
<div class="hypothesis">
<p><span id="hyp:independence2StepCluster" class="hypothesis"><strong>Hypothesis 11.7  (Independence in a two-step clustered design) </strong></span>We assume that the allocation of the proportion of neighbors treated and of the individual treatment level are independent of potential outcomes:</p>
<p><span class="math display">\[\begin{align*}
  (R_i,p^R_c)\Ind\left(\left\{Y_i^{0,p},Y_i^{1,p}\right\}_{p\in\left[0,1\right]}\right).
\end{align*}\]</span></p>
</div>
<p>We also assume that the randomized allocation does not interfere with how units respond to the treatment:</p>
<div class="hypothesis">
<p><span id="hyp:TwoStepClusterValidity" class="hypothesis"><strong>Hypothesis 11.8  (Validity of the 2-step clustered design) </strong></span>We assume that the randomized allocation of the program does not interfere with how potential outcomes are generated:</p>
<p><span class="math display">\[\begin{align*}
Y_i &amp; =
  \begin{cases}
    Y_i^{1,p} &amp; \text{ if } R_i=1 \text{ and } p^R_c=p\\
    Y_i^{0,p} &amp; \text{ if } R_i=0 \text{ and } p^R_c=p      
  \end{cases}
\end{align*}\]</span></p>
<p>with <span class="math inline">\(Y_i^{1,p}\)</span> and <span class="math inline">\(Y_i^{0,p}\)</span> the same potential outcomes as defined with a routine allocation of the treatment.</p>
</div>
<p>We are now equipped to prove the identification of <span class="math inline">\(A&#39;&#39;\)</span>:</p>
<div class="theorem">
<p><span id="thm:IdentSmoothSymAlloc" class="theorem"><strong>Theorem 11.5  (Identification of $A''$ in a 2-step clustered randomized controlled trial) </strong></span>Under Assumptions <a href="Diffusion.html#hyp:PropSpecifyExpMap">11.2</a>, <a href="Diffusion.html#hyp:CoarseNetwork">11.3</a>, <a href="Diffusion.html#hyp:SymAllocHyp">11.5</a>, <a href="Diffusion.html#hyp:SmoothResponseHyp">11.6</a>, <a href="Diffusion.html#hyp:independence2StepCluster">11.7</a>, and <a href="Diffusion.html#hyp:TwoStepClusterValidity">11.8</a>, the numerator of <span class="math inline">\(A&#39;&#39;\)</span> is identified by the following quantity:</p>
<p><span class="math display">\[\begin{align*}
  \text{sign}(A&#39;&#39;) &amp; = \text{sign}\left(\frac{\esp{Y_i|p^R_c=1}-\esp{Y_i|p^R_c=p^*}}{1-p^*}-\frac{\esp{Y_i|p^R_c=p^*}-\esp{Y_i|p^R_c=0}}{p^*}\right).
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-268" class="proof"><em>Proof</em>. </span>See Section <a href="proofs.html#proofIdentSmoothSymAlloc">A.6.2</a>.</p>
</div>
<p>One thing that is pretty amazing is that we can relate the sign of <span class="math inline">\(A&#39;&#39;\)</span> to the relative size of contagion and amplification effects:</p>
<div class="theorem">
<p><span id="thm:ContagionDiffusionSmoothSymAlloc" class="theorem"><strong>Theorem 11.6  (The sign of $A''$ depends on the relative size of contagion vs amplification effects) </strong></span>Under Assumptions <a href="Diffusion.html#hyp:PropSpecifyExpMap">11.2</a>, <a href="Diffusion.html#hyp:CoarseNetwork">11.3</a>, <a href="Diffusion.html#hyp:SymAllocHyp">11.5</a>, <a href="Diffusion.html#hyp:SmoothResponseHyp">11.6</a>, <a href="Diffusion.html#hyp:independence2StepCluster">11.7</a>, and <a href="Diffusion.html#hyp:TwoStepClusterValidity">11.8</a>, we have:</p>
<p><span class="math display">\[\begin{align*}
  \text{sign}(A&#39;&#39;) &amp; = \text{sign}\left(\frac{\esp{Y^{1,1}_i-Y^{1,p^*}}}{1-p^*} -\frac{\esp{Y^{0,p^*}_i-Y^{0,0}_i}}{p^*}\right),
\end{align*}\]</span>
where <span class="math inline">\(\esp{Y^{1,1}_i-Y^{1,p^*}}\)</span> measures the strength of amplification effects and <span class="math inline">\(\esp{Y^{0,p^*}_i-Y^{0,0}_i}\)</span> measures the strength of contagion effects.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-269" class="proof"><em>Proof</em>. </span>See Section <a href="proofs.html#proofContagionDiffusionSmoothSymAlloc">A.6.3</a>.</p>
</div>
<p>Theorem <a href="Diffusion.html#thm:ContagionDiffusionSmoothSymAlloc">11.6</a> suggests an alternative identification strategy for the sign of <span class="math inline">\(A&#39;&#39;\)</span>:</p>
<div class="theorem">
<p><span id="thm:IdentContagionDiffusionSmoothSymAlloc" class="theorem"><strong>Theorem 11.7  (Identifying the sign of $A''$ from the relative size of contagion and amplification effects) </strong></span>Under Assumptions <a href="Diffusion.html#hyp:PropSpecifyExpMap">11.2</a>, <a href="Diffusion.html#hyp:CoarseNetwork">11.3</a>, <a href="Diffusion.html#hyp:SymAllocHyp">11.5</a>, <a href="Diffusion.html#hyp:SmoothResponseHyp">11.6</a>, <a href="Diffusion.html#hyp:independence2StepCluster">11.7</a>, and <a href="Diffusion.html#hyp:TwoStepClusterValidity">11.8</a>, we have:</p>
<p><span class="math display">\[\begin{align*}
    \text{sign}(A&#39;&#39;) &amp; = \text{sign}\left(\frac{\esp{Y_i|R_i=1,p^R_c=1}-\esp{Y_i|R_i=1,p^R_c=p^*}}{1-p^*}\right.\\
                      &amp; \phantom{=\text{sign}\left(\right.}\left.-\frac{\esp{Y_i|R_i=0,p^R_c=p^*}-\esp{Y_i|R_i=0,p^R_c=0}}{p^*}\right)
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-270" class="proof"><em>Proof</em>. </span>The proof is immediate using Theorem <a href="Diffusion.html#thm:ContagionDiffusionSmoothSymAlloc">11.6</a> and Assumptions <a href="Diffusion.html#hyp:independence2StepCluster">11.7</a> and <a href="Diffusion.html#hyp:TwoStepClusterValidity">11.8</a>.</p>
</div>
<p>Thanks to Theorems <a href="Diffusion.html#thm:IdentSmoothSymAlloc">11.5</a> and <a href="Diffusion.html#thm:IdentContagionDiffusionSmoothSymAlloc">11.7</a>, we therefore have two ways to estimate the sign of <span class="math inline">\(A&#39;&#39;\)</span>: either by comparing the overall changes in expected outcomes when moving from <span class="math inline">\(0\)</span> to <span class="math inline">\(p^*\)</span> and from <span class="math inline">\(p^*\)</span> to <span class="math inline">\(1\)</span>, or by comparing the relative size of amplification and contagion effects.
As a consequence, we can form two with/without estimators of <span class="math inline">\(A&#39;&#39;\)</span>:</p>
<p><span class="math display">\[\begin{align*}
  \hat{A}&#39;&#39;_{All}(\frac{1}{2}) &amp; = \frac{\frac{\sum_{i\in\mathcal{I}_{ST}}Y_i}{N_{ST}}-\frac{\sum_{i\in\mathcal{I}_{PT}}Y_i}{N_{PT}}}{1-p^*}-
                         \frac{\frac{\sum_{i\in\mathcal{I}_{SP}}Y_i}{N_{SP}}-\frac{\sum_{i\in\mathcal{I}_{SC}}Y_i}{N_{SC}}}{p^*}\\
  \hat{A}&#39;&#39;_{Diff}(\frac{1}{2}) &amp; = \frac{\frac{\sum_{i\in\mathcal{I}_{ST}}Y_i}{N_{ST}}-\frac{\sum_{i\in\mathcal{I}^1_{PT}}Y_i}{N^1_{PT}}}{1-p^*}-
                         \frac{\frac{\sum_{i\in\mathcal{I}^0_{SP}}Y_i}{N^0_{SP}}-\frac{\sum_{i\in\mathcal{I}_{SC}}Y_i}{N_{SC}}}{p^*},
\end{align*}\]</span></p>
<p>with <span class="math inline">\(\mathcal{I}_{T}\)</span>, <span class="math inline">\(T\in\left\{ST,SC,PT\right\}\)</span>, the set of units <span class="math inline">\(i\)</span> that belong to a cluster of type <span class="math inline">\(T\)</span>, <span class="math inline">\(\mathcal{I}^d_{PT}\)</span>, <span class="math inline">\(d\in\left\{0,1\right\}\)</span>, the set of units that belong to a cluster of type <span class="math inline">\(PT\)</span> and have <span class="math inline">\(R_i=d\)</span>, <span class="math inline">\(N_{T}\)</span>, <span class="math inline">\(T\in\left\{ST,SC,PT\right\}\)</span>, the number of units <span class="math inline">\(i\)</span> belonging to clusters of type <span class="math inline">\(T\)</span>, and <span class="math inline">\(\mathcal{N}^d_{PT}\)</span>, <span class="math inline">\(d\in\left\{0,1\right\}\)</span>, the number of units that belong to clusters of type <span class="math inline">\(PT\)</span> and have <span class="math inline">\(R_i=d\)</span>.
Following usual arguments, these estimators are both unbiased and consistent (as the number of clusters goes to infinity) for <span class="math inline">\(A&#39;&#39;(\frac{1}{2})\)</span>.
Their components can both be estimated separately by using OLS with a linear model on separate subsamples.
The covariance of each separate with/without comparison can be estimated by estimating both components jointly, for example by estimating the following model by OLS:</p>
<p><span class="math display">\[\begin{align*}
  Y_i &amp; = \alpha^{All} + \beta^{All}_{PT}\uns{i\in\mathcal{I}_{PT}} + \beta^{All}_{ST}\uns{i\in\mathcal{I}_{ST}} + \epsilon_i^{All}\\
  Y_i &amp; = \alpha^{Diff} + \alpha^{Diff}_{1}\uns{R_i=1} + \beta^{Diff}_{0}\uns{i\in\mathcal{I}^0_{PT}}+ \beta^{Diff}_{1}\uns{i\in\mathcal{I}_{ST}} + \epsilon_i^{All}.
\end{align*}\]</span></p>
<p>With these parameter estimates, we have:</p>
<p><span class="math display">\[\begin{align*}
  \hat{A}&#39;&#39;_{All}(\frac{1}{2}) &amp; = \frac{\hat\beta^{All}_{ST}}{1-p^*}-\frac{\hat\beta^{All}_{SP}}{p^*}\\
  \hat{A}&#39;&#39;_{Diff}(\frac{1}{2}) &amp; = \frac{\hat\beta^{Diff}_{1}}{1-p^*}-\frac{\hat\beta^{Diff}_{0}}{p^*}.
\end{align*}\]</span></p>
<p>To estimate the precision of each of the parameters, one has to use standard errors clustered at the cluster level.
To obtain the precision of <span class="math inline">\(\hat{A}&#39;&#39;(\frac{1}{2})\)</span>, one can simply use the Delta Method.</p>
<div class="remark">
<p><span id="unlabeled-div-271" class="remark"><em>Remark</em>. </span>Note that in practice, the actual proportion of treated in each cluster of type <span class="math inline">\(PT\)</span> is going to differ from <span class="math inline">\(p^*\)</span>.
Does this affect consistency and unbiasedness of both estimators?
Could we estimate <span class="math inline">\(\hat p^*\)</span> and try to use it to get access to a wider share of the <span class="math inline">\(A\)</span> function, or at least to an average effect?
See Davide’s discussion of that issue.</p>
</div>
</div>
</div>
<div id="identifying-optimal-treatment-levels" class="section level3 hasAnchor" number="11.2.2">
<h3><span class="header-section-number">11.2.2</span> Identifying optimal treatment levels<a href="Diffusion.html#identifying-optimal-treatment-levels" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the previous section, we discussed ways of identifying diffusion effects, and we focused on the task of finding the optimal treatment allocation when total treatment capacity was fixed to a limited number of treatments.
In that scenario, what turned out to be super important was the shape of the returns to treatment effort (convex or concave), and it turned out to be related to whether contagion or amplification effects dominated.
Though this scenario of constant treatment effort sometimes happens in real life, in other situations, policymakers might want to decide whether to increase or decrease their treatment effort, and to find the optimal treatment level, taking into account diffusion effects.
This is the goal of this section, which is fully based on Davide <a href="http://arxiv.org/abs/2011.08174">Viviano (2023)</a>’s recent working paper on the topic.</p>
<div id="setup-and-assumptions" class="section level4 hasAnchor" number="11.2.2.1">
<h4><span class="header-section-number">11.2.2.1</span> Setup and assumptions<a href="Diffusion.html#setup-and-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Davide considers a setting with <span class="math inline">\(K\)</span> clusters of equal size <span class="math inline">\(N\)</span>.
Researchers sample a proportion <span class="math inline">\(\lambda\in\left]0,1\right]\)</span> of the <span class="math inline">\(N\)</span> units in each cluster at each period <span class="math inline">\(t\)</span> and they have access to the following information for the sampled observations in each cluster: <span class="math inline">\(\left(Y^{(k)}_{i,t},X^{(k)}_{i},D^{(k)}_{i,t}\right)_{i=1}^n\)</span> where <span class="math inline">\(n=\lambda N\)</span> and <span class="math inline">\(X^{(k)}_{i}\)</span> are baseline characteristics.
There are <span class="math inline">\(T\)</span> periods.
Despite the data being allowed to be a panel or a repeated cross section, we denote observations as if there was repeated sampling.
Potential outcomes are denoted <span class="math inline">\(Y^{(k)}_{i,t}(\mathbf{D}_1^{(k)},\dots,\mathbf{D}_t^{(k)})\)</span>, where <span class="math inline">\(\mathbf{D}_s^{(k)}\in\left\{0,1\right\}^N\)</span>, and <span class="math inline">\(s\leq t\)</span>.
We denote <span class="math inline">\(Y^{(k)}(.)\)</span> and <span class="math inline">\(X^{(k)}\)</span> the vectors of potential outcomes and covariates in cluster <span class="math inline">\(k\)</span>.</p>
<p>The key policy parameter that we are going to be after is a treatment rule, <span class="math inline">\(\pi(.;\beta):\mathcal{X}\leftrightarrow\left[0,1\right]\)</span>, indexed by a (possibly vector valued) parameter <span class="math inline">\(\beta\)</span> which lies in a compact set.
The treatment rules selects a probability of allocating each agent with characteristics <span class="math inline">\(x\)</span> at date <span class="math inline">\(t\)</span> in cluster <span class="math inline">\(k\)</span> to the treatment.
We would like to choose <span class="math inline">\(\pi\)</span> so that we maximize an objective function, for example total program returns net of program implementation costs.</p>
<p>In order to determine this optimal function, we are going to run two-step clustered experiments.
These experiments are as follows:</p>
<div class="hypothesis">
<p><span id="hyp:TreatAssignDavide" class="hypothesis"><strong>Hypothesis 11.9  (Treatment Assignement in the experiment) </strong></span>For <span class="math inline">\(\beta_{k,t}\Ind\left(X^{(k)},Y^{(k)}(.)\right)\)</span>,</p>
<p><span class="math display">\[\begin{align*}
    D^{(k)}_{i,t}|X^{(k)},Y^{(k)}(.),\beta_{k,t}\sim_{i.n.i.d.}\mathcal{B}(\pi(X^{(k)}_{i};\beta_{k,t})).
  \end{align*}\]</span></p>
</div>
<p>Assumption <a href="Diffusion.html#hyp:TreatAssignDavide">11.9</a> implies that the allocation of treatment follows a Bernoulli distribution indexed by parameters <span class="math inline">\(\beta_{k,t}\)</span>, and can be different for individuals with different baseline characteristics.</p>
<div class="example">
<p><span id="exm:unnamed-chunk-443" class="example"><strong>Example 11.1  </strong></span>Examples of experimental allocation rules are the equal probability rule: <span class="math inline">\(\pi(.;\beta)=\beta\in\left[0,1\right]\)</span> or targeted treatments <span class="math inline">\(\pi(x;\beta)=\beta_x\)</span>.
The treatment rules can also be made conditional on cluster characteristics.</p>
</div>
<p>Davide now needs another assumption about the data generating process:</p>
<div class="hypothesis">
<p><span id="hyp:DGPDavide" class="hypothesis"><strong>Hypothesis 11.10  (Data generating process) </strong></span>For any <span class="math inline">\((i,t,k)\)</span>, we assume that:</p>
</div>
<p>Assumption <a href="Diffusion.html#hyp:DGPDavide">11.10</a> says that there are no carryover effects of the treatment beyond the period in which it is assigned (<a href="#it:noCarryOver"><strong>??</strong></a>), that there are no interactions between the treatment and time and cluster fixed effects (<a href="#it:FunctForm"><strong>??</strong></a>), and finally that outcomes depend on at most <span class="math inline">\(\gamma_N\)</span> other outcomes in the same cluster.</p>
<p>We are now equipped to define welfare as a function of the parameters of the allocation rule:</p>
<div class="definition">
<p><span id="def:WelfareDavide" class="definition"><strong>Definition 11.1  </strong></span>For treatments as in Assumption <a href="Diffusion.html#hyp:TreatAssignDavide">11.9</a>, and under the assumptions on the d.g.p. in Assumption <a href="Diffusion.html#hyp:DGPDavide">11.10</a>, we can define welfare as <span class="math inline">\(W(\beta)=\int y(x,\beta)dF_X(x)\)</span>, with <span class="math inline">\(y(x,\beta)=\pi(x;\beta)m(1,x,\beta)+(1-\pi(x;\beta))m(0,x,\beta)\)</span>,</p>
</div>
<p>with <span class="math inline">\(y(x,\beta)\)</span> the outcome net of costs.
Equipped with this definition, and assuming all functions are differentiable, we can define the direct effect of the treatment (<span class="math inline">\(\Delta(x,\beta)\)</span>), the marginal spillover effect (<span class="math inline">\(S(d,x,\beta)\)</span>), the marginal policy effect (<span class="math inline">\(M(\beta)\)</span>) and the welfare optimizing policy (<span class="math inline">\(\beta^*\)</span>) as follows:</p>
<p><span class="math display">\[\begin{align*}
  \Delta(x,\beta) &amp; = m(1,x,\beta)-m(0,x,\beta)\\
  S(d,x,\beta) &amp; = \partder{m(d,x,\beta)}{\beta}\\
  M(\beta) &amp; = \partder{W(\beta)}{\beta}\\
          &amp; = \int\left[S(0,x,\beta)+\pi(x;\beta)(S(1,x,\beta)-S(0,x,\beta))\right.\\
          &amp; \phantom{=\int\left[\right.}\left.+\partder{\pi(x,\beta)}{\beta} \Delta(x,\beta)\right]dF_X(x)\\
  \beta^* &amp; = \arg\sup_{\beta}W(\beta).
\end{align*}\]</span></p>
<div class="example">
<p><span id="exm:unnamed-chunk-444" class="example"><strong>Example 11.2  </strong></span>A first example that Davide gives is the case of positive externalities with decreasing returns from neighbours’ treatments.
We pose <span class="math inline">\(D^{(k)}_{i,t}\sim_{i.i.d.}\mathcal{B}(\beta)\)</span>, and <span class="math inline">\(\mathcal{N}_i\)</span> is the set of neighbours of individual <span class="math inline">\(i\)</span>.
We let</p>
<p><span class="math display">\[\begin{align*}
  Y^{(k)}_{i,t} &amp; = \alpha_t + D^{(k)}_{i,t}\phi_1 + \frac{\sum_{j\in\mathcal{N}_i}D^{(k)}_{j,t}}{\left|\mathcal{N}_i\right|}\phi_2
                    -\left(\frac{\sum_{j\in\mathcal{N}_i}D^{(k)}_{j,t}}{\left|\mathcal{N}_i\right|}\right)^2\phi_3 + \nu_{i,t}
\end{align*}\]</span></p>
<p>In that case, assuming that <span class="math inline">\(\left|\mathcal{N}_i\right|\sim\mathcal{D}_N\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
  \espsub{Y^{(k)}_{i,t}|\alpha_t,D^{(k)}_{i,t}=d}{\beta}
                  &amp; = \alpha_t + d\phi_1
                    + \espsub{\frac{\sum_{j\in\mathcal{N}_i}D^{(k)}_{j,t}}{\left|\mathcal{N}_i\right|}}{\beta}\phi_2
                    -\espsub{\left(\frac{\sum_{j\in\mathcal{N}_i}D^{(k)}_{j,t}}{\left|\mathcal{N}_i\right|}\right)^2}{\beta}\phi_3 \\
                  &amp; = \alpha_t + d\phi_1
                    + \esp{\espsub{\frac{\sum_{j\in\mathcal{N}_i}D^{(k)}_{j,t}}{n}}{\beta}|\left|\mathcal{N}_i\right|}\phi_2
                    -\esp{\espsub{\left(\frac{\sum_{j\in\mathcal{N}_i}D^{(k)}_{j,t}}{n}\right)^2}{\beta}|\left|\mathcal{N}_i\right|}\phi_3 \\
                  &amp; =  \alpha_t + d\phi_1
                    + \esp{\frac{\left|\mathcal{N}_i\right|\beta}{\left|\mathcal{N}_i\right|}|\left|\mathcal{N}_i\right|}\phi_2
            -\esp{\frac{\left|\mathcal{N}_i\right|\beta(1-\beta)+\left|\mathcal{N}_i\right|^2\beta^2}{\left|\mathcal{N}_i\right|^2}|\left|\mathcal{N}_i\right|}\phi_3 \\
                  &amp; =  \alpha_t + d\phi_1 + \beta\phi_2-\beta\phi_3\left(\beta+(1-\beta)\esp{\frac{1}{\left|\mathcal{N}_i\right|}}\right)
\end{align*}\]</span></p>
<p>As a consequence, we have:</p>
<p><span class="math display">\[\begin{align*}
  m(d,1,\beta) &amp; = d\phi_1 + \beta\phi_2-\beta\phi_3\left(\beta+(1-\beta)\esp{\frac{1}{\left|\mathcal{N}_i\right|}}\right)\\
  \Delta(x,\beta) &amp; = \phi_1\\
  S(d,x,\beta) &amp; = \phi_2-\phi_3(2\beta+(1-2\beta)\esp{\frac{1}{\left|\mathcal{N}_i\right|}})\\
  y(1,\beta) &amp; = \beta\phi_2-\beta\phi_3\left(\beta+(1-\beta)\esp{\frac{1}{\left|\mathcal{N}_i\right|}}\right)+\beta(\phi_1-c)\\
  M(\beta) &amp; = \phi_2-\phi_3(2\beta+(1-2\beta)\esp{\frac{1}{\left|\mathcal{N}_i\right|}})+\phi_1-c.
\end{align*}\]</span></p>
</div>
<div class="example">
<p><span id="exm:unnamed-chunk-445" class="example"><strong>Example 11.3  </strong></span>Let us now look at an example with negative externalties.
We similarly have <span class="math inline">\(D^{(k)}_{i,t}\sim_{i.i.d.}\mathcal{B}(\beta)\)</span>, but now outcomes are negatively affected by the proportion of treated, and all the more so if they are treated themselves:</p>
<p><span class="math display">\[\begin{align*}
  Y^{(k)}_{i,t} &amp; = \alpha_t + D^{(k)}_{i,t}\phi_1 - \frac{\sum_{j\in\mathcal{N}_i}D^{(k)}_{j,t}}{\left|\mathcal{N}_i\right|}\phi_2
                    -D^{(k)}_{i,t}\frac{\sum_{j\in\mathcal{N}_i}D^{(k)}_{j,t}}{\left|\mathcal{N}_i\right|}\phi_3 + \nu_{i,t}
\end{align*}\]</span></p>
<p>In that case, we have, after similar manipulation, <span class="math inline">\(y(1,\beta)=\beta(\phi_1-c-\phi_2-\phi_3\beta)\)</span>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-272" class="remark"><em>Remark</em>. </span>One open question is whether Assumption <a href="Diffusion.html#hyp:DGPDavide">11.10</a> is compatible with any real-looking network.
Davide formalizes a nice proposition that shows that indeed this assumption can be rationalized by an actual network formation model.
Let units be spaced on a latent space, and each unit can interact with at most the <span class="math inline">\(\sqrt{\gamma_N}\)</span> closest units.
Let <span class="math inline">\(\uns{i_k\leftrightarrow j_{k}}\)</span> denote whether or not <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> can be connected in the resulting the latent network.
Let <span class="math inline">\(\mathcal{I}_k\)</span> be the matrix of these potential connections in cluster <span class="math inline">\(k\)</span>.
Davide’s first assumption is:</p>
</div>
<div class="hypothesis">
<p><span id="hyp:iidConnect" class="hypothesis"><strong>Hypothesis 11.11  (Network) </strong></span>Actual connections are generated as follows:</p>
<p><span class="math display">\[\begin{align*}
    A_{i,j}^{(k)} &amp; = l(X^{(k)}_{i},X^{(k)}_{j},U^{(k)}_{i},U^{(k)}_{j})\uns{i_k\leftrightarrow j_{k}},
  \end{align*}\]</span></p>
<p>for some function <span class="math inline">\(l\)</span> and unobservables <span class="math inline">\(U^{(k)}_{i}\)</span> with <span class="math inline">\(\left(X^{(k)}_{i},U^{(k)}_{i}\right)|\mathcal{I}_k\sim F_{U|X}F_{X}\)</span> and with <span class="math inline">\(\sum_{j=1}^N\uns{i_k\leftrightarrow j_{k}}=\sqrt{\gamma_N}\)</span>.</p>
</div>
<p>The second assumption Davide makes is on how potential outcomes are generated:</p>
<div class="hypothesis">
<p><span id="hyp:PoNetwork" class="hypothesis"><strong>Hypothesis 11.12  (Potential outcomes) </strong></span>Potential outcomes are generated as follows:</p>
<p><span class="math display">\[\begin{align*}
    Y^{(k)}_{i,t}(\mathbf{D}_1^{(k)},\dots,\mathbf{D}_t^{(k)}) &amp; = r(D_{i,t}^{(k)},\mathbf{D}_{\mathcal{N}_i^{k},t}^{(k)},X^{(k)}_{i},X^{(k)}_{\mathcal{N}_i^{k},t},U^{(k)}_{i},U^{(k)}_{\mathcal{N}_i^{k},t},A^{(k)}_{i,.},|\mathcal{N}_i^{k}|,\nu^{(k)}_{i,t})+\tau_k+\alpha_t,
  \end{align*}\]</span></p>
<p>for some function <span class="math inline">\(r\)</span> which attains the same value for any permutations of the entries of <span class="math inline">\(A^{(k)}_{i,.}\)</span>, with <span class="math inline">\(A^{(k)}_{i,.}\)</span> the vector of connections of <span class="math inline">\(i\)</span> in <span class="math inline">\((k)\)</span>, and unobservables <span class="math inline">\(\nu^{(k)}_{i,t}|\left(X^{(k)}_{i},U^{(k)}_{i}\right)\sim F_{\nu}\)</span>, and where <span class="math inline">\(\mathcal{N}_i^{k}=\left\{j:A^{(k)}_{i,j}&gt;0\right\}\)</span>.</p>
</div>
<p>Davide can then prove that this setting implies Assumption <a href="Diffusion.html#hyp:DGPDavide">11.10</a>:</p>
<div class="proposition">
<p><span id="prp:DavideEquiv" class="proposition"><strong>Proposition 11.1  </strong></span>With a treatment assigned following Assumption <a href="Diffusion.html#hyp:TreatAssignDavide">11.9</a>, if Assumptions <a href="Diffusion.html#hyp:iidConnect">11.11</a> and <a href="Diffusion.html#hyp:PoNetwork">11.12</a> hold, then Assumption <a href="Diffusion.html#hyp:DGPDavide">11.10</a> holds.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-273" class="proof"><em>Proof</em>. </span>See <a href="http://arxiv.org/abs/2011.08174">Viviano (2023)</a>, Section B.1.2.</p>
</div>
</div>
<div id="identifying-and-estimating-the-marginal-policy-effect-with-a-one-wave-experiment" class="section level4 hasAnchor" number="11.2.2.2">
<h4><span class="header-section-number">11.2.2.2</span> Identifying and estimating the marginal policy effect with a one-wave experiment<a href="Diffusion.html#identifying-and-estimating-the-marginal-policy-effect-with-a-one-wave-experiment" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Davide proposes a one-wave experiment to get at the marginal policy effect.
Here is the algorithm he proposes, with <span class="math inline">\(p_1=1\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Organize clusters into <span class="math inline">\(G=\frac{K}{2}\)</span> pairs with consecutive indexes <span class="math inline">\(\left\{k,k+1\right\}\)</span></p></li>
<li><p>At <span class="math inline">\(t=0\)</span>, either nobody receives the treatment, or treatment is assigned using rule <span class="math inline">\(\pi(.;\beta)\)</span>.
Collect baseline outcomes and observe <span class="math inline">\((Y_{i,0}^{(h)},X_{i}^{(h)})_{i=1}^N\)</span>, for <span class="math inline">\(h\in\left\{1,\dots,K\right\}\)</span>.</p></li>
<li><p>At <span class="math inline">\(t=1\)</span>, start the experiment:</p></li>
</ol>
<ul>
<li><p>For each pair <span class="math inline">\(g=\left\{k,k+1\right\}\)</span>, randomize</p>
<p><span class="math display">\[\begin{align*}
    D_{i,1}^{(k)}|\beta,X_i^{(k)}=x &amp; \sim \begin{cases}
                                              \mathcal{B}(\pi(x,\beta+\eta_n\underline{e}_1)) &amp; \text{ if } h=k\\
                                              \mathcal{B}(\pi(x,\beta-\eta_n\underline{e}_1)) &amp; \text{ if } h=k+1
                                            \end{cases}
  \end{align*}\]</span>
with <span class="math inline">\(\bar{C}n^{-\frac{1}{2}}&lt;\eta_n&lt;\bar{C}n^{-\frac{1}{4}}\)</span>, and <span class="math inline">\(\underline{e}_j=\left[0,\dots,0,1,0,\dots,0\right]\)</span>, where <span class="math inline">\(\underline{e}_j\in\left\{0,1\right\}^p\)</span>, and <span class="math inline">\(\underline{e}_j[j]=1\)</span>.</p></li>
<li><p>For <span class="math inline">\(n\)</span> units in cluster <span class="math inline">\(h\)</span> observe <span class="math inline">\(Y_{i,1}^{(h)}\)</span></p></li>
<li><p>Estimate the marginal effect as follows:</p>
<p><span class="math display">\[\begin{align*}
  \bar{M}_n(\beta) &amp; = \frac{1}{G}\sum_{g=1}^G\widehat{M}_g(\beta) \\
  \widehat{M}_g(\beta) &amp; =
\frac{1}{2\eta_n}\left[\frac{1}{n}\sum_{i=1}^nY^{(k)}_{i,1}-\frac{1}{n}\sum_{i=1}^nY^{(k)}_{i,0}\right]
-\frac{1}{2\eta_n}\left[\frac{1}{n}\sum_{i=1}^nY^{(k+1)}_{i,1}-\frac{1}{n}\sum_{i=1}^nY^{(k+1)}_{i,0}\right]
\end{align*}\]</span></p></li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Construct the following test statistic</li>
</ol>
<p><span class="math display">\[\begin{align*}
    \mathcal{T}_n &amp; = \sqrt{G}\frac{\bar{M}_n(\beta)}{\sqrt{\frac{1}{G-1}\sum_{g=1}^G(\widehat{M}_g(\beta)-\bar{M}_n(\beta))^2}}
  \end{align*}\]</span>
to test whether the current allocation is optimal.
Indeed, if <span class="math inline">\(\beta^*=\arg\max_{\beta} W(\beta)\)</span> is an interior point, then <span class="math inline">\(W(\beta)=W(\beta^*)\Rightarrow M(\beta)[j]=0\)</span>, <span class="math inline">\(\forall j\in\left\{1,\dots,p_1\right\}\)</span>, with <span class="math inline">\(p_1\leq p\)</span>.</p>
<ol start="5" style="list-style-type: decimal">
<li>Constructs tests <span class="math inline">\(\uns{\left|\mathcal{T}_n\right|&gt; \text{cv}_{G}(\alpha)}\)</span>, with size <span class="math inline">\(\alpha\)</span> and critical values obtained by permuting the sign of the estimated marginal effects.</li>
</ol>
<div class="remark">
<p><span id="unlabeled-div-274" class="remark"><em>Remark</em>. </span>Davide’s approach runs a pairwise randomized controlled trial similar to the ones we studied in Section <a href="stratification.html#PairRCT">16.2</a>, but at the cluster level.
Each cluster within the pair is allocated a slightly different value of the allocation parameter, with a perturbation aound the current level of allocation (or a <span class="math inline">\(\beta\)</span> of interest).
Davide then proposes a DID estimator to get rid of the time and cluster fixed effects (mostly for precision, since they do not affect consistency, at least they do not seem to).
He estimates a test statistic for whether the average marginal effect is zero across clusters, which is a necessary condition for being at the optimum (and a sufficient one if we assume sufficiency).</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-275" class="remark"><em>Remark</em>. </span>Davide’s approach also recovers several other important treatment effects:</p>
</div>
<p><span class="math display">\[\begin{align*}
  \bar{W}_n(\beta) &amp; = \frac{1}{K}\sum_{k=1}^K\left(\frac{1}{n}\sum_{i=1}^nY^{(k)}_{i,1}-\frac{1}{n}\sum_{i=1}^nY^{(k)}_{i,0}\right) \\
  \bar{\Delta}_n(\beta) &amp; = \frac{1}{G}\sum_{g=1}^G\hat{\Delta}_g(\beta) \\
  \hat{\Delta}_{g}(\beta) &amp; = \frac{1}{2n}\sum_{h\in\left\{k,k+1\right\}}\sum_{i=1}^n
                              \left[\frac{D^{(h)}_{i,1}Y^{(h)}_{i,1}}{\pi(X_i^{h};\beta+\eta_n\nu_h\underline{e}_1)}
                                   -\frac{(1-D^{(h)}_{i,1})Y^{(h)}_{i,1}}{1-\pi(X_i^{h};\beta+\eta_n\nu_h\underline{e}_1)}\right]\\
  \bar{S}_n(1,\beta) &amp; = \frac{1}{G}\sum_{g=1}^G\hat{S}_{g}(1,\beta) \\
  \hat{S}_{g}(1,\beta) &amp; = \frac{1}{2n}\sum_{h\in\left\{k,k+1\right\}}\frac{\nu_h}{\eta_n}\sum_{i=1}^n
                              \left[\frac{D^{(h)}_{i,1}Y^{(h)}_{i,1}}{\pi(X_i^{h};\beta+\eta_n\nu_h\underline{e}_1)}
                                   -\frac{1}{n}\sum_{i=1}^nY^{(k)}_{i,0}\right]\\
  \bar{S}_n(0,\beta) &amp; = \frac{1}{G}\sum_{g=1}^G\hat{S}_{g}(0,\beta) \\
  \hat{S}_{g}(0,\beta) &amp; = \frac{1}{2n}\sum_{h\in\left\{k,k+1\right\}}\frac{\nu_h}{\eta_n}\sum_{i=1}^n
                              \left[\frac{(1-D^{(h)}_{i,1})Y^{(h)}_{i,1}}{1-\pi(X_i^{h};\beta+\eta_n\nu_h\underline{e}_1)}
                                   -\frac{1}{n}\sum_{i=1}^nY^{(k)}_{i,0}\right]\\
  \nu_h &amp; = \begin{cases} 1 &amp; \text{ if } h=k \\ -1 &amp; \text{ if } h=k+1 \end{cases} \\
\end{align*}\]</span></p>
<div class="remark">
<p><span id="unlabeled-div-276" class="remark"><em>Remark</em>. </span>Note that Davide’s approach is politically much more palatable than a 2-step clustered design with super controls and super treated clusters.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-277" class="remark"><em>Remark</em>. </span>At the same time, note that Davide’s approach just gives the direction to change <span class="math inline">\(\beta\)</span> but does not deliver an optimal <span class="math inline">\(\beta^*\)</span>, unelss we are already there.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-278" class="remark"><em>Remark</em>. </span>Davide also proves several theorems that ensure that the estimates above are consistent under some reasonable assumptions, and can be approximated by a normal.
The randomization tests also have correct coverage.</p>
</div>
</div>
<div id="identifying-and-estimating-the-optimal-treatment-allocation" class="section level4 hasAnchor" number="11.2.2.3">
<h4><span class="header-section-number">11.2.2.3</span> Identifying and estimating the optimal treatment allocation<a href="Diffusion.html#identifying-and-estimating-the-optimal-treatment-allocation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Davide also proposes an algorithm to sequentially converge to the optimal treatment level.
Here is Davide’s proposed algorithm for <span class="math inline">\(p_1=1\)</span>, with <span class="math inline">\(\beta\in\left[\underline{\beta},\overline{\beta}\right]\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Organize clusters into pairs <span class="math inline">\(\left\{k,k+1\right\}\)</span>, with <span class="math inline">\(k\in\left\{1,3,\dots,K-1\right\}\)</span>;</p></li>
<li><p>At <span class="math inline">\(t=0\)</span>, treatment is assigned using rule <span class="math inline">\(D_{i,0}^{(h)}|X_i^{(h)}=x\sim\mathcal{B}(\pi(x;\beta_0))\)</span>, <span class="math inline">\(\forall k\in\left\{1,\dots,K\right\}\)</span>.
Collect baseline outcomes and observe <span class="math inline">\((Y_{i,0}^{(h)},X_{i}^{(h)})_{i=1}^N\)</span>, for <span class="math inline">\(h\in\left\{1,\dots,K\right\}\)</span>.
Initialize <span class="math inline">\(\widehat{M}_{k,0}=0\)</span>, <span class="math inline">\(\tilde{\beta}_k^0=\beta_0\)</span>.</p></li>
<li><p>while <span class="math inline">\(1\leq t\leq T\)</span>, do:</p></li>
</ol>
<ul>
<li><p>Define</p>
<p><span class="math display">\[\begin{align*}
    \tilde{\beta}_{h}^{(t)} &amp; = \mathcal{P}_{\underline{\beta},\overline{\beta}-\eta_n}(\tilde{\beta}_k^0+\alpha_{h+2,t}\widehat{M}_{h+2,t-1})
  \end{align*}\]</span>
with the convention <span class="math inline">\(h+2=1\)</span> when <span class="math inline">\(h=K\)</span> and <span class="math inline">\(h=K+1\)</span>, <span class="math inline">\(\alpha_{k,t}\)</span> is the learning rate and <span class="math inline">\(\mathcal{P}_{a,b}(x)=\arg\min_{x&#39;\in\left[a,b\right]^p}||x-x&#39;||^2\)</span>):</p></li>
<li><p>Assign treatments as (for <span class="math inline">\(\bar{C}n^{-\frac{1}{2}}&lt;\eta_n&lt;\bar{C}n^{-\frac{1}{4}}\)</span>):</p>
<p><span class="math display">\[\begin{align*}
    D_{i,0}^{(h)}|X_i^{(h)}=x &amp; \sim\mathcal{B}(\pi(x;\beta_{h,t}))\\
    \beta_{h,t} &amp; = \begin{cases}
                      \tilde{\beta}_{h,t}+\eta_n &amp; \text{ if } h \text{ is odd}\\
                      \tilde{\beta}_{h,t}-\eta_n &amp; \text{ if } h \text{ is even}
                    \end{cases}
\end{align*}\]</span></p></li>
<li><p>For <span class="math inline">\(n\)</span> units in cluster <span class="math inline">\(h\)</span> observe <span class="math inline">\(Y_{i,t}^{(h)}\)</span></p></li>
<li><p>For each pair <span class="math inline">\(\left\{k,k+1\right\}\)</span>, estimate the marginal effect as follows:</p>
<p><span class="math display">\[\begin{align*}
  \widehat{M}_{k,t}=\widehat{M}_{k+1,t} &amp; =
\frac{1}{2\eta_n}\left[\frac{1}{n}\sum_{i=1}^nY^{(k)}_{i,1}-\frac{1}{n}\sum_{i=1}^nY^{(k)}_{i,0}\right]
-\frac{1}{2\eta_n}\left[\frac{1}{n}\sum_{i=1}^nY^{(k+1)}_{i,1}-\frac{1}{n}\sum_{i=1}^nY^{(k+1)}_{i,0}\right]
\end{align*}\]</span></p></li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li><p>End while.</p></li>
<li><p>Return <span class="math inline">\(\hat{\beta}^*=\frac{1}{K}\sum_{k=1}^K\tilde{\beta}^T_{k}\)</span></p></li>
</ol>
<div class="remark">
<p><span id="unlabeled-div-279" class="remark"><em>Remark</em>. </span>The algorithm simply mimicks gradient descent as in a Newton-Raphson algorithm.
One twist is that it uses as estimate of the gradient the marginal treatment effect estimated in another pair of clusters.
This ensures that there will not be overfitting: the choices of optimal treatment level remain indpendent at each stage of the potential outcomes abd covariates in the cluster.
Using all pairs but the treated pairs would not work neither (see Appendix B.1.4 in <a href="http://arxiv.org/abs/2011.08174">Davide’s paper</a>).</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-280" class="remark"><em>Remark</em>. </span>When <span class="math inline">\(p_1&gt;1\)</span>, the algorithm is split in <span class="math inline">\(\frac{T}{p_1}\)</span> sub-waves of length <span class="math inline">\(p_1\)</span>, where we move each coordinate sequentially before moving to the next wave.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-281" class="remark"><em>Remark</em>. </span>How to choose the optimal learning rate <span class="math inline">\(\alpha_{k,t}\)</span>?
Under strong concavity of the objective function, the learning rate should be of order <span class="math inline">\(\frac{J}{t}\)</span>, with for example <span class="math inline">\(J\in\left[0.1,0.2\right]\)</span> when <span class="math inline">\(\beta\)</span> is a proportion.
More robust with moderate to large <span class="math inline">\(T\)</span>:</p>
</div>
<p><span class="math display">\[\begin{align*}  
      \alpha_{k,t} &amp; = \begin{cases}
  \frac{J}{T^{\frac{1-\nu}{2}}||\widehat{M}_{k,t}||} &amp; \text{ if }||\widehat{M}_{k,t}||_2^2&gt;\frac{\kappa}{T^{1-\nu}}-\epsilon_n,\\
  0 &amp; \text{ otherwise }  
                      \end{cases}
  \end{align*}\]</span></p>
<p>for <span class="math inline">\(\epsilon_n&gt;0\)</span>, <span class="math inline">\(\epsilon_n\rightarrow 0\)</span>, and small constants <span class="math inline">\(\nu\leq 1\)</span>, <span class="math inline">\(J\)</span>, <span class="math inline">\(\kappa&gt;0\)</span>.
This approach of dividing the estimated marginal effect by its norm is called gradient norm rescaling and guarantees control of out-of-sample regret under strict quasi-concavity.</p>
<div class="remark">
<p><span id="unlabeled-div-282" class="remark"><em>Remark</em>. </span>Under the assmption that <span class="math inline">\(W(\beta)\)</span> is <span class="math inline">\(\sigma\)</span>-strongly concave, for some strictly positive <span class="math inline">\(\sigma\)</span>, and under additional technical assumptions, the distance between ^*$ and the optimal <span class="math inline">\(\beta^*\)</span> is arbitrarily small, as well as the distance between <span class="math inline">\(W(\beta^*)\)</span> and <span class="math inline">\(W(\hat{\beta}^*)\)</span>.
Davide also shows that regret can be made arbitrarily small in his approach, whether in and out-of-sample.</p>
</div>
</div>
</div>
</div>
<div id="diffusion-effects-with-detailed-networks" class="section level2 hasAnchor" number="11.3">
<h2><span class="header-section-number">11.3</span> Diffusion effects with detailed networks<a href="Diffusion.html#diffusion-effects-with-detailed-networks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We are now going to study what happens when we have access to detailed network information.
We observe the contiguity matrix <span class="math inline">\(A\)</span>, or at least all the relevant links for each member of our sample and the treatment status of each peer of our sample members.
The analysis of such data is going to closely follow the treatment by <a href="https://doi.org/10.1162/rest_a_00818">Michael Leung (2020)</a>.</p>
<div id="setting" class="section level3 hasAnchor" number="11.3.1">
<h3><span class="header-section-number">11.3.1</span> Setting<a href="Diffusion.html#setting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We consider a network of total size <span class="math inline">\(n\)</span>, where connections are represented by the matrix <span class="math inline">\(A\)</span>, which is such that there are no self-links (<span class="math inline">\(A_{i,i}=0\)</span>, <span class="math inline">\(\forall i\in\left\{1,\dots,n\right\}\)</span>).
For each unit <span class="math inline">\(i\)</span> we observe <span class="math inline">\(D_i\)</span>, <span class="math inline">\(Y_i\)</span>, <span class="math inline">\(\gamma_i=\sum_{j=1}^na_{i,j}\)</span> (<span class="math inline">\(i\)</span>’s <em>degree</em> or number of neighbors), and <span class="math inline">\(T_i=\sum_{j=1}^na_{i,j}D_j\)</span> (the number of <span class="math inline">\(i\)</span>’s neighbors that are treated).
We posit a treatment response function that is as follows:</p>
<p><span class="math display">\[\begin{align*}
  Y_i &amp; = r(D_i,T_i,\gamma_i,\epsilon_i),
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\epsilon_i\in\mathbb{R}^{d_{\epsilon}}\)</span> are unobserved influences to outcomes, and <span class="math inline">\(r\)</span> is a function.
One specification of <span class="math inline">\(r\)</span> is the linear first-degree influence model:</p>
<p><span class="math display" id="eq:linearnetworkmodel">\[\begin{equation}
  Y_i  = \beta_1 + \beta_2D_i + \beta_3\frac{T_i}{\gamma_i} + \epsilon_i
  \tag{11.1}
\end{equation}\]</span></p>
<p>where outcomes depend linearly on the proportion of neighbors treated.</p>
<div class="remark">
<p><span id="unlabeled-div-283" class="remark"><em>Remark</em>. </span>Michael’s model implicitly imposes that diffusion effects can only stem from direct connexions.
Connexions further away on the network (friends of friends) have no direct effect on <span class="math inline">\(i\)</span>’s outcome.
This assumption can be relaxed, as long as there is a maximum distance <span class="math inline">\(K\)</span> on the network after which neighbors treatments have no effect on <span class="math inline">\(i\)</span>’s outcome.
In our formulation so far, <span class="math inline">\(K=1\)</span>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-284" class="remark"><em>Remark</em>. </span>The way we collect data is either we observe the full network or, more often, we conduct a snowball-sampling of <span class="math inline">\(1\)</span>-neighborhoods.
In this sampling strategy, we first randomly select a set of <span class="math inline">\(\tilde{n}\leq n\)</span> focal units on which we collect <span class="math inline">\(\left(Y_i,D_i\right)\)</span> and the identity of their neighbors, which gives us <span class="math inline">\(\gamma_i\)</span> and <span class="math inline">\(a_{i,j}\)</span>.
We then collect the treatment status of each of the neighbors, which gives us <span class="math inline">\(T_i\)</span>.
We therefore have as data: <span class="math inline">\(\left(Y_i,D_i,T_i,\gamma_i\right)_{i=1}^{\tilde{n}}\)</span> as well as <span class="math inline">\(\tilde{A}\)</span>, the set of sampled links.</p>
</div>
</div>
<div id="identification-of-causal-effects" class="section level3 hasAnchor" number="11.3.2">
<h3><span class="header-section-number">11.3.2</span> Identification of causal effects<a href="Diffusion.html#identification-of-causal-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We define conditional causal effects as follows:</p>
<p><span class="math display">\[\begin{align*}
  \Delta^Y_{TDT}(t,\gamma) &amp; = \esp{r(1,t,\gamma,\epsilon_i)-r(0,t,\gamma,\epsilon_i)|D_i=1,T_i=t,\gamma_i=\gamma}\\
  \Delta^Y_{TIT}(d,\gamma) &amp; = \esp{r(d,t,\gamma,\epsilon_i)-r(d,t&#39;,\gamma,\epsilon_i)|D_i=d,T_i=t,\gamma_i=\gamma},
\end{align*}\]</span></p>
<p>with <span class="math inline">\(\Delta^Y_{TDT}(t,\gamma)\)</span> the average treatment effect on the directly treated, keeping the indirect level of treatment and the degree of each individual constant; and <span class="math inline">\(\Delta^Y_{TIT}(d,\gamma)\)</span> the average treatment effect on the indirectly treated, keeping the direct level of treatment and the degree of each individual constant.</p>
<div class="remark">
<p><span id="unlabeled-div-285" class="remark"><em>Remark</em>. </span><a href="https://doi.org/10.1162/rest_a_00818">Leung (2020)</a> also allows for the identification of the effect on function of <span class="math inline">\(Y_i\)</span>, <span class="math inline">\(h(Y_i)\)</span>.</p>
</div>
<p>To state identification results for both treatment effects, we are going to make several assumptions on <span class="math inline">\(\tilde{D}=\left\{D_i\right\}_{i=1}^{\tilde{n}}\)</span>, <span class="math inline">\(\tilde{\epsilon}=\left\{\epsilon_i\right\}_{i=1}^{\tilde{n}}\)</span> and <span class="math inline">\(\tilde{A}\)</span>:</p>
<div class="hypothesis">
<p><span id="hyp:Exo" class="hypothesis"><strong>Hypothesis 11.13  (Treatment exogeneity) </strong></span>We assume that <em>(a)</em> <span class="math inline">\(\tilde{D}\Ind\left(\tilde{A},\tilde{\epsilon}\right)\)</span> and <em>(b)</em> <span class="math inline">\(\forall i\in\left\{1,\dots,\tilde{n}\right\}\)</span>, <span class="math inline">\(\epsilon_i\Ind\tilde{A}|\gamma_i\)</span>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-286" class="remark"><em>Remark</em>. </span>Assumption <a href="Diffusion.html#hyp:Exo">11.13</a> imposes that the treatment does not alter links between units across the network, and, furthermore, since treatment is assumed i.i.d., it also imposes that the treatment is not allocated with respect to network characteristics.
This can be relaxed, for example by conducting an experiment stratified on network charcateristics.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-287" class="remark"><em>Remark</em>. </span>Assumption <a href="Diffusion.html#hyp:Exo">11.13</a> imposes full independence between treatment and unobservables, which is satisfied mostly when the treatment is randomly allocated across units.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-288" class="remark"><em>Remark</em>. </span>Part <em>(b)</em> of Assumption <a href="Diffusion.html#hyp:Exo">11.13</a> imposes that links are independent from error terms, conditional on degree.
This assumption rules out unobserved homophily (individuals forming links based on unobserved determinants of outcomes), a key open issue in the literature.</p>
</div>
<p>We also need one technical assumption:</p>
<div class="hypothesis">
<p><span id="hyp:Support" class="hypothesis"><strong>Hypothesis 11.14  (Support) </strong></span>We assume that <em>(a)</em> <span class="math inline">\(\Pr(D_i=1)\in]0,1[\)</span> and <em>(b)</em> there exists <span class="math inline">\(P\)</span>: <span class="math inline">\(\N\rightarrow\left[0,1\right]\)</span> such that, <span class="math inline">\(\forall\gamma\in\N\)</span>, <span class="math inline">\(\frac{1}{\tilde{n}}\sum_{i=1}^{\tilde{n}}\uns{\gamma_i=\gamma}\probconv P(\gamma)\)</span> and <span class="math inline">\(\Gamma=\left\{\gamma:P(\gamma)&gt;0\right\}\)</span> is not empty and is different from <span class="math inline">\(\left\{0\right\}\)</span>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-289" class="remark"><em>Remark</em>. </span>Assumption <a href="Diffusion.html#hyp:Support">11.14</a> imposes that at least some sampled units have at least one link, and some of them are treatedd and some of them are not.</p>
</div>
<p>We can now prove identification of our effects of interest:</p>
<div class="theorem">
<p><span id="thm:IdentLeung" class="theorem"><strong>Theorem 11.8  </strong></span>Under Assumptions <a href="Diffusion.html#hyp:Exo">11.13</a> and <a href="Diffusion.html#hyp:Support">11.14</a>, <span class="math inline">\(\Delta^Y_{TDT}(t,\gamma)\)</span> and <span class="math inline">\(\Delta^Y_{TIT}(d,\gamma)\)</span> are identified, <span class="math inline">\(\forall d\in\left\{0,1\right\}\)</span>, <span class="math inline">\(\forall t \leq \gamma\)</span>, <span class="math inline">\(\forall\gamma\in\Gamma\)</span>:</p>
<p><span class="math display">\[\begin{align*}
  \Delta^Y_{TDT}(t,\gamma) &amp; = \esp{Y_i|D_i=1,T_i=t,\gamma_i=\gamma}-\esp{Y_i|D_i=0,T_i=t,\gamma_i=\gamma},\\
  \Delta^Y_{TIT}(d,\gamma) &amp; = \esp{Y_i|D_i=d,T_i=t,\gamma_i=\gamma}-\esp{Y_i|D_i=d,T_i=t&#39;,\gamma_i=\gamma}.
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-290" class="proof"><em>Proof</em>. </span>Under Assumption <a href="Diffusion.html#hyp:Support">11.14</a>, <span class="math inline">\(\esp{Y_i|D_i=d,T_i=t,\gamma_i=\gamma}\)</span> is well defined <span class="math inline">\(\forall d\in\left\{0,1\right\}\)</span>, <span class="math inline">\(\forall t \leq \gamma\)</span>, <span class="math inline">\(\forall\gamma\in\Gamma\)</span>.
We have, <span class="math inline">\(\forall d\in\left\{0,1\right\}\)</span>, <span class="math inline">\(\forall t,t&#39; \leq \gamma\)</span>:</p>
<p><span class="math display">\[\begin{align*}
  \esp{Y_i|D_i=d,T_i=t,\gamma_i=\gamma} &amp; = \esp{r(d,t,\gamma,\epsilon_i)|D_i=d,T_i=t,\gamma_i=\gamma}\\
                                        &amp; = \esp{\esp{r(d,t,\gamma,\epsilon_i)|\tilde{A},D_i=d,T_i=t,\gamma_i=\gamma}|D_i=d,T_i=t,\gamma_i=\gamma}\\
                                        &amp; = \esp{\esp{r(d,t,\gamma,\epsilon_i)|\gamma_i=\gamma}|\gamma_i=\gamma}\\
                                        &amp; = \esp{r(d,t,\gamma,\epsilon_i)|\gamma_i=\gamma}\\
                                        &amp; = \esp{r(d,t,\gamma,\epsilon_i)|D_i=d&#39;,T_i=t&#39;,\gamma_i=\gamma},
\end{align*}\]</span></p>
<p>where the first quality is by definition, the second equality uses the Law of Iterated Expectations, and the third equality uses Assumption <a href="Diffusion.html#hyp:Exo">11.13</a>.
The third equality uses the fact that Assumption <a href="Diffusion.html#hyp:Exo">11.13</a> implies that <span class="math inline">\((D_i=d,T_i=t)\Ind\epsilon_i|(\tilde{A},\gamma_i)\)</span>, which enables us to undo the conditioning on <span class="math inline">\((D_i=d,T_i=t)\)</span> in the inner expectation.
We then use part <em>(b)</em> of Assumption <a href="Diffusion.html#hyp:Exo">11.13</a> to undo the conditioning on <span class="math inline">\(\tilde{A}\)</span>.
Since the inner expectation then does only depend on <span class="math inline">\(\gamma_i\)</span>, the outer expectation also does, by the Law of Iterated Expectations.
This undoes the conditioning on <span class="math inline">\((D_i=d,T_i=t)\)</span> in the outer expectation.
The same reasoning applied in reverse gives the last equality.
This proves the result.</p>
</div>
</div>
<div id="estimation-of-causal-effects" class="section level3 hasAnchor" number="11.3.3">
<h3><span class="header-section-number">11.3.3</span> Estimation of causal effects<a href="Diffusion.html#estimation-of-causal-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Michael explores two estimators of the causal effects, one nonparametric and one parametric.
The nonparametric estimators are as follows:</p>
<p><span class="math display">\[\begin{align*}
  \hat{\Delta}^{Y^{np}}_{TDT}(t,\gamma) &amp; = \hat{\mu}(1,t,\gamma)-\hat{\mu}(0,t,\gamma)\\
  \hat{\Delta}^{Y^{np}}_{TIT}(d,\gamma) &amp; = \hat{\mu}(d,t,\gamma)-\hat{\mu}(d,t&#39;,\gamma)\\
  \hat{\mu}(d,t,\gamma) &amp; = \frac{\sum_{i=1}^{\tilde{n}}Y_i\unsi{i}{d,t,\gamma}}{\sum_{i=1}^{\tilde{n}}\unsi{i}{d,t,\gamma}} \\
  \unsi{i}{d,t,\gamma} &amp; = \uns{D_i=1,T_i=t,\gamma_i=\gamma}.
\end{align*}\]</span></p>
<p>The parametric estimators are:</p>
<p><span class="math display">\[\begin{align*}
  \hat{\Delta}^{Y^{p}}_{TDT}(t,\gamma) &amp; = \hat{\beta}^{OLS}_2\\
  \hat{\Delta}^{Y^{np}}_{TIT}(d,\gamma) &amp; = \hat{\beta}^{OLS}_3\left(\frac{t-t&#39;}{\gamma}\right),
\end{align*}\]</span></p>
<p>using the OLS estimates of Equation <a href="Diffusion.html#eq:linearnetworkmodel">(11.1)</a>.</p>
<p>We need several assumptions to ensure that our estimators converge to the actual treatment effect when the network size grows large.
Let <span class="math inline">\(\max_kA_{ik}A_{jk}\)</span> denote whether <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are indirectly linked through a common friend <span class="math inline">\(k\)</span>.</p>
<div class="hypothesis">
<p><span id="hyp:IndepShocks" class="hypothesis"><strong>Hypothesis 11.15  </strong></span>For any pair <span class="math inline">\((i,j)\in\left\{1,\dots,n\right\}^2\)</span>, <em>(a)</em> <span class="math inline">\(\epsilon_i\Ind\epsilon_j|\tilde{A},A_{ij}=0,\max_kA_{ik}A_{jk}=0\)</span> and <em>(b)</em> <span class="math inline">\((\epsilon_i,\epsilon_j)\Ind\tilde{A}|A_{ij},\gamma_i,\gamma_j,\sum_kA_{ik}A_{jk}\)</span>.</p>
</div>
<p>Part (a) of Assumption <a href="Diffusion.html#hyp:IndepShocks">11.15</a> imposes that <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(\epsilon_j\)</span> can only be correlated if <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> they are neighbors or share a common neighbor.
Part (b) of Assumption <a href="Diffusion.html#hyp:IndepShocks">11.15</a> imposes that <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(\epsilon_j\)</span> depend on the network only through <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>’s own connection, own degrees, and number of common connections.</p>
<p>We are now going to define a set of properties on the connections between units that will enable to apply a CLT with non independent data.
Under Assumption <a href="Diffusion.html#hyp:IndepShocks">11.15</a>, we know that the outcome of observations are correlated across the netework when either they are direct neighbor or they have a neighbor in common.
Let us encode these connections within a new matrix, <span class="math inline">\(G\)</span>, such that each entry measures whether the outcome of two observations are correlated or not: <span class="math inline">\(G_{ij}=\uns{A_{ij}+\max_kA_{ik}A_{jk}+\uns{i=j}&gt;0}\)</span>.
Let <span class="math inline">\(\mathbf{N}_i=\left\{j:G_{ij}=1\right\}\)</span> be the set of units whose outcomes are correlated with that of <span class="math inline">\(i\)</span>, and <span class="math inline">\(|\mathbf{N}_i|\)</span> the cardinal of this set (<em>i.e.</em> the number of units whose outcomes are correlated with <span class="math inline">\(i&#39;s\)</span> outcome).
Let <span class="math inline">\(G^3\)</span> be the third matrix power of <span class="math inline">\(G\)</span>.</p>
<div class="hypothesis">
<p><span id="hyp:DegreeDistribution" class="hypothesis"><strong>Hypothesis 11.16  </strong></span><span class="math inline">\(\frac{1}{\tilde{n}}\sum_{i=1}^{\tilde{n}}|\mathbf{N}_i|^3\)</span> and <span class="math inline">\(\frac{1}{\tilde{n}}\sum_{i=1}^{\tilde{n}}\sum_{j\neq i}(G^3)_{ij}\)</span> are bounded in probability.</p>
</div>
<p>Assumption <a href="Diffusion.html#hyp:DegreeDistribution">11.16</a> imposes that the amount of links in the network is small enough so that the <span class="math inline">\(G\)</span> matrix is sparse.
Assumption <a href="Diffusion.html#hyp:DegreeDistribution">11.16</a> implies that the average degree is bounded asymptotically, so that average degree is sunstantially smaller than sample size.
There is no direct way to test for this, but one convenient approach is to compute the density of <span class="math inline">\(G\)</span> (its proportion of linked pairs), <span class="math inline">\(\frac{1}{\left(\begin{array}\tilde{n}\\2\end{array}\right)}\sum_{i\leq j}G_{ij}=\frac{2\sum_{i=1}^{\tilde{n}}\gamma_i}{\tilde{n}-1}\)</span>.
In sparse enough networks, the density is around 10%.
The second part of Assumption <a href="Diffusion.html#hyp:DegreeDistribution">11.16</a> requires that higher order moments of the degree distribution are bounded, which controls the tails of the distribution of degrees.
One way to test for this condition is to compute the tail index of the distribution of degrees, and to find that they decrease exponentially fast, as for example in <a href="https://link.springer.com/10.1007/978-3-319-16877-7">Ibragimov et al. (2015)</a>.</p>
<div class="remark">
<p><span id="unlabeled-div-291" class="remark"><em>Remark</em>. </span>We can allow for connections of order higher than two in Assumption <a href="Diffusion.html#hyp:IndepShocks">11.15</a>, as long as there exists some finite <span class="math inline">\(K\)</span> beyind which correlations between the outcomes of units <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are zero.
We can also allow for connections across time or across clusters or across overlapping clusters</p>
</div>
<p>Let us now state Michael’s Theorem for the distribution of the nonparametric estimator of treatment effects:</p>
<div class="theorem">
<p><span id="thm:EstimLeungNP" class="theorem"><strong>Theorem 11.9  (CLT for nonparametric estimator of diffusion effects) </strong></span>Under Assumptions <a href="Diffusion.html#hyp:Exo">11.13</a>, <a href="Diffusion.html#hyp:Support">11.14</a>, <a href="Diffusion.html#hyp:IndepShocks">11.15</a> and <a href="Diffusion.html#hyp:DegreeDistribution">11.16</a> (and some regularity conditions summrized in Assumptions 5 and 6 in <a href="https://doi.org/10.1162/rest_a_00818">Leung (2019)</a>), <span class="math inline">\(\forall d,d&#39;\in\left\{0,1\right\}\)</span>, <span class="math inline">\(\gamma\in\Gamma\)</span> and integers <span class="math inline">\(t,t&#39;\leq\gamma\)</span>, there exists <span class="math inline">\(\sigma^2_{TS}\)</span> such that:</p>
<p><span class="math display">\[\begin{align*}
  \sqrt{\tilde{n}}\left(\left(\hat{\mu}(d,t,\gamma)-\hat{\mu}(d&#39;,t&#39;,\gamma)\right)-\left(\mu(d,t,\gamma)-\mu(d&#39;,t&#39;,\gamma)\right)\right)\distr\mathcal{N}(0,\sigma^2_{TS}).
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-292" class="proof"><em>Proof</em>. </span>See proof of Theorem 2 in <a href="https://doi.org/10.1162/rest_a_00818">Leung (2019)</a>.</p>
</div>
<p>Let us now state Michael’s Theorem for the parametric estimator of diffusion effects.
We first need the following assumption, with <span class="math inline">\(X\)</span> the <span class="math inline">\(\tilde{n}\times 3\)</span> matrix of regressors in Equation <a href="Diffusion.html#eq:linearnetworkmodel">(11.1)</a>, <span class="math inline">\(\Theta\)</span> the corresponding <span class="math inline">\(3\times 1\)</span> matrix of coefficients, and <span class="math inline">\(\rho_{ij}(X,G)=\esp{\epsilon_i,\epsilon_j|X_i,X_j,G_{ij}=1}\)</span>:</p>
<div class="hypothesis">
<p><span id="hyp:Covariance" class="hypothesis"><strong>Hypothesis 11.17  </strong></span>We assume that there exists positive definite matrices <span class="math inline">\(V\)</span> and <span class="math inline">\(S\)</span> such that <span class="math inline">\(\esp{\frac{1}{\tilde{n}}X&#39;X|\tilde{A}}\probconv V\)</span> and <span class="math inline">\(\frac{1}{\tilde{n}}\sum_{i=1}^{\tilde{n}}\sum_{j\in\mathbf{N}_i}\esp{\rho_{ij}(X,G)X_iX_j&#39;|\tilde{A}}\probconv S\)</span> and that <span class="math inline">\(\esp{\epsilon_i|X_i,\tilde{A}}=0\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:EstimLeungP" class="theorem"><strong>Theorem 11.10  (CLT for parametric estimator of diffusion effects) </strong></span>Under Assumptions <a href="Diffusion.html#hyp:Exo">11.13</a>, <a href="Diffusion.html#hyp:Support">11.14</a>, <a href="Diffusion.html#hyp:IndepShocks">11.15</a>, <a href="Diffusion.html#hyp:DegreeDistribution">11.16</a>, and <a href="Diffusion.html#hyp:Covariance">11.17</a>:</p>
<p><span class="math display">\[\begin{align*}
  \sqrt{\tilde{n}}\left(\hat{\Theta}-\Theta\right)\distr\mathcal{N}(0,V^{-1}SV^{-1}).
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-293" class="proof"><em>Proof</em>. </span>See proof of Theorem SA.2.2 in <a href="https://doi.org/10.1162/rest_a_00818">Leung (2019)</a>.</p>
</div>
</div>
<div id="estimation-of-sampling-noise-6" class="section level3 hasAnchor" number="11.3.4">
<h3><span class="header-section-number">11.3.4</span> Estimation of sampling noise<a href="Diffusion.html#estimation-of-sampling-noise-6" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For estimating sampling noise, Michael proposes two estimators: one for the sampling noise of the nonparametric estimator and one for the sampling noise of the pâraetric estimator.
For the nonparametric estimator, Michael proposes the following approach:</p>
<p><span class="math display">\[\begin{align*}
  \hat{\sigma}^2_{TS} &amp; = \frac{1}{\tilde{n}}\sum_{i=1}^{\tilde{n}}\sum_{j=1}^{\tilde{n}}G_{ij}((Y_ia_i-b_i)(Y_ja_j-b_j))\\
  a_i &amp; = \frac{\unsi{i}{d,t,\gamma}}{\hat{\rho}(d,t,\gamma)}-\frac{\unsi{i}{d&#39;,t&#39;,\gamma}}{\hat{\rho}(d&#39;,t&#39;,\gamma)}\\
  b_i &amp; = \hat{\mu}(d,t,\gamma)\frac{\unsi{i}{d,t,\gamma}}{\hat{\rho}(d,t,\gamma)}-\hat{\mu}(d&#39;,t&#39;,\gamma)\frac{\unsi{i}{d&#39;,t&#39;,\gamma}}{\hat{\rho}(d&#39;,t&#39;,\gamma)}\\
  \hat{\rho}(d,t,\gamma) &amp; = \frac{1}{\tilde{n}}\sum_{i=1}^{\tilde{n}}\unsi{i}{d,t,\gamma}.
\end{align*}\]</span></p>
<p>For the parametric estimator Michael proposes the following estimator for the covariance matrix of the parameters of Equation <a href="Diffusion.html#eq:linearnetworkmodel">(11.1)</a> estimated by OLS:</p>
<p><span class="math display">\[\begin{align*}
  \hat{\mathbf{\Sigma}} &amp; = (X&#39;X)^{-1}\mathcal{M}&#39;G\mathcal{M}(X&#39;X)^{-1},
\end{align*}\]</span></p>
<p>with <span class="math inline">\(\mathcal{M}=(X_1\hat{\epsilon}_1,\dots,X_{\tilde{n}}\hat{\epsilon}_{\tilde{n}})&#39;\)</span>, and <span class="math inline">\(\hat{\epsilon}\)</span> the vector of regression residuals.</p>
<p>We can now state two propositions showing that the estimators of sampling noise proposed by Michael are consistent:</p>
<div class="proposition">
<p><span id="prp:ConsistentNoiseNP" class="proposition"><strong>Proposition 11.2  (Consistyency of nonparametric estimator of sampling noise) </strong></span>Under the Assumptions of Theorem <a href="Diffusion.html#thm:EstimLeungNP">11.9</a>, <span class="math inline">\(\hat{\sigma}^2_{TS}\probconv\sigma^2_{TS}\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-294" class="proof"><em>Proof</em>. </span>See proof of Proposition 2 in <a href="https://doi.org/10.1162/rest_a_00818">Leung (2019)</a>.</p>
</div>
<div class="proposition">
<p><span id="prp:ConsistentNoiseP" class="proposition"><strong>Proposition 11.3  (Consistyency of nonparametric estimator of sampling noise) </strong></span>Under Assumptions <a href="Diffusion.html#hyp:Exo">11.13</a>, <a href="Diffusion.html#hyp:Support">11.14</a>, <a href="Diffusion.html#hyp:IndepShocks">11.15</a>, <a href="Diffusion.html#hyp:DegreeDistribution">11.16</a>, <a href="Diffusion.html#hyp:Covariance">11.17</a>, and regularity conditions made clear in Proposition SA.2.4 in <a href="https://doi.org/10.1162/rest_a_00818">Leung (2019)</a>, <span class="math inline">\(\hat{\mathbf{\Sigma}}\probconv V^{-1}SV^{-1}\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-295" class="proof"><em>Proof</em>. </span>See proof of Proposition SA.2.4 in <a href="https://doi.org/10.1162/rest_a_00818">Leung (2019)</a>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-296" class="remark"><em>Remark</em>. </span>One assumption that is hard to swallow here is that effects die after <span class="math inline">\(K\)</span> connections and correlations after <span class="math inline">\(K+1\)</span> connections.
To relax these assumptions you can to conduct inference at the cluster level, such as in <a href="https://science.sciencemag.org/content/341/6144/1236498">Banerjee et al. (2013)</a>, <a href="https://www.science.org/doi/10.1126/science.aaa0491">Guiteras et al. (2015)</a> and <a href="https://www.aeaweb.org/articles?id=10.1257/aer.20200295">Beaman et al. (2021)</a> for example.
Another approach is to use the bootstrap, with justifications in <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-49/issue-2/Empirical-process-results-for-exchangeable-arrays/10.1214/20-AOS1981.full">Davezies et al. (2021)</a> and
<a href="https://kamilanowakowicz.com/network_bootstrap.pdf">Nowakowicz (2024)</a>.</p>
</div>
</div>
<div id="nonparametric-tests-for-the-existence-of-diffusion-effects-based-on-randomization-inference" class="section level3 hasAnchor" number="11.3.5">
<h3><span class="header-section-number">11.3.5</span> Nonparametric tests for the existence of diffusion effects based on randomization inference<a href="Diffusion.html#nonparametric-tests-for-the-existence-of-diffusion-effects-based-on-randomization-inference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One final question that we might have about diffusion effects with detailed network information is whether we can adapt nonparametric tests such as Randomization Inference to test for the existence and shape of diffusion effects.
One especially interesting thing to test would be the existence of diffusion effects up to <span class="math inline">\(K\)</span> levels of separation within the network, as assumption used to identify diffusion effects in the sections just above.
<a href="https://doi.org/10.1080/01621459.2016.1241178">Athey, Eckles, and Imbens (2018)</a> have developed an approach to do just that, that we are going to detail here.</p>
<div id="overview-of-athey-eckles-and-imbens-2018s-approach" class="section level4 hasAnchor" number="11.3.5.1">
<h4><span class="header-section-number">11.3.5.1</span> Overview of <a href="https://doi.org/10.1080/01621459.2016.1241178">Athey, Eckles, and Imbens (2018)</a>’s approach<a href="Diffusion.html#overview-of-athey-eckles-and-imbens-2018s-approach" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A key concept to understand what <a href="https://doi.org/10.1080/01621459.2016.1241178">Athey, Eckles, and Imbens (2018)</a> are doing is that of <strong>sharpness</strong>.
An assumption is <strong>sharp</strong> if it enables the researcher to infer the outcomes of each observation under counterfactual treatment vectors.
For example, the assumption of no-effect is sharp because it imposes that all the observed outcomes are the ones with and without the treatment at the same time, and thus we can reallocate the treatment and infer the outcomes for each reallocation (they do not change).</p>
<p>The problem that <a href="https://doi.org/10.1080/01621459.2016.1241178">Athey, Eckles, and Imbens (2018)</a> try to solve is that of testing assumptions that are non sharp in the original experiment.
The assumption of no spillover effects for example is not sharp in the original experiment since it allows for the treatment to have a direct unknown effect on each unit.
When drawing a new treatment allocation, we do not know what the outcome of those who have changed treatment status should be.</p>
<p><a href="https://doi.org/10.1080/01621459.2016.1241178">Athey, Eckles, and Imbens (2018)</a>’s approach for randomization inference tries to avoid having to assume the absence of treatment effects everywhere in order to test higher for order level assumptions.
For that, AEI uses a set of focal units for which treatment status is going to remain fixed over repetitions.
Then, we select a set of non focal units (who they are depends on the actual assumption to test).
We select a test statistic measuring the strength of evidence against <span class="math inline">\(H0\)</span>.
We allocate the treatment at random among the non-focal units multiple times.
We derive the distribution of the test statistic under <span class="math inline">\(H0\)</span>.
Compute the actual test statistic in the real sample and derive its p-value using the empirically-derived distribution.</p>
</div>
<div id="some-definitions" class="section level4 hasAnchor" number="11.3.5.2">
<h4><span class="header-section-number">11.3.5.2</span> Some definitions<a href="Diffusion.html#some-definitions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Level sets</strong></p>
</div>
<div id="testing-for-the-existence-of-any-effect-of-the-treatment" class="section level4 hasAnchor" number="11.3.5.3">
<h4><span class="header-section-number">11.3.5.3</span> Testing for the existence of any effect of the treatment<a href="Diffusion.html#testing-for-the-existence-of-any-effect-of-the-treatment" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The null hypothesis in this case is that all the effects of the treatment are zero: <span class="math inline">\(Y_{i}(\mathbf{D})=Y_{i}(\mathbf{D}&#39;)\)</span>, <span class="math inline">\(\forall i, \forall \mathbf{D},\mathbf{D}&#39;\in\mathbf{\Omega}\)</span>.</p>
<p>To test for this assumption, we randomly allocate all hydrographic zones to placebo vulnerable zones, which gives us a randomized treatment vector <span class="math inline">\(\tilde{D}_{i}\)</span>.
To test for the existence of a treatment effect, we use as test statistics <span class="math inline">\(\hat{\beta}_{OLS}\)</span> and <span class="math inline">\(\hat{\delta}_{DID}\)</span>, estimated using the following regressions:</p>
</div>
</div>
</div>
<div id="diffusion-effects-with-did" class="section level2 hasAnchor" number="11.4">
<h2><span class="header-section-number">11.4</span> Diffusion effects with DID<a href="Diffusion.html#diffusion-effects-with-did" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sylvain’s approach vs Kyle’s approach</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="LaLonde.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Distribution.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/chabefer/STCI/blob/master/10_Diffusion.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["STCI.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"toc_depth": 1
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
